{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building one model per meter\n",
    "\n",
    "> This notebook is a slight modification of `all_meters_one_model.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from ashrae import loading, preprocessing, feature_testing, modelling\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import typing\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn import linear_model, tree, model_selection, ensemble\n",
    "\n",
    "from fastai.tabular.all import *\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.plotting.backend = \"plotly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_test = True\n",
    "do_submit = False\n",
    "data_path = loading.DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loading.N_TRAIN = 10_000\n",
    "loading.N_TEST = 10_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ashrae_data = loading.load_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = preprocessing.Processor() # t_train=t_train\n",
    "tfms_config = {\n",
    "    'add_random_noise_features':{},\n",
    "    'add_time_features':{},\n",
    "    'add_weather_features':{'fix_time_offset':True,\n",
    "                            'add_na_indicators':True,\n",
    "                            'impute_nas':True},\n",
    "    'add_building_features':{},\n",
    "}\n",
    "\n",
    "df, var_names = processor(ashrae_data['meter_train'], tfms_configs=tfms_config,\n",
    "                          df_weather=ashrae_data['weather_train'],\n",
    "                          df_building=ashrae_data['building'])\n",
    "\n",
    "%time\n",
    "df_test, _ = processor(ashrae_data['meter_test'], tfms_configs=tfms_config,\n",
    "                         df_weather=ashrae_data['weather_test'],\n",
    "                         df_building=ashrae_data['building'])\n",
    "df_test = preprocessing.align_test(df, var_names, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling `df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n = len(df)\n",
    "\n",
    "if False: # per building_id and meter sampling\n",
    "    n_sample_per_bid = 500\n",
    "    replace = True\n",
    "\n",
    "    df = (df.groupby(['building_id', 'meter'])\n",
    "         .sample(n=n_sample_per_bid, replace=replace))\n",
    "\n",
    "if False: # general sampling\n",
    "    frac_samples = .1\n",
    "    replace = False\n",
    "\n",
    "    df = (df.sample(frac=frac_samples, replace=replace))\n",
    "\n",
    "print(f'using {len(df)} samples = {len(df)/n*100:.2f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# t_train = pd.read_parquet(data_path/'t_train.parquet')\n",
    "t_train = None\n",
    "\n",
    "%time\n",
    "#split_kind = 'random'\n",
    "#split_kind = 'time'\n",
    "# split_kind = 'fix_time'\n",
    "split_kind = 'time_split_day'\n",
    "train_frac = .9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meter_train_samples = {}\n",
    "\n",
    "all_splits = {}\n",
    "for meter, _df in df.groupby('meter'):\n",
    "    splits = preprocessing.split_dataset(_df, split_kind=split_kind, train_frac=train_frac,\n",
    "                                         t_train=t_train)\n",
    "    all_splits[meter] = splits\n",
    "    meter_train_samples[meter] = len(splits[0])\n",
    "    print(f'meter: {meter} â‡’ sets {len(splits)}, train {len(splits[0])} = {len(splits[0])/len(_df):.4f}, valid {len(splits[1])} = {len(splits[1])/len(_df):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# procs = [] \n",
    "procs = [FillMissing, Normalize, Categorify]\n",
    "\n",
    "tos = {}\n",
    "for meter, _df in df.groupby('meter'):\n",
    "    splits = all_splits[meter]\n",
    "    \n",
    "    tos[meter] = feature_testing.get_tabular_object(_df,\n",
    "                                                    var_names,\n",
    "                                                    splits=splits,\n",
    "                                                    procs=procs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_bs = val_bs = {\n",
    "    0: 500, # 100000,\n",
    "    1: 200, # 40000,\n",
    "    2: 150, # 20000,\n",
    "    3: 50, # 10000,\n",
    "}\n",
    "\n",
    "all_dls = {meter: to.dataloaders(bs=train_bs[meter], val_bs=val_bs[meter])\n",
    "           for meter, to in tos.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: Takes about 12min with the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_bs = 500\n",
    "\n",
    "if do_test:\n",
    "    all_test_dls = {meter: dls.test_dl(df_test.loc[df_test['meter']==meter,:], bs=test_bs) \n",
    "                    for meter, dls in all_dls.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_ys(ys:typing.List[tuple]):\n",
    "    'preds is a list of tuples, each of size 2. the first entry is the index and the second the predictions'\n",
    "    y = pd.Series(np.concatenate([_y for (_,_y) in ys]),\n",
    "                  index=np.concatenate([_ix for (_ix,_) in ys]))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "params = {'n_estimators': 20, 'max_features': 'sqrt'}\n",
    "model = ensemble.RandomForestRegressor\n",
    "# params = {}\n",
    "# model = linear_model.LinearRegression\n",
    "\n",
    "ms = {meter: model(**params) for meter in tos}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for meter in ms:\n",
    "    ms[meter].fit(tos[meter].train.xs.values, \n",
    "                  tos[meter].train.ys.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_valid_preds = [(tos[meter].valid.xs.index, ms[meter].predict(tos[meter].valid.xs.values))\n",
    "                 for meter in tos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_pred = sort_ys(y_valid_preds)\n",
    "y_valid_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if do_test:\n",
    "    y_test_preds = [(all_test_dls[meter].xs.index, \n",
    "                     ms[meter].predict(all_test_dls[meter].xs))\n",
    "                     for meter in all_test_dls]\n",
    "    y_test_pred = sort_ys(y_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_true = [(tos[meter].valid.ys.index, tos[meter].valid.ys.values.ravel())\n",
    "                for meter in tos]\n",
    "y_valid_true = sort_ys(y_valid_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_score = modelling.evaluate_torch(torch.from_numpy(y_valid_true.values), \n",
    "                                    torch.from_numpy(y_valid_pred.values)).item()\n",
    "print(f'sklearn loss {nb_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `dep_var` distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train vs validation distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "pick_random = lambda x: x if len(x)<5000 else np.random.choice(x, size=5000, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_testing.hist_plot_preds(pick_random(y_valid_true), \n",
    "                                pick_random(y_valid_pred), \n",
    "                                label0='truth', label1='prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_test:\n",
    "    feature_testing.hist_plot_preds(pick_random(y_valid_true), \n",
    "                                    pick_random(y_test_pred), \n",
    "                                    label0='truth (validation)', \n",
    "                                    label1='prediction (test set)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boldly wrong predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "base_cols = ['building_id', 'meter','timestamp']\n",
    "miss_cols = [v for v in base_cols if v not in tos[0].valid.xs.columns]\n",
    "miss_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(miss_cols) > 0:\n",
    "    tmp = pd.concat([to.valid.xs.drop(columns=set(base_cols).difference(miss_cols)).join(df.loc[:,base_cols])\n",
    "                     for to in tos.values()])\n",
    "else:\n",
    "    tmp = pd.concat([to.valid.xs for to in tos.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwt = feature_testing.BoldlyWrongTimeseries(tmp, y_valid_true, y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwt.run_boldly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission to kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = y_test_pred.sort_index()\n",
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if do_submit:\n",
    "    y_test_pred_original = torch.exp(tensor(y_test_pred)) - 1\n",
    "\n",
    "    y_out = pd.DataFrame(cnr(y_test_pred_original),\n",
    "                         columns=['meter_reading'],\n",
    "                         index=df_test.index)\n",
    "    display(y_out.head())\n",
    "\n",
    "    assert len(y_out) == 41697600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if do_submit:\n",
    "    y_out.to_csv(data_path/'my_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`kaggle competitions submit -c ashrae-energy-prediction -f submission.csv -m \"Message\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act = 'ReLu'\n",
    "\n",
    "model_msg = f'RandomForest'\n",
    "# model_msg = f'tabular_learner (one per meter): act {act}, layers {layers}, ps {ps}, embed_p {embed_p}'\n",
    "split_msg = f'split kind \"{split_kind}\" train_frac {train_frac}'\n",
    "samples_msg = f'num samples {len(df)} = {len(df)/n*100:.2f} %'\n",
    "features_msg = f'train_bs = {train_bs} dep_var_stats and 1hot meter and remove leading empty weeks and us_holidays and fix bid 363'\n",
    "score_msg = f'nb score {nb_score:.4f}'\n",
    "# message = ['baseline (linear regression on dep_var_stats and 1hot meter) ', '500 obs/bid', f'nb score {nb_score:.4f}']\n",
    "# message = ['random forest', '500 obs/bid', 'all features', f'nb score {nb_score:.4f}']\n",
    "# message = ['lightgbm', '500 obs/bid', '100 rounds', '42 leaves', 'lr .5', f'nb score {nb_score:.4f}']\n",
    "# message = ['tabular_learner', '500 obs/bid', 'all features', f'layers {layers}, embed_p .1, ps [.1,.1,.1]', f'nb score {nb_score:.4f}']\n",
    "message = ' + '.join([model_msg, samples_msg, split_msg, features_msg, score_msg])\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_test and do_submit:\n",
    "    print('Submitting...')\n",
    "    !kaggle competitions submit -c ashrae-energy-prediction -f '{data_path}/my_submission.csv' -m '{message}'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38_fastai]",
   "language": "python",
   "name": "conda-env-py38_fastai-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
