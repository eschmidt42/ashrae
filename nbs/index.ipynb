{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Ashrae project\n",
    "\n",
    "> Building models for the Ashrae prediction challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "* continue debugging `preprocessing.ipynb`: high resource demand with `nbdev_test_nbs`, figure out why and fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from ashrae import loading, preprocessing, feature_testing, leaderboard, inspection, modelling\n",
    "import plotly.express as px\n",
    "from fastcore.foundation import L, Path\n",
    "from fastai.tabular.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "pd.options.plotting.backend = \"plotly\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining wether to process the test set (warning, this alone takes 12+ minutes) and submit the results to kaggel (you will need your credentials set up)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_test = True\n",
    "do_submit = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining where the csv files are located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loading.DATA_PATH = Path(\"../data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle competitions download -c ashrae-energy-prediction -p {data_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip {data_path}/ashrae-energy-prediction.zip -d {data_path}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle competitions leaderboard -c ashrae-energy-prediction -p {data_path} --download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip {data_path}/ashrae-energy-prediction.zip -d {data_path}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "loading.N_TRAIN = 10_000\n",
    "loading.N_TEST = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "csvs = loading.get_csvs()\n",
    "csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ashrae_data = loading.load_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_leaderboard = pd.read_csv(csvs['public-leaderboard'], parse_dates=['SubmissionDate'])\n",
    "df_leaderboard.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dis = leaderboard.get_leaderboard_distribution(df_leaderboard)\n",
    "dis['Score'].describe(percentiles=[.05, .1, .25, .5, .75, .95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = preprocessing.Processor() \n",
    "tfms_config = {\n",
    "    'add_time_features':{},\n",
    "    'add_weather_features':{'fix_time_offset':True,\n",
    "                            'add_na_indicators':True,\n",
    "                            'impute_nas':True},\n",
    "    'add_building_features':{},\n",
    "}\n",
    "\n",
    "df, var_names = processor(ashrae_data['meter_train'], tfms_configs=tfms_config,\n",
    "                          df_weather=ashrae_data['weather_train'],\n",
    "                          df_building=ashrae_data['building'])\n",
    "\n",
    "if do_test:\n",
    "    %time\n",
    "    df_test, _ = processor(ashrae_data['meter_test'], tfms_configs=tfms_config,\n",
    "                             df_weather=ashrae_data['weather_test'],\n",
    "                             df_building=ashrae_data['building'])\n",
    "    df_test = preprocessing.align_test(df, var_names, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from `df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n = len(df)\n",
    "\n",
    "if False: # per building_id and meter sampling\n",
    "    n_sample_per_bid = 500\n",
    "    replace = True\n",
    "\n",
    "    df = (df.groupby(['building_id', 'meter'])\n",
    "         .sample(n=n_sample_per_bid, replace=replace))\n",
    "\n",
    "if False: # general sampling\n",
    "    frac_samples = .05\n",
    "    replace = False\n",
    "\n",
    "    df = (df.sample(frac=frac_samples, replace=replace))\n",
    "\n",
    "print(f'using {len(df)} samples = {len(df)/n*100:.2f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# t_train = pd.read_parquet(data_path/'t_train.parquet')\n",
    "t_train = None\n",
    "\n",
    "%time\n",
    "#split_kind = 'random'\n",
    "#split_kind = 'time'\n",
    "# split_kind = 'fix_time'\n",
    "split_kind = 'time_split_day'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_frac = .9\n",
    "splits = preprocessing.split_dataset(df, split_kind=split_kind, train_frac=train_frac,\n",
    "                                     t_train=t_train)\n",
    "print(f'sets {len(splits)}, train {len(splits[0])} = {len(splits[0])/len(df):.4f}, valid {len(splits[1])} = {len(splits[1])/len(df):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# procs = [] \n",
    "procs = [Categorify, FillMissing, Normalize]\n",
    "to = feature_testing.get_tabular_object(df,\n",
    "                                        var_names,\n",
    "                                        splits=splits,\n",
    "                                        procs=procs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_bs = 256\n",
    "val_bs = 256\n",
    "\n",
    "dls = to.dataloaders(bs=train_bs, val_bs=val_bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_bs = 1024\n",
    "\n",
    "if do_test:\n",
    "    test_dl = dls.test_dl(df_test, bs=test_bs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a neural net using `tabular_learner`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [50, 25, 12]\n",
    "y_range = (-.1, 17)\n",
    "\n",
    "config = tabular_config(embed_p=.01, ps = [.5, .5, .5])\n",
    "\n",
    "learn = tabular_learner(dls, y_range=y_range, layers=layers,\n",
    "                        n_out=1, config=config, \n",
    "                        loss_func=modelling.evaluate_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "learn.fit_one_cycle(5, lr_max=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_valid_pred, y_valid_true = learn.get_preds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if do_test:\n",
    "    y_test_pred, _ = learn.get_preds(dl=test_dl)\n",
    "    y_test_pred = modelling.cnr(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_score = modelling.evaluate_torch(torch.tensor(y_valid_true), \n",
    "                                    torch.tensor(y_valid_pred)).item()\n",
    "print(f'fastai loss {nb_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of  `dep_var`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_testing.hist_plot_preds(modelling.pick_random(y_valid_true), \n",
    "                                modelling.pick_random(y_valid_pred), \n",
    "                                label0='truth', label1='prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_test:\n",
    "    feature_testing.hist_plot_preds(modelling.pick_random(y_valid_true), \n",
    "                                    modelling.pick_random(y_test_pred), \n",
    "                                    label0='truth (validation)', \n",
    "                                    label1='prediction (test set)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidently wrong predictions by `building_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "miss_cols = [v for v in ['building_id', 'meter','timestamp'] if v not in to.valid.xs.columns]\n",
    "tmp = to.valid.xs.join(df.loc[:,miss_cols]) if len(miss_cols)>0 else to.valid.xs\n",
    "bwt = feature_testing.BoldlyWrongTimeseries(tmp, y_valid_true, y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwt.run_boldly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if do_test and do_submit:\n",
    "    y_test_pred_original = torch.exp(tensor(y_test_pred)) - 1\n",
    "\n",
    "    y_out = pd.DataFrame(cnr(y_test_pred_original),\n",
    "                         columns=['meter_reading'],\n",
    "                         index=df_test.index)\n",
    "    display(y_out.head())\n",
    "\n",
    "    assert len(y_out) == 41697600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if do_submit:\n",
    "    y_out.to_csv(data_path/'my_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# message = ['random forest', '500 obs/bid', 'all features', f'nb score {nb_score:.4f}']\n",
    "message = ['lightgbm', '500 obs/bid', '100 rounds', '42 leaves', 'lr .5', f'nb score {nb_score:.4f}']\n",
    "# message = ['tabular_learner', '500 obs/bid', 'all features', f'layers {layers}, embed_p .1, ps [.1,.1,.1]', f'nb score {nb_score:.4f}']\n",
    "message = ' + '.join(message)\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_test and do_submit:\n",
    "    print('Submitting...')\n",
    "    !kaggle competitions submit -c ashrae-energy-prediction -f '{data_path}/my_submission.csv' -m '{message}'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38_fastai]",
   "language": "python",
   "name": "conda-env-py38_fastai-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
