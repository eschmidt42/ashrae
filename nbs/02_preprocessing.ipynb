{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data\n",
    "\n",
    "> Inspecting any particular irregularities and general preparation of the data for modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import typing\n",
    "import pickle\n",
    "\n",
    "from sklearn import linear_model, tree, model_selection, ensemble\n",
    "\n",
    "from fastai.tabular.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.plotting.backend = \"plotly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvs = sorted([base_path/v for v in os.listdir(base_path) if v.endswith('.csv')])\n",
    "csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = csvs[3]\n",
    "train_weather_csv = csvs[-1]\n",
    "test_csv = csvs[2]\n",
    "test_weather_csv = csvs[-2]\n",
    "meta_csv = csvs[0]\n",
    "\n",
    "train_csv, train_weather_csv, test_csv, test_weather_csv, meta_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train = pd.read_csv(train_csv, parse_dates=['timestamp'])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test = pd.read_csv(test_csv, parse_dates=['timestamp'])\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "weather_train = pd.read_csv(train_weather_csv, parse_dates=['timestamp'])\n",
    "weather_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "weather_test = pd.read_csv(test_weather_csv, parse_dates=['timestamp'])\n",
    "weather_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "building = pd.read_csv(meta_csv)\n",
    "building.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspection of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kicking out outlying measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_meter_stats = (train.groupby(['meter'])['meter_reading']\n",
    "                     .describe(percentiles=[.05, .25, .5, .75, .95]))\n",
    "train_meter_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "mask = pd.concat([(train['meter']==m) & (train['meter_reading'] < 1e2*grp['95%'].iloc[0])\n",
    "         for m, grp in train_meter_stats.groupby(['meter'])], axis=1).any(axis=1)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.sum() / len(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_meter_stats['95%'] == 0 ).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: there are some building id and meter combinations with timeseries essentially only consisting of the value zero $\\rightarrow$ how to clean up? \n",
    "\n",
    "TODO: find out if 0s are trailing, leading or randomly intermittent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "share_of_zeros = lambda x: (x==0).sum()/len(x)\n",
    "df_zeros = (train.groupby(['building_id', 'meter'])['meter_reading']\n",
    "            .apply(share_of_zeros)\n",
    "            .sort_values()\n",
    "            .to_frame()\n",
    "            .rename(columns={'meter_reading':'fraction 0s'}))\n",
    "df_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_zeros['ix'] = df_zeros.groupby(['meter']).cumcount()\n",
    "df_zeros.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(df_zeros.reset_index(level=1), x='ix', y='fraction 0s', color='meter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radically merging all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def radical_merging(df:pd.DataFrame, building:pd.DataFrame, \n",
    "                    weather:pd.DataFrame, n_sample:int=None,\n",
    "                    training:bool=True):\n",
    "    \n",
    "    tmp = df.copy(deep=True)\n",
    "\n",
    "    bid_col = 'building_id'\n",
    "    sid_col = 'site_id'\n",
    "    time_col = 'timestamp'\n",
    "    target_col = 'meter_reading'\n",
    "    \n",
    "    categorical = ['meter', 'primary_use', 'cloud_coverage', bid_col, sid_col]\n",
    "    continuous = ['square_feet', 'year_built', 'floor_count', \n",
    "                  'air_temperature', 'dew_temperature',\n",
    "                  'precip_depth_1_hr', 'sea_level_pressure', 'wind_direction',\n",
    "                  'wind_speed']\n",
    "\n",
    "    x_cols = [bid_col, 'meter', target_col, time_col] if training \\\n",
    "            else [bid_col, 'meter', time_col]\n",
    "    X = tmp.loc[:,x_cols].copy()\n",
    "\n",
    "    X = pd.merge(X, building, on=bid_col, how='left')\n",
    "    X = pd.merge(X, weather, on=[sid_col, time_col], how='left')\n",
    "\n",
    "    #return_cols =  categorical + continuous + [target_col,]  # time_col\n",
    "\n",
    "    #X = X.loc[:,return_cols]\n",
    "    if n_sample is not None:\n",
    "        X = X.sample(n_sample)\n",
    "        \n",
    "    if training:\n",
    "        X[target_col] = np.log(X[target_col] + 1)\n",
    "        \n",
    "    X = add_datepart(X, time_col)\n",
    "    categorical.extend(['timestampMonth', 'timestampWeek', 'timestampDay',\n",
    "                        'timestampDayofweek', 'timestampDayofyear', 'timestampIs_month_end',\n",
    "                        'timestampIs_month_start', 'timestampIs_quarter_end',\n",
    "                        'timestampIs_quarter_start', 'timestampIs_year_end',\n",
    "                        'timestampIs_year_start'])\n",
    "    \n",
    "    continuous.extend(['timestampYear', 'timestampElapsed'])\n",
    "        \n",
    "    X = X.loc[:, [col for col in X.columns.values if col not in [time_col]]]\n",
    "    \n",
    "    missing_cont = [col for col in continuous if col not in X.columns]\n",
    "    missing_cat = [col for col in categorical if col not in X.columns]\n",
    "    assert len(missing_cat) == 0, f'{missing_cat} not in X!'\n",
    "    assert len(missing_cont) == 0, f'{missing_cont} not in X!'\n",
    "    \n",
    "    X.loc[:,continuous] = X.loc[:,continuous].astype(float)\n",
    "    X.loc[:,categorical] = X.loc[:,categorical].astype('category')\n",
    "    \n",
    "    return X, continuous, categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating train / validate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_sample = None  #10000\n",
    "X, continuous, categorical = radical_merging(train.copy(), building, weather_train,\n",
    "                    n_sample=n_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X.to_parquet('../data/X.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing variable types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/var_types.pckl', 'wb') as f:\n",
    "    pickle.dump({'cont':continuous, 'cat':categorical}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating test set features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_test, _, _ = radical_merging(test.copy(), building, weather_test,\n",
    "                    n_sample=None, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_parquet('../data/X_test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-py38_fastai]",
   "language": "python",
   "name": "conda-env-.conda-py38_fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
