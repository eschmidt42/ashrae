{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling & submitting\n",
    "\n",
    "> Playing with different models and submitting predictions over the test set to kaggle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current implementation of this notebook leads to (private leaderboard score): \n",
    "- baseline (linear regression on dep_var_stats and meter 1hot) 1.7\n",
    "- RandomForest, tabular_learner, lgbm at ~1.45, \n",
    "- ensembling tabular_learner, RandomForest and lgbm gives ~1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from ashrae import loading, preprocessing, feature_testing\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import typing\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn import linear_model, tree, model_selection, ensemble\n",
    "\n",
    "from fastai.tabular.all import *\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.plotting.backend = \"plotly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_test = True\n",
    "do_submit = False\n",
    "data_path = loading.DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loading.N_TRAIN = 10_000\n",
    "loading.N_TEST = 10_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ashrae_data = loading.load_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes about 3min30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = preprocessing.Processor() # t_train=t_train\n",
    "tfms_config = {\n",
    "#     'fix_bid_363':{},\n",
    "#     'fix_bid_1099':{'threshold': 10.},\n",
    "#     'remove_bad_meter0_readings_of_first_141days': {},\n",
    "#     'remove_not_summer_0s_meter_2_and_3': {},\n",
    "#     'remove_0s_meter0': {},\n",
    "#     'remove_outliers':{'f':10,'dep_var':'meter_reading'},\n",
    "#     'remove_imputed_weeks':{'dep_var':'meter_reading'},\n",
    "#     'add_dep_var_stats':{},\n",
    "    'add_random_noise_features':{},\n",
    "    'add_time_features':{},\n",
    "    'add_weather_features':{'fix_time_offset':True,\n",
    "                            'add_na_indicators':True,\n",
    "                            'impute_nas':True},\n",
    "    'add_building_features':{},\n",
    "#     'add_onehot_encoded':{},\n",
    "}\n",
    "\n",
    "df, var_names = processor(ashrae_data['meter_train'], tfms_configs=tfms_config,\n",
    "                          df_weather=ashrae_data['weather_train'],\n",
    "                          df_building=ashrae_data['building'])\n",
    "display(df.head(), var_names)\n",
    "\n",
    "%time\n",
    "df_test, _ = processor(ashrae_data['meter_test'], tfms_configs=tfms_config,\n",
    "                         df_weather=ashrae_data['weather_test'],\n",
    "                         df_building=ashrae_data['building'])\n",
    "df_test = preprocessing.align_test(df, var_names, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# df.to_parquet(data_path/'X.parquet')\n",
    "# df_test.to_parquet(data_path/'X_test.parquet')\n",
    "# pickle.dump(var_names, open(data_path/'var_names.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def evaluate_torch(y_true:torch.Tensor, y_pred:torch.Tensor): return torch.sqrt(F.mse_loss(y_true, y_pred))\n",
    "\n",
    "cnr = lambda x: x.clone().numpy().ravel() # clone numpy ravel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# var_names = preprocessing.load_var_names(data_path/'var_names.pckl')\n",
    "# var_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# df = preprocessing.load_df(data_path/'X.parquet') #.sample(100000)\n",
    "\n",
    "# if do_test:\n",
    "#     df_test = preprocessing.load_df(data_path/'X_test.parquet') #.sample(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df), len(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling `df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n = len(df)\n",
    "\n",
    "if False: # per building_id and meter sampling\n",
    "    n_sample_per_bid = 500\n",
    "    replace = True\n",
    "\n",
    "    df = (df.groupby(['building_id', 'meter'])\n",
    "         .sample(n=n_sample_per_bid, replace=replace))\n",
    "\n",
    "if False: # general sampling\n",
    "    frac_samples = .1\n",
    "    replace = False\n",
    "\n",
    "    df = (df.sample(frac=frac_samples, replace=replace))\n",
    "\n",
    "print(f'using {len(df)} samples = {len(df)/n*100:.2f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# t_train = pd.read_parquet(data_path/'t_train.parquet')\n",
    "t_train = None\n",
    "\n",
    "%time\n",
    "#split_kind = 'random'\n",
    "#split_kind = 'time'\n",
    "# split_kind = 'fix_time'\n",
    "split_kind = 'time_split_day'\n",
    "train_frac = .9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = preprocessing.split_dataset(df, split_kind=split_kind, train_frac=train_frac,\n",
    "                                     t_train=t_train)\n",
    "print(f'sets {len(splits)}, train {len(splits[0])} = {len(splits[0])/len(df):.4f}, valid {len(splits[1])} = {len(splits[1])/len(df):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names_no_anns = {\n",
    "    'dep_var': var_names['dep_var'],\n",
    "    'conts': [v for v in var_names['conts'] if 'meter_reading' in v],\n",
    "    'cats': [v for v in var_names['cats'] if v.startswith('meter_')]\n",
    "}\n",
    "var_names_no_anns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names_anns = {\n",
    "    'dep_var': var_names['dep_var'],\n",
    "    'conts': var_names['conts'],\n",
    "    'cats': [v for v in var_names['cats'] if not v.startswith('meter_')]\n",
    "}\n",
    "var_names_anns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes about 6 minutes on 100% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "procs = [Categorify, FillMissing, Normalize]\n",
    "to = feature_testing.get_tabular_object(df,\n",
    "                                        var_names,\n",
    "                                        splits=splits,\n",
    "                                        procs=procs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing `to`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# pickle.dump(to, open(data_path/'to.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading `to`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# to = pickle.load(open(data_path/'to.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_bs = 100\n",
    "val_bs = 100\n",
    "\n",
    "dls = to.dataloaders(bs=train_bs, val_bs=val_bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# torch.save(dls, data_path/'dls.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: Takes about 14min with the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_bs = 100\n",
    "\n",
    "if do_test:\n",
    "    test_dl = dls.test_dl(df_test, bs=test_bs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# torch.save(test_dl, data_path/'test_dl.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# dls = torch.load(data_path/'dls.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# test_dl = torch.load(data_path/'test_dl.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `fastai`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fastai finding: make sure your test set values are not out of domain $\\Rightarrow$ `timestampYear` in this notebook is put into the training set but there only takes on the value 2016.0, but in the test set it's 2017.0 and 2018.0, causing the predictions to zero out everywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min([to.train.ys.values.min(), to.valid.ys.values.min()]), np.max([to.train.ys.values.max(), to.valid.ys.values.max()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_range = (-.1, 17)\n",
    "\n",
    "# layers = [4000, 2000, 1000, 500, 250]\n",
    "layers = [50, 20] # [1600, 800, 400, 200]\n",
    "\n",
    "# embed_p = .01\n",
    "embed_p = 0.\n",
    "\n",
    "# ps = [.1, .1, .1, .1, .1]\n",
    "ps = [.0 for _ in layers]\n",
    "# ps[0] = .2\n",
    "\n",
    "# config = None\n",
    "config = tabular_config(embed_p=embed_p, ps=ps,\n",
    "#                         act_cls=Swish(inplace=True)\n",
    "                        )\n",
    "# config = tabular_config(act_cls=nn.ReLU(inplace=True))\n",
    "# config = tabular_config(act_cls=Swish(inplace=True))\n",
    "# config = tabular_config(act_cls=Sine(inplace=True))\n",
    "\n",
    "learn = tabular_learner(dls, y_range=y_range, \n",
    "                        layers=layers, n_out=1, \n",
    "                        config=config, \n",
    "#                         wd=.01,\n",
    "                        loss_func=evaluate_torch) #.to_fp16()\n",
    "run = -1 # a counter for `fit_one_cycle` executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# learn.save('1600-800-400-200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# learn = learn.load('1600-800-400-200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run += 1\n",
    "print(f'run #{run}')\n",
    "learn.fit_one_cycle(5, lr_max=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_valid_pred, y_valid_true = learn.get_preds()\n",
    "y_valid_pred_fast = cnr(y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_score = evaluate_torch(y_valid_true, \n",
    "                          y_valid_pred).item()\n",
    "print(f'fastai loss {nb_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if do_test:\n",
    "    y_test_pred, _ = learn.get_preds(dl=test_dl)\n",
    "    y_test_pred_fast = cnr(y_test_pred)\n",
    "    y_test_pred = cnr(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_pred, y_valid_true = cnr(y_valid_pred), cnr(y_valid_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "replacing categorical features for trees with learned embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees_with_embeddings = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if trees_with_embeddings:\n",
    "    X_emb_train = to.train.xs.copy()\n",
    "    X_emb_val = to.valid.xs.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EmbeddingFeatures:\n",
    "    def __init__(self, to:TabularPandas, learn:Learner):        \n",
    "        self.df_embs = {col: self.get_embedding_features_df(col, to, learn) for col in to.cat_names}\n",
    "\n",
    "    def get_embedding_features_df(self, col:str, to:TabularPandas, learn:Learner): # , to:TabularPandas, learn:Learner\n",
    "        ix = to.cat_names.index(col)\n",
    "        w_emb = learn.model.embeds[ix].weight.cpu().detach().clone().numpy()\n",
    "        df_emb = (pd.DataFrame(w_emb)\n",
    "                  .add_prefix(f'{col}_embedding_'))\n",
    "        df_emb.index.rename(col, inplace=True)\n",
    "        return df_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ef = EmbeddingFeatures(to, learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ef.df_embs['building_id'].iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def replace_cat_features_with_embeddings(self:EmbeddingFeatures, X:pd.DataFrame):\n",
    "    for col, df_emb in self.df_embs.items():\n",
    "        X = X.join(df_emb, on=col, how='left').drop(columns=col)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: fix memory error in the creation of `X_emb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if trees_with_embeddings:\n",
    "    X_emb_train = ef.replace_cat_features_with_embeddings(X_emb_train)\n",
    "    X_emb_val = ef.replace_cat_features_with_embeddings(X_emb_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "params = {'n_estimators': 20, 'max_features': 'sqrt', 'n_jobs':1}\n",
    "model = ensemble.RandomForestRegressor\n",
    "# params = {}\n",
    "# model = linear_model.LinearRegression\n",
    "\n",
    "m = model(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if trees_with_embeddings:\n",
    "    m.fit(X_emb_train, to.train.ys.values.ravel())\n",
    "else:\n",
    "    m.fit(to.train.xs.values, to.train.ys.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def embedding_assignment_func(self:EmbeddingFeatures, stuff:tuple):\n",
    "    k, grp = stuff\n",
    "    grp.drop(columns=['group'], inplace=True)\n",
    "    grp = self.replace_cat_features_with_embeddings(grp)\n",
    "    return pd.Series(m.predict(grp.values), index=grp.index)\n",
    "\n",
    "@patch\n",
    "def predict_with_embeddings(self:EmbeddingFeatures, X:pd.DataFrame, m,\n",
    "                            num_rows:int=2_000_000, num_workers:int=1):\n",
    "    tmp = X.copy()\n",
    "    tmp['group'] = np.floor(tmp.index.values / num_rows).astype(int)\n",
    "    n = int(np.ceil(len(tmp)/num_rows))\n",
    "    y_test_pred = []\n",
    "    \n",
    "    if num_workers > 1:\n",
    "        pool = Pool(processes=num_workers)\n",
    "        for grp in tqdm.tqdm(pool.imap(self.embedding_assignment_func, tmp.groupby('group')), total=n):\n",
    "            y_test_pred.append(grp)\n",
    "        pool.join()\n",
    "        pool.close()\n",
    "    else:\n",
    "        for stuff in tqdm.tqdm(tmp.groupby('group'), total=n):\n",
    "            y_test_pred.append(self.embedding_assignment_func(stuff))\n",
    "    \n",
    "    y_test_pred = pd.concat(y_test_pred)\n",
    "    display(y_test_pred.head(), y_test_pred.tail())\n",
    "    return y_test_pred.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if trees_with_embeddings:\n",
    "    y_valid_pred = m.predict(X_emb_val)\n",
    "else:\n",
    "    y_valid_pred = m.predict(to.valid.xs.values)\n",
    "\n",
    "y_valid_pred_sk = np.copy(y_valid_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: the prediction with embeddings takes ~ 37 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if do_test:\n",
    "    if trees_with_embeddings:\n",
    "        y_test_pred = ef.predict_with_embeddings(test_dl.xs, m.set_params(n_jobs=-1), \n",
    "                                                 num_workers=1)\n",
    "    else:\n",
    "        y_test_pred = m.predict(test_dl.xs)\n",
    "    y_test_pred_sk = np.copy(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_true = to.valid.ys.values.ravel()\n",
    "nb_score = evaluate_torch(torch.from_numpy(y_valid_true), \n",
    "                          torch.from_numpy(y_valid_pred)).item()\n",
    "print(f'sklearn loss {nb_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `lightgbm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if trees_with_embeddings:\n",
    "    lgb_train = lgb.Dataset(X_emb_train, to.train.ys.values.ravel())\n",
    "    lgb_eval = lgb.Dataset(X_emb_val, to.valid.ys.values.ravel(), \n",
    "                           reference=lgb_train)\n",
    "else:\n",
    "    lgb_train = lgb.Dataset(to.train.xs.values, to.train.ys.values.ravel())\n",
    "    lgb_eval = lgb.Dataset(to.valid.xs.values, to.valid.ys.values.ravel(), \n",
    "                           reference=lgb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'l2',\n",
    "    'num_leaves': 42,\n",
    "    'learning_rate': 0.5,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gbm = lgb.train(params, lgb_train,\n",
    "                num_boost_round=10,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if trees_with_embeddings:\n",
    "    y_valid_pred_lgbm = gbm.predict(X_emb_val,\n",
    "                                    num_iteration=gbm.best_iteration)\n",
    "else:\n",
    "    y_valid_pred_lgbm = gbm.predict(to.valid.xs.values,\n",
    "                                    num_iteration=gbm.best_iteration)\n",
    "\n",
    "y_valid_pred = np.copy(y_valid_pred_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if do_test:\n",
    "    if trees_with_embeddings:\n",
    "        y_test_pred_lgbm = gbm.predict(X_emb_test,\n",
    "                                       num_iteration=gbm.best_iteration)\n",
    "    else:\n",
    "        y_test_pred_lgbm = gbm.predict(test_dl.xs.values,\n",
    "                                       num_iteration=gbm.best_iteration)\n",
    "    y_test_pred = np.copy(y_test_pred_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_true = to.valid.ys.values.ravel()\n",
    "nb_score = evaluate_torch(torch.from_numpy(y_valid_true), \n",
    "                          torch.from_numpy(y_valid_pred)).item()\n",
    "print(f'lgbm loss {nb_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_pred = np.mean([\n",
    "    y_valid_pred_sk, \n",
    "    y_valid_pred_fast,\n",
    "    y_valid_pred_lgbm\n",
    "], axis=0)\n",
    "y_valid_true = to.valid.ys.values.ravel()\n",
    "nb_score = evaluate_torch(torch.from_numpy(y_valid_true), \n",
    "                          torch.from_numpy(y_valid_pred)).item()\n",
    "print(f'ensembling loss {nb_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if do_test:\n",
    "    y_test_pred = np.mean([\n",
    "        y_test_pred_sk, \n",
    "        y_test_pred_fast, \n",
    "        y_test_pred_lgbm\n",
    "    ], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `dep_var` distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train vs validation distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def pick_random(x,s:int=50): return np.random.choice(x.ravel(), size=s, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_testing.hist_plot_preds(pick_random(y_valid_true), \n",
    "                                pick_random(y_valid_pred), \n",
    "                                label0='truth', label1='prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_test:\n",
    "    feature_testing.hist_plot_preds(pick_random(y_valid_true), \n",
    "                                    pick_random(y_test_pred), \n",
    "                                    label0='truth (validation)', \n",
    "                                    label1='prediction (test set)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boldly wrong predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "miss_cols = [v for v in ['building_id', 'meter','timestamp'] if v not in to.valid.xs.columns]\n",
    "tmp = to.valid.xs.join(df.loc[:,miss_cols]) if len(miss_cols)>0 else to.valid.xs\n",
    "bwt = feature_testing.BoldlyWrongTimeseries(tmp, y_valid_true, y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwt.run_boldly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission to kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if do_test:\n",
    "    y_test_pred_original = torch.exp(tensor(y_test_pred)) - 1\n",
    "\n",
    "    y_out = pd.DataFrame(cnr(y_test_pred_original),\n",
    "                         columns=['meter_reading'],\n",
    "                         index=test_dl.xs.index)\n",
    "    display(y_out.head())\n",
    "\n",
    "    assert len(y_out) == 41697600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Writing to csv takes ~2min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if do_submit:\n",
    "    y_out.to_csv(data_path/'my_submission.csv',\n",
    "                 float_format='%.4f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`kaggle competitions submit -c ashrae-energy-prediction -f submission.csv -m \"Message\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def pretty_dictionary(d:dict): return ', '.join(f'{k} = {v}' for k,v in d.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act = 'ReLu'\n",
    "\n",
    "# lin_model_msg = f'baseline (linear regression)'\n",
    "rf_model_msg = f'RandomForest: {pretty_dictionary(params)}'\n",
    "if trees_with_embeddings:\n",
    "    rf_model_msg += ' (with embeddings)'\n",
    "lgbm_model_msg = f'LGBM: {pretty_dictionary(params)}'\n",
    "\n",
    "fast_model_msg = f'tabular_learner (run #{run}): act {act}, layers {layers}, ps {ps}, embed_p {embed_p}'\n",
    "\n",
    "# model_msg = f'Ensembling tabular_learner and RandomForest ({fast_model_msg}, {rf_model_msg})'\n",
    "model_msg = f'Ensembling LGBM, tabular_learner and RandomForest ({lgbm_model_msg}, {fast_model_msg}, {rf_model_msg})'\n",
    "# model_msg = rf_model_msg\n",
    "# model_msg = lgbm_model_msg\n",
    "\n",
    "split_msg = f'split kind \"{split_kind}\" N_TRAIN {loading.N_TRAIN}'\n",
    "samples_msg = f'num samples {len(dls.xs)} = {len(dls.xs)/20216100/2*100:.2f} %'\n",
    "features_msg = f'weather and building features'\n",
    "score_msg = f'nb score {nb_score:.4f}'\n",
    "# message = ['baseline (linear regression on dep_var_stats and 1hot meter) ', '500 obs/bid', f'nb score {nb_score:.4f}']\n",
    "# message = ['random forest', '500 obs/bid', 'all features', f'nb score {nb_score:.4f}']\n",
    "# message = ['lightgbm', '500 obs/bid', '100 rounds', '42 leaves', 'lr .5', f'nb score {nb_score:.4f}']\n",
    "# message = ['tabular_learner', '500 obs/bid', 'all features', f'layers {layers}, embed_p .1, ps [.1,.1,.1]', f'nb score {nb_score:.4f}']\n",
    "message = ' + '.join([model_msg, samples_msg, split_msg, features_msg, score_msg])\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_test and do_submit:\n",
    "    print('Submitting...')\n",
    "    !kaggle competitions submit -c ashrae-energy-prediction -f '{data_path}/my_submission.csv' -m '{message}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38_fastai]",
   "language": "python",
   "name": "conda-env-py38_fastai-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
