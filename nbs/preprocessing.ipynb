{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data\n",
    "\n",
    "> Preprocessing data for the modelling step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import typing\n",
    "import pickle\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from sklearn import linear_model, tree, model_selection, ensemble\n",
    "from ashrae import loading, inspection\n",
    "from fastai.tabular.all import *\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from sklearn import linear_model, tree, model_selection, ensemble\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import itertools\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as us_calendar\n",
    "\n",
    "import math\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.plotting.backend = \"plotly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_var = 'meter_reading'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = loading.DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to None if the pre-processing is supposed to be done for all samples, otherwise choose a number of samples\n",
    "n_samples_quick = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ashrae_data = loading.load_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def split_dataset(X:pd.DataFrame, split_kind:str='random',\n",
    "                  train_frac:float=8, t_train:pd.DataFrame=None):\n",
    "\n",
    "    def random_split():\n",
    "        n_train = int(len(X)*train_frac)\n",
    "        train_bool = X.index.isin(np.random.choice(X.index.values, size=n_train, replace=False))\n",
    "        return train_bool\n",
    "\n",
    "    def time_split():\n",
    "        assert 'timestamp' in X.columns\n",
    "        time_col = 'timestamp'\n",
    "        ts = X[time_col].sort_values(ascending=True)\n",
    "        ix = int(len(X)*train_frac)\n",
    "        threshold_t = ts.iloc[ix:].values[0]\n",
    "        return X[time_col] < threshold_t\n",
    "\n",
    "    def time_split_day():\n",
    "        time_col = 'timestampDayofyear'\n",
    "\n",
    "        if time_col not in X.columns:\n",
    "            t = X['timestamp'].dt.dayofyear\n",
    "        else:\n",
    "            t = X[time_col]\n",
    "\n",
    "        days = (t.value_counts()\n",
    "                .rename('count')\n",
    "                .sample(frac=1)\n",
    "                .to_frame()\n",
    "                .cumsum()\n",
    "                .pipe(lambda x: x.loc[x['count'] <= (train_frac * len(t))]))\n",
    "\n",
    "        num_train_days = len(days)\n",
    "        mask = t.isin(days.index.values)\n",
    "\n",
    "        assert mask.sum() > 0\n",
    "        return mask\n",
    "\n",
    "    def fix_time_split():\n",
    "        assert t_train is not None\n",
    "        time_col = 'timestamp'\n",
    "        assert time_col in X.columns\n",
    "\n",
    "        mask = X[time_col].isin(t_train[time_col])\n",
    "        assert mask.sum() > 0\n",
    "        return mask\n",
    "\n",
    "    split_funs = {\n",
    "        'random': random_split,\n",
    "        'time': time_split,\n",
    "        'fix_time': fix_time_split,\n",
    "        'time_split_day': time_split_day,\n",
    "    }\n",
    "\n",
    "    assert split_kind in split_funs\n",
    "    train_bool = split_funs[split_kind]()\n",
    "\n",
    "    train_idx = np.where(train_bool)[0]\n",
    "    valid_idx = np.where(~train_bool)[0]\n",
    "\n",
    "    return (list(train_idx), list(valid_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#split_kind = 'random'\n",
    "#split_kind = 'time'\n",
    "# split_kind = 'fix_time'\n",
    "split_kind = 'time_split_day'\n",
    "\n",
    "t_train = None\n",
    "train_frac = .8\n",
    "splits = split_dataset(ashrae_data['meter_train'], split_kind=split_kind, train_frac=train_frac,\n",
    "                       t_train=t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'sets {len(splits)}, train {len(splits[0])} = {len(splits[0])/len(ashrae_data[\"meter_train\"]):.4f}, valid {len(splits[1])} = {len(splits[1])/len(ashrae_data[\"meter_train\"]):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_train = ashrae_data['meter_train'].iloc[splits[0]][['timestamp']]\n",
    "t_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t_train.to_parquet(data_path/'t_train.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tmp = pd.concat((ashrae_data['meter_train'].iloc[splits[0]]\n",
    "                 .assign(label='train')\n",
    "                 .assign(meter_reading=lambda x: np.log(x['meter_reading']+1)),\n",
    "                (ashrae_data['meter_train'].iloc[splits[1]]\n",
    "                 .assign(label='valid')\n",
    "                 .assign(meter_reading=lambda x: np.log(x['meter_reading']+1)))), \n",
    "                axis=0, ignore_index=True)\n",
    "tmp.groupby('label').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(tmp.sample(10000), x='timestamp', y='meter_reading', color='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cleaning: https://www.kaggle.com/purist1024/ashrae-simple-data-cleanup-lb-1-08-no-leaks\n",
    "* remove all 0s for meter 0\n",
    "* remove all 0s for meter 2 and 3 if not summer\n",
    "* potentially remove 0s for meter 1 during winter\n",
    "* remove \"known-bad\" electrical readings from the first 141 days of the data for site 0 (i.e. UCF)\n",
    "* remove most absurdly high readings from building 1099. These are orders of magnitude higher than all data, and have been emperically seen in LB probes to be harmful outliers.\n",
    "* time time zone for weather data\n",
    "* impute nas for weather data\n",
    "* convert cyclic features, like hour, to  2d features (sin,cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "DEP_VAR = 'meter_reading'\n",
    "TIME_COL = 'timestamp'\n",
    "\n",
    "class Processor:\n",
    "\n",
    "    dep_var_stats:dict = None\n",
    "    \n",
    "    def __init__(self, dep_var:str=None, time_col:str=None,\n",
    "                 t_train:pd.Series=None):\n",
    "        self.dep_var = DEP_VAR if dep_var is None else dep_var\n",
    "        self.time_col = TIME_COL if time_col is None else time_col\n",
    "        self.t_train = t_train\n",
    "    \n",
    "    def __call__(self, df_core:pd.DataFrame, df_building:pd.DataFrame=None,\n",
    "                 df_weather:pd.DataFrame=None, t_train:pd.DataFrame=None,\n",
    "                 tfms_configs:dict=None) -> (pd.DataFrame, dict):\n",
    "\n",
    "        # check if `df` is a test set (dep_var is missing)\n",
    "        self.is_train = self.dep_var in df_core.columns\n",
    "        self.df_core = df_core.copy()\n",
    "        self.conts, self.cats, self.cats_order = [], [], {}\n",
    "        self.cats += ['building_id', 'meter']\n",
    "        self.n = len(df_core)\n",
    "\n",
    "        # core pieces of dependent and independent variables\n",
    "        self.update_dep_var()\n",
    "\n",
    "        if tfms_configs is None:\n",
    "            logger.info('Empty transform configs `tfms_configs`. Returning ...')\n",
    "        else:\n",
    "            # if `t_train` (timestamps which belong to the training set) are provided perform a check which rows are effected\n",
    "            if self.t_train is not None:\n",
    "                self.t_train_set = set(self.t_train.values.ravel())\n",
    "\n",
    "            self.df_building = df_building.copy() if df_building is not None else None\n",
    "            self.df_weather = df_weather.copy() if df_weather is not None else None\n",
    "\n",
    "            # running transformations\n",
    "            self.sanity_check_input_for_tfms(tfms_configs)\n",
    "            for fun_name, config in tfms_configs.items():\n",
    "                self.df_core = getattr(self, fun_name)(**config)\n",
    "\n",
    "        df_core, var_names = self.cleanup()\n",
    "        logger.info(f'Reduced samples by {self.n - len(df_core)} rows = {(self.n - len(df_core))/self.n*100:.2f} %')\n",
    "        return df_core, var_names\n",
    "\n",
    "    @property\n",
    "    def t_in_train_set(self):\n",
    "        return self.df_core['timestamp'].isin(self.t_train_set)\n",
    "\n",
    "    def update_dep_var(self) -> pd.DataFrame:\n",
    "\n",
    "        if self.dep_var.endswith('log1p'):\n",
    "            return self.df_core\n",
    "\n",
    "        dep_var_new = f'{self.dep_var}_log1p'\n",
    "        if self.is_train:\n",
    "            self.df_core[dep_var_new] = np.log(self.df_core[self.dep_var].values + 1) # 3s\n",
    "#             self.df_core[dep_var_new] = self.df_core[self.dep_var].apply(lambda x: math.log(x+1)) # 12s with math.log, 27s with np.log\n",
    "#             self.df_core[dep_var_new] = self.df_core[self.dep_var].swifter.apply(lambda x: math.log(x+1)) # 15s with math.log + swifter\n",
    "        self.dep_var = dep_var_new\n",
    "        return self.df_core\n",
    "    \n",
    "    def sanity_check_input_for_tfms(self, tfms_configs:dict):\n",
    "        # sanity check presence of df_building if df_weather is given\n",
    "        if self.df_weather is not None:\n",
    "            assert self.df_building is not None, 'To join the weather info in `df_weather` you need to pass `df_building`.'\n",
    "\n",
    "        # making sure all required inputs are specified in `tfms_configs`\n",
    "        self.test_run = True\n",
    "        if tfms_configs is not None:\n",
    "            building_fun_names = ['add_building_features']\n",
    "            weather_fun_names = ['add_weather_features']\n",
    "            for fun_name, config in tfms_configs.items():\n",
    "                getattr(self, fun_name)(**config)\n",
    "                if fun_name in building_fun_names:\n",
    "                    assert self.df_building is not None, 'You need to pass `df_building` in Processor.__call__.'\n",
    "                if fun_name in weather_fun_names:\n",
    "                    assert self.df_weather is not None, 'You need to pass `df_weather` in Processor.__call__.'\n",
    "        self.test_run = False\n",
    "\n",
    "    def get_var_names(self) -> dict:\n",
    "        return {'conts': self.conts, 'cats': self.cats, 'dep_var': self.dep_var}\n",
    "\n",
    "    def cleanup(self) -> (pd.DataFrame, dict):\n",
    "        # converting cats to category type\n",
    "        for col in self.cats:\n",
    "            if self.df_core[col].dtype == bool: continue\n",
    "            self.df_core[col] = self.df_core[col].astype('category')\n",
    "            if col in self.cats_order:\n",
    "                self.df_core[col].cat.set_categories(self.cats_order[col],\n",
    "                                                     ordered=True, inplace=True)\n",
    "\n",
    "        # removing features\n",
    "        to_remove_cols = ['meter_reading', 'timestampYear'] # , self.time_col\n",
    "        self.df_core.drop(columns=[c for c in self.df_core.columns if c in to_remove_cols],\n",
    "                          inplace=True)\n",
    "\n",
    "        # shrinking the data frame\n",
    "        self.df_core = df_shrink(self.df_core, int2uint=True)\n",
    "\n",
    "        var_names = self.get_var_names()\n",
    "\n",
    "        if not self.is_train:\n",
    "            self.df_core.set_index('row_id', inplace=True)\n",
    "\n",
    "        missing_cols = [col for col in self.df_core.columns.values if col not in self.cats + self.conts + [self.dep_var]\n",
    "                        and col not in ['timestampElapsed', self.time_col, 'meter_reading']]\n",
    "\n",
    "        assert len(missing_cols) == 0, f'Missed to assign columns: {missing_cols} to `conts` or `cats`'\n",
    "\n",
    "        return self.df_core, var_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only applying the `dep_var` transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor()\n",
    "_df, _vars = processor(ashrae_data['meter_train'])\n",
    "display(_df.head(), _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(_df.groupby('meter').sample(10000), \n",
    "             x='meter_reading_log1p', facet_row='meter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "cols = ['building_id', 'meter', 'timestamp', 'meter_reading_log1p']\n",
    "assert len(_df.columns) == len(cols) and cols == list(_df.columns.values), f'Unexpected columns: {_df.columns} != {cols}'\n",
    "assert len(_vars) == 3 and len(_vars['conts']) == 0 and _vars['cats'] == ['building_id', 'meter'] and _vars['dep_var'] == 'meter_reading_log1p'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The meter readings for `building_id` 363 before July 30th are likely due to a construction phase since the bulding's year value is 2017. So this method removes the readings from during the construction time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def fix_bid_363(self:Processor):\n",
    "    if self.test_run: return\n",
    "    if not self.is_train: return self.df_core\n",
    "    assert 'timestamp' in self.df_core.columns\n",
    "    rm = (self.df_core['building_id'] == 363)\n",
    "    rm = rm & (self.df_core['meter'] == 0)\n",
    "    rm = rm & (self.df_core['timestamp'] < pd.to_datetime('2016-07-30'))\n",
    "    ok = ~rm\n",
    "    if self.is_train and self.t_train is not None:\n",
    "        ok = ok | ~self.t_in_train_set\n",
    "    logger.info(f'Fixing building_id 363: removing {(~ok).sum()} data points = {(~ok).sum()/len(ok)*100:.2f} %')\n",
    "    self.df_core = self.df_core.loc[ok,:]\n",
    "    return self.df_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor()\n",
    "tfms_config = {'fix_bid_363':{}}\n",
    "_df, _vars = processor(ashrae_data['meter_train'], tfms_configs=tfms_config)\n",
    "display(_df.head(), _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert len(_df) == len(ashrae_data['meter_train']) - 5063"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor(t_train=t_train)\n",
    "tfms_config = {'fix_bid_363':{}}\n",
    "_df, _vars = processor(ashrae_data['meter_train'], tfms_configs=tfms_config)\n",
    "display(_df.head(), _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert len(ashrae_data['meter_train']) - 5063 <= len(_df) < len(ashrae_data['meter_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "it = inspection.InspectTimeseries(_df, building=ashrae_data['building'],\n",
    "                                  weather=ashrae_data['weather_train'],\n",
    "                                  dep_var='meter_reading_log1p')\n",
    "it.inspect_boldly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seem to be quite a few imputed / filled values in the meter readings, being visible as constant meter readings for more than a week at a time. This method removes those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "it = inspection.InspectTimeseries(ashrae_data['meter_train'], building=ashrae_data['building'],\n",
    "                                  weather=ashrae_data['weather_train'])\n",
    "it.inspect_boldly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing all 0s for meter 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def remove_0s_meter0(self:Processor):\n",
    "    if self.test_run: return\n",
    "    if not self.is_train: return self.df_core\n",
    "    assert 'timestamp' in self.df_core.columns\n",
    "    rm = (self.df_core['meter'] == 0)\n",
    "    rm = rm & (self.df_core[self.dep_var] == 0)\n",
    "    ok = ~rm\n",
    "    if self.is_train and self.t_train is not None:\n",
    "        ok = ok | ~self.t_in_train_set\n",
    "    logger.info(f'Removing 0s for meter 0: removing {(~ok).sum()} data points = {(~ok).sum()/len(ok)*100:.2f} %')\n",
    "    self.df_core = self.df_core.loc[ok,:]\n",
    "    return self.df_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor()\n",
    "tfms_config = {'remove_0s_meter0':{}}\n",
    "_df, _vars = processor(ashrae_data['meter_train'], tfms_configs=tfms_config)\n",
    "display(_df.head(), _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert len(_df) == len(ashrae_data['meter_train']) - 530169\n",
    "assert 0 in _df.loc[_df['meter']!=0, 'meter_reading_log1p'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "it = inspection.InspectTimeseries(_df, building=ashrae_data['building'],\n",
    "                                  weather=ashrae_data['weather_train'],\n",
    "                                  dep_var='meter_reading_log1p')\n",
    "it.inspect_boldly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing all 0s for meters 2 and 3 outside of summer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def remove_not_summer_0s_meter_2_and_3(self:Processor):\n",
    "    if self.test_run: return\n",
    "    if not self.is_train: return self.df_core\n",
    "    assert 'timestamp' in self.df_core.columns\n",
    "    add_month = 'timestampMonth' not in self.df_core.columns\n",
    "    if add_month:\n",
    "        self.df_core['timestampMonth'] = self.df_core['timestamp'].dt.month\n",
    "    rm = (self.df_core['meter'].isin([2,3]))\n",
    "    rm = rm & (self.df_core[self.dep_var] == 0)\n",
    "    rm = rm & (self.df_core['timestampMonth'].isin([6,7,8]))\n",
    "    ok = ~rm\n",
    "    if self.is_train and self.t_train is not None:\n",
    "        ok = ok | ~self.t_in_train_set\n",
    "    logger.info(f'Removing 0s for meter 2 and 3 during summer: removing {(~ok).sum()} data points = {(~ok).sum()/len(ok)*100:.2f} %')\n",
    "    self.df_core = self.df_core.loc[ok,:]\n",
    "    if add_month:\n",
    "        self.df_core.drop(columns=['timestampMonth'],inplace=True)\n",
    "    return self.df_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor()\n",
    "tfms_config = {'remove_not_summer_0s_meter_2_and_3':{}}\n",
    "_df, _vars = processor(ashrae_data['meter_train'], tfms_configs=tfms_config)\n",
    "display(_df.head(), _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert len(_df) == len(ashrae_data['meter_train']) - 253743"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "it = inspection.InspectTimeseries(_df, building=ashrae_data['building'],\n",
    "                                  weather=ashrae_data['weather_train'],\n",
    "                                  dep_var='meter_reading_log1p')\n",
    "it.inspect_boldly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing \"bad\" electrical (meter 0) readings from the first 141 days of site 0 (i.e. UCF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def remove_bad_meter0_readings_of_first_141days(self:Processor):\n",
    "    if self.test_run: return\n",
    "    if not self.is_train: return self.df_core\n",
    "    assert 'timestamp' in self.df_core.columns\n",
    "    add_month = 'timestampDayofyear' not in self.df_core.columns\n",
    "    if add_month:\n",
    "        self.df_core['timestampDayofyear'] = self.df_core['timestamp'].dt.dayofyear\n",
    "    add_site_id = 'site_id' not in self.df_core.columns\n",
    "    if add_site_id:\n",
    "        assert self.df_building is not None, 'df_building cannot be None for this method.'\n",
    "        self.df_core = pd.merge(self.df_core, self.df_building.loc[:,['building_id','site_id']], on='building_id', how='left')\n",
    "        assert self.df_core['site_id'].isna().sum() == 0\n",
    "    rm = self.df_core['meter'] == 0\n",
    "    rm = rm & (self.df_core['site_id'] == 0)\n",
    "    rm = rm & (self.df_core['timestampDayofyear'] < 141)\n",
    "    ok = ~rm\n",
    "    if self.is_train and self.t_train is not None:\n",
    "        ok = ok | ~self.t_in_train_set\n",
    "    logger.info(f'Bad readings for meter 0 for the first 141 days for site 0: removing {(~ok).sum()} data points = {(~ok).sum()/len(ok)*100:.2f} %')\n",
    "    self.df_core = self.df_core.loc[ok,:]\n",
    "    if add_month:\n",
    "        self.df_core.drop(columns=['timestampDayofyear'],inplace=True)\n",
    "    if add_site_id:\n",
    "        self.df_core.drop(columns=['site_id'],inplace=True)\n",
    "    return self.df_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor()\n",
    "tfms_config = {'remove_bad_meter0_readings_of_first_141days':{}}\n",
    "_df, _vars = processor(ashrae_data['meter_train'], df_building=ashrae_data['building'],\n",
    "                       tfms_configs=tfms_config)\n",
    "display(_df.head(), _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert len(_df) == len(ashrae_data['meter_train']) - 346112\n",
    "assert (_df.loc[(_df['building_id']==0)&(_df['meter']==0), 'timestamp'] < pd.to_datetime('2016-05-20')).sum() == 0, 'Not correctly removed all first 141 days for meter 0 and site_id 0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "it = inspection.InspectTimeseries(_df, building=ashrae_data['building'],\n",
    "                                  weather=ashrae_data['weather_train'],\n",
    "                                  dep_var='meter_reading_log1p')\n",
    "it.inspect_boldly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing absurdly high meter 2 readings for building 1099"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def fix_bid_1099(self:Processor, threshold:float=10.):\n",
    "    if self.test_run: return\n",
    "    if not self.is_train: return self.df_core\n",
    "    assert 'timestamp' in self.df_core.columns\n",
    "    rm = (self.df_core['building_id'] == 1099)\n",
    "    rm = rm & (self.df_core['meter'] == 2)\n",
    "    rm = rm & (self.df_core[self.dep_var] > threshold)\n",
    "    ok = ~rm\n",
    "    if self.is_train and self.t_train is not None:\n",
    "        ok = ok | ~self.t_in_train_set\n",
    "    logger.info(f'Fixing building_id 1099: removing {(~ok).sum()} data points = {(~ok).sum()/len(ok)*100:.2f} %')\n",
    "    self.df_core = self.df_core.loc[ok,:]\n",
    "    return self.df_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor()\n",
    "tfms_config = {'fix_bid_1099':{}}\n",
    "_df, _vars = processor(ashrae_data['meter_train'], tfms_configs=tfms_config)\n",
    "display(_df.head(), _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert len(_df) == len(ashrae_data['meter_train']) - 3377\n",
    "assert (_df.loc[(_df['building_id']==1099) & (_df['meter']==2), 'meter_reading_log1p'] > 10).sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor(t_train=t_train)\n",
    "tfms_config = {'fix_bid_1099':{}}\n",
    "_df, _vars = processor(ashrae_data['meter_train'], tfms_configs=tfms_config)\n",
    "display(_df.head(), _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert len(ashrae_data['meter_train']) - 3377 <= len(_df) < len(ashrae_data['meter_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "it = inspection.InspectTimeseries(_df, building=ashrae_data['building'],\n",
    "                                  weather=ashrae_data['weather_train'],\n",
    "                                  dep_var='meter_reading_log1p')\n",
    "it.inspect_boldly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def remove_imputed_weeks(self:Processor, dep_var='meter_reading'):\n",
    "    if self.test_run: return\n",
    "    if not self.is_train: return self.df_core\n",
    "    grp = ['building_id', 'meter', pd.Grouper(key='timestamp', freq='W-MON')]\n",
    "    wks = (self.df_core.groupby(grp)[dep_var]).describe(percentiles=[.05,.95])\n",
    "\n",
    "    min_date = self.df_core['timestamp'].dt.date.min() - pd.Timedelta(7,unit='w')\n",
    "    max_date = self.df_core['timestamp'].dt.date.max() + pd.Timedelta(7,unit='d')\n",
    "    w_range = pd.date_range(min_date, max_date, freq='W-MON')\n",
    "\n",
    "    self.df_core['week'] = [v.right for v in pd.cut(self.df_core['timestamp'], w_range)]\n",
    "\n",
    "    self.df_core = self.df_core.join(wks.loc[:,['5%','95%']],\n",
    "                                     on=['building_id','meter','week'])\n",
    "    rm = np.isclose(self.df_core['5%'], self.df_core['95%'])\n",
    "    ok = ~rm\n",
    "    if self.is_train and self.t_train is not None:\n",
    "        ok = ok | ~self.t_in_train_set\n",
    "    logger.info(f'Imputed weeks: removing {(~ok).sum()} data points = {(~ok).sum()/len(ok)*100:.2f} %')\n",
    "    self.df_core = self.df_core.loc[ok,:].drop(columns=['5%','95%','week'])\n",
    "    return self.df_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor()\n",
    "tfms_config = {'remove_imputed_weeks':{'dep_var':'meter_reading'}}\n",
    "tmp = ashrae_data['meter_train'].loc[(ashrae_data['meter_train']['building_id']==0)&(ashrae_data['meter_train']['meter']==0)]\n",
    "_df, _vars = processor(tmp, tfms_configs=tfms_config)\n",
    "display(_df.head(), _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert len(_df) == len(tmp) - 3265"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor(t_train=t_train)\n",
    "tfms_config = {'remove_imputed_weeks':{'dep_var':'meter_reading'}}\n",
    "tmp = ashrae_data['meter_train'].loc[(ashrae_data['meter_train']['building_id']==0)&(ashrae_data['meter_train']['meter']==0)]\n",
    "_df, _vars = processor(tmp, tfms_configs=tfms_config)\n",
    "display(_df.head(), _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert len(tmp) - 3265 <= len(_df) < len(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "it = inspection.InspectTimeseries(_df, building=ashrae_data['building'],\n",
    "                                  weather=ashrae_data['weather_train'], \n",
    "                                  dep_var='meter_reading_log1p')\n",
    "it.inspect_boldly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are outliers! ðŸ˜¨ Let's remove them as well. Example `building_id` 60 and `meter` 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "it = inspection.InspectTimeseries(ashrae_data['meter_train'], building=ashrae_data['building'],\n",
    "                                  weather=ashrae_data['weather_train'])\n",
    "it.inspect_boldly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def remove_outliers(self:Processor, f:float=10, dep_var:str='meter_reading'):\n",
    "    if self.test_run: return None\n",
    "    if not self.is_train: return self.df_core\n",
    "    s = self.df_core.groupby(['building_id','meter'])[dep_var].describe()\n",
    "    s['threshold'] = s['50%'] + (s['75%'] - s['50%']) * f\n",
    "    self.df_core = self.df_core.join(s.loc[:,['threshold']],\n",
    "                                     on=['building_id', 'meter'])\n",
    "    ok = self.df_core[dep_var] < self.df_core['threshold']\n",
    "    if self.is_train and self.t_train is not None:\n",
    "        ok = ok | ~self.t_in_train_set\n",
    "\n",
    "    logger.info(f'Outliers: removing {(~ok).sum()} data points = {(~ok).sum()/len(ok)*100:.2f} %')\n",
    "    self.df_core = self.df_core.loc[ok,:].drop(columns=['threshold'])\n",
    "    return self.df_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor()\n",
    "tfms_config = {'remove_outliers':{'f':10,'dep_var':'meter_reading'}}\n",
    "tmp = ashrae_data['meter_train'].loc[(ashrae_data['meter_train']['building_id']==60)&(ashrae_data['meter_train']['meter']==1)]\n",
    "_df, _vars = processor(tmp, tfms_configs=tfms_config)\n",
    "display(_df.head(), _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert len(_df) == len(tmp) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor(t_train=t_train)\n",
    "tfms_config = {'remove_outliers':{'f':10,'dep_var':'meter_reading'}}\n",
    "tmp = ashrae_data['meter_train'].loc[(ashrae_data['meter_train']['building_id']==60)&(ashrae_data['meter_train']['meter']==1)]\n",
    "_df, _vars = processor(tmp, tfms_configs=tfms_config)\n",
    "display(_df.head(), _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert  len(tmp) - 1 <= len(_df) <= len(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "it = inspection.InspectTimeseries(_df, building=ashrae_data['building'],\n",
    "                                  weather=ashrae_data['weather_train'], \n",
    "                                  dep_var='meter_reading_log1p')\n",
    "it.inspect_boldly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding building information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def add_building_features(self:Processor):\n",
    "    if self.test_run: return\n",
    "    n = len(self.df_core)\n",
    "    self.df_core = pd.merge(self.df_core, self.df_building, on='building_id', how='left')\n",
    "    assert n == len(self.df_core)\n",
    "    _cats = ['site_id', 'primary_use']\n",
    "    _conts = ['square_feet', 'year_built', 'floor_count']\n",
    "    logger.info(f'Added building features: \\n\\tcategorical: {_cats}\\n\\tcontinuous: {_conts}')\n",
    "    self.cats.extend(_cats)\n",
    "    self.conts.extend(_conts)\n",
    "    return self.df_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor()\n",
    "tfms_config = {'add_building_features':{}}\n",
    "tmp = ashrae_data['meter_train'].loc[(ashrae_data['meter_train']['building_id']==60)&(ashrae_data['meter_train']['meter']==1)]\n",
    "_df, _vars = processor(tmp, tfms_configs=tfms_config,\n",
    "                       df_building=ashrae_data['building'])\n",
    "display(_df.head(), _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "expected_cols = ['building_id', 'meter', 'timestamp', 'meter_reading_log1p', \n",
    "                 'site_id', 'primary_use', 'square_feet', 'year_built', 'floor_count']\n",
    "assert list(_df.columns.values) == expected_cols, f'columns {_df.columns.values} did not meet the expected columns: {expected_cols}'\n",
    "assert (_vars['conts'] == ['square_feet', 'year_built', 'floor_count'])\n",
    "assert (_vars['cats'] == ['building_id', 'meter', 'site_id', 'primary_use'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding weather information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "site_GMT_offsets = [-5, 0, -7, -5, -8, 0, -5, -5, -5, -6, -7, -5, 0, -6, -5, -5]\n",
    "GMT_offset_map = {site: offset for site, offset in enumerate(site_GMT_offsets)}\n",
    "\n",
    "@patch\n",
    "def add_weather_features(self:Processor,\n",
    "                         fix_time_offset:bool=False,\n",
    "                         add_na_indicators:bool=False,\n",
    "                         impute_nas:bool=False):\n",
    "    if self.test_run: return\n",
    "    n = len(self.df_core)\n",
    "    add_site_id = 'site_id' not in self.df_core.columns\n",
    "    if add_site_id:\n",
    "        self.df_core = self.df_core.join(self.df_building.set_index('building_id').loc[:,['site_id']],\n",
    "                                         on='building_id')\n",
    "\n",
    "    if fix_time_offset:\n",
    "        dt = (self.df_weather['site_id']\n",
    "              .map(GMT_offset_map)\n",
    "              .apply(lambda x: pd.Timedelta(x, unit='hours')))\n",
    "        self.df_weather['timestamp'] = self.df_weather['timestamp'] + dt\n",
    "\n",
    "    cols = ['air_temperature', 'cloud_coverage', 'dew_temperature',\n",
    "            'precip_depth_1_hr', 'sea_level_pressure', 'wind_direction',\n",
    "            'wind_speed']\n",
    "\n",
    "    # adding na columns\n",
    "    na_cols = []\n",
    "    if add_na_indicators:\n",
    "        for col in cols:\n",
    "            nas = self.df_weather[col].isna()\n",
    "            na_col = f'{col}_na(processor)'\n",
    "            if nas.sum()>0:\n",
    "                self.df_weather[na_col] = nas\n",
    "                self.df_weather[na_col] = self.df_weather[na_col].astype(bool)\n",
    "                na_cols.append(na_col)\n",
    "\n",
    "    # imputing na columns\n",
    "    if impute_nas:\n",
    "        new_weather = []\n",
    "        aggs = {col: self.df_weather[col].median()\n",
    "                for col in cols}\n",
    "        for site, grp in self.df_weather.groupby('site_id'):\n",
    "            grp = grp.sort_values('timestamp')\n",
    "            for col in cols:\n",
    "                nas = grp[col].isna()\n",
    "                if nas.sum() == len(grp):\n",
    "                    grp[col] = aggs[col]\n",
    "                elif nas.sum() > 0:\n",
    "                    grp[col] = grp[col].interpolate(limit_direction='both',\n",
    "                                                    method='linear')\n",
    "                nas = grp[col].isna()\n",
    "                grp.loc[nas, col] = aggs[col]\n",
    "\n",
    "            new_weather.append(grp)\n",
    "        n_weather = len(self.df_weather)\n",
    "        self.df_weather = pd.concat(new_weather)\n",
    "        assert len(self.df_weather) == n_weather, f'Interpolation step changed rows from {n_weather} to {len(self.df_weather)}'\n",
    "\n",
    "    self.df_core = pd.merge(self.df_core, self.df_weather,\n",
    "                            on=['site_id', 'timestamp'],\n",
    "                            how='left')\n",
    "    assert n == len(self.df_core), f'Merging lead to an increase from {n} rows to {len(self.df_core)}'\n",
    "\n",
    "    if add_site_id:\n",
    "        self.df_core.drop(columns=['site_id'], inplace=True)\n",
    "    \n",
    "    _conts = ['wind_direction', 'air_temperature', 'dew_temperature', 'precip_depth_1_hr',\n",
    "              'sea_level_pressure', 'wind_speed', 'cloud_coverage']\n",
    "    self.cats.extend(na_cols)\n",
    "#     self.cats_order['cloud_coverage'] = sorted([v for v in self.df_core['cloud_coverage'].unique() if np.isfinite(v)])\n",
    "    self.conts.extend(_conts)\n",
    "    logger.info(f'Added weather features: \\n\\tcategorical: {na_cols}\\n\\tcontinuous: {_conts}')\n",
    "    return self.df_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor()\n",
    "tfms_config = {'add_weather_features':{'fix_time_offset':False,\n",
    "                                       'add_na_indicators':False,\n",
    "                                       'impute_nas':False}}\n",
    "tmp = ashrae_data['meter_train'].loc[(ashrae_data['meter_train']['building_id']==60)&(ashrae_data['meter_train']['meter']==1)]\n",
    "_df, _vars = processor(tmp, tfms_configs=tfms_config,\n",
    "                       df_building=ashrae_data['building'],\n",
    "                       df_weather=ashrae_data['weather_train'])\n",
    "display(_df.head().T, _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "expected_cols = ['building_id', 'meter', 'timestamp', 'meter_reading_log1p', \n",
    "                 'air_temperature', 'cloud_coverage', 'dew_temperature', \n",
    "                 'precip_depth_1_hr', 'sea_level_pressure',\n",
    "                 'wind_direction', 'wind_speed']\n",
    "assert list(_df.columns.values) == expected_cols, f'columns {_df.columns.values} did not meet the expected columns: {expected_cols}'\n",
    "assert (_vars['conts'] == ['wind_direction', 'air_temperature',\n",
    "                           'dew_temperature', 'precip_depth_1_hr',\n",
    "                           'sea_level_pressure',  'wind_speed',\n",
    "                           'cloud_coverage'])\n",
    "assert (_vars['cats'] == ['building_id', 'meter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor()\n",
    "tfms_config = {'add_weather_features':{'fix_time_offset':True,\n",
    "                                       'add_na_indicators':False,\n",
    "                                       'impute_nas':False}}\n",
    "tmp = ashrae_data['meter_train'].loc[(ashrae_data['meter_train']['building_id']==60)&(ashrae_data['meter_train']['meter']==1)]\n",
    "_df, _vars = processor(tmp, tfms_configs=tfms_config,\n",
    "                       df_building=ashrae_data['building'],\n",
    "                       df_weather=ashrae_data['weather_train'])\n",
    "display(_df.head().T, _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "expected_cols = ['building_id', 'meter', 'timestamp', 'meter_reading_log1p', \n",
    "                 'air_temperature', 'cloud_coverage', 'dew_temperature', \n",
    "                 'precip_depth_1_hr', 'sea_level_pressure',\n",
    "                 'wind_direction', 'wind_speed']\n",
    "assert list(_df.columns.values) == expected_cols, f'columns {_df.columns.values} did not meet the expected columns: {expected_cols}'\n",
    "assert (_vars['conts'] == ['wind_direction', 'air_temperature',\n",
    "                           'dew_temperature', 'precip_depth_1_hr',\n",
    "                           'sea_level_pressure',  'wind_speed',\n",
    "                           'cloud_coverage'])\n",
    "assert (_vars['cats'] == ['building_id', 'meter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor()\n",
    "tfms_config = {'add_weather_features':{'fix_time_offset':True,\n",
    "                                       'add_na_indicators':True,\n",
    "                                       'impute_nas':False}}\n",
    "tmp = ashrae_data['meter_train'].loc[(ashrae_data['meter_train']['building_id']==60)&(ashrae_data['meter_train']['meter']==1)]\n",
    "_df, _vars = processor(tmp, tfms_configs=tfms_config,\n",
    "                       df_building=ashrae_data['building'],\n",
    "                       df_weather=ashrae_data['weather_train'])\n",
    "display(_df.head().T, _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "expected_cols = ['building_id', 'meter', 'timestamp', 'meter_reading_log1p', \n",
    "                 'air_temperature', 'cloud_coverage', 'dew_temperature', \n",
    "                 'precip_depth_1_hr', 'sea_level_pressure',\n",
    "                 'wind_direction', 'wind_speed',\n",
    "                 'air_temperature_na(processor)', 'cloud_coverage_na(processor)',\n",
    "                 'dew_temperature_na(processor)', 'precip_depth_1_hr_na(processor)',\n",
    "                 'sea_level_pressure_na(processor)', 'wind_direction_na(processor)',\n",
    "                 'wind_speed_na(processor)']\n",
    "assert list(_df.columns.values) == expected_cols, f'columns {_df.columns.values} did not meet the expected columns: {expected_cols}'\n",
    "assert (_vars['conts'] == ['wind_direction', 'air_temperature',\n",
    "                           'dew_temperature', 'precip_depth_1_hr',\n",
    "                           'sea_level_pressure',  'wind_speed', 'cloud_coverage'])\n",
    "assert (_vars['cats'] == ['building_id', 'meter',\n",
    "                          'air_temperature_na(processor)', 'cloud_coverage_na(processor)',\n",
    "                          'dew_temperature_na(processor)', 'precip_depth_1_hr_na(processor)',\n",
    "                          'sea_level_pressure_na(processor)', 'wind_direction_na(processor)',\n",
    "                          'wind_speed_na(processor)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor()\n",
    "tfms_config = {'add_weather_features':{'fix_time_offset':True,\n",
    "                                       'add_na_indicators':False,\n",
    "                                       'impute_nas':True}}\n",
    "tmp = ashrae_data['meter_train'].loc[(ashrae_data['meter_train']['building_id']==60)&(ashrae_data['meter_train']['meter']==1)]\n",
    "_df, _vars = processor(tmp, tfms_configs=tfms_config,\n",
    "                       df_building=ashrae_data['building'],\n",
    "                       df_weather=ashrae_data['weather_train'])\n",
    "display(_df.head().T, _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "expected_cols = ['building_id', 'meter', 'timestamp', 'meter_reading_log1p', \n",
    "                 'air_temperature', 'cloud_coverage', 'dew_temperature', \n",
    "                 'precip_depth_1_hr', 'sea_level_pressure',\n",
    "                 'wind_direction', 'wind_speed']\n",
    "assert list(_df.columns.values) == expected_cols, f'columns {_df.columns.values} did not meet the expected columns: {expected_cols}'\n",
    "assert (_vars['conts'] == ['wind_direction', 'air_temperature',\n",
    "                           'dew_temperature', 'precip_depth_1_hr',\n",
    "                           'sea_level_pressure',  'wind_speed',\n",
    "                           'cloud_coverage'])\n",
    "assert (_vars['cats'] == ['building_id', 'meter'])\n",
    "assert _df[expected_cols[:4]].isna().sum().sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def add_time_features(self:Processor):\n",
    "    if self.test_run: return\n",
    "    _cats = ['timestampMonth', 'timestampDay', 'timestampWeek', 'timestampDayofweek',\n",
    "                      'timestampDayofyear', 'timestampIs_month_end', 'timestampIs_month_start',\n",
    "                      'timestampIs_quarter_start', 'timestampIs_quarter_end',\n",
    "                      'timestampIs_year_start', 'timestampIs_year_end', 'timestampHour',\n",
    "                      'timestampIs_us_holiday']\n",
    "    self.cats.extend(_cats)\n",
    "\n",
    "    self.df_core = add_datepart(self.df_core, self.time_col, drop=False)\n",
    "\n",
    "    self.df_core['timestampHour'] = self.df_core[self.time_col].dt.hour\n",
    "\n",
    "    dates_range = pd.date_range(start='2015-12-31', end='2019-01-01')\n",
    "    us_holidays = us_calendar().holidays(start=dates_range.min(), end=dates_range.max())\n",
    "\n",
    "    self.df_core['timestampIs_us_holiday'] = (self.df_core['timestamp'].dt.date.astype('datetime64')\n",
    "                                              .isin(us_holidays)\n",
    "                                              .astype(bool))\n",
    "    logger.info(f'Added categorical time features: {_cats}')\n",
    "    self.cats_order.update({\n",
    "        c: sorted(self.df_core[c].unique()) for c in ['timestampMonth', 'timestampDay',\n",
    "                                                      'timestampWeek', 'timestampDayofweek',\n",
    "                                                      'timestampDayofyear', 'timestampHour']\n",
    "    })\n",
    "    return self.df_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor()\n",
    "tfms_config = {'add_time_features':{}}\n",
    "tmp = ashrae_data['meter_train'].loc[(ashrae_data['meter_train']['building_id']==60)&(ashrae_data['meter_train']['meter']==1)]\n",
    "_df, _vars = processor(tmp, tfms_configs=tfms_config)\n",
    "display(_df.head().T, _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "expected_cols = ['building_id', 'meter', 'timestamp', 'timestampWeek', 'meter_reading_log1p', \n",
    "                 'timestampMonth', 'timestampDay', 'timestampDayofweek',\n",
    "                 'timestampDayofyear', 'timestampIs_month_end', 'timestampIs_month_start',\n",
    "                 'timestampIs_quarter_end', 'timestampIs_quarter_start',\n",
    "                 'timestampIs_year_end', 'timestampIs_year_start', 'timestampElapsed',\n",
    "                 'timestampHour', 'timestampIs_us_holiday']\n",
    "assert list(_df.columns.values) == expected_cols, f'columns {_df.columns.values} did not meet the expected columns: {expected_cols}'\n",
    "assert (_vars['conts'] == [])\n",
    "assert (_vars['cats'] == ['building_id', 'meter', 'timestampMonth',\n",
    "                          'timestampDay', 'timestampWeek', 'timestampDayofweek',\n",
    "                          'timestampDayofyear', 'timestampIs_month_end', \n",
    "                          'timestampIs_month_start', 'timestampIs_quarter_start', \n",
    "                          'timestampIs_quarter_end', 'timestampIs_year_start',\n",
    "                          'timestampIs_year_end', 'timestampHour',\n",
    "                          'timestampIs_us_holiday'])\n",
    "assert len(_df) == len(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding `dep_var_stats`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "DEFAULT_GRP_COLS = ['building_id', 'timestampHour', 'meter']\n",
    "\n",
    "@patch\n",
    "def add_dep_var_stats(self:Processor, grp_cols:typing.List[str]=None):\n",
    "    if self.test_run: return\n",
    "\n",
    "    grp_cols = DEFAULT_GRP_COLS if grp_cols is None else grp_cols\n",
    "\n",
    "    assert self.is_train or self.dep_var_stats is not None\n",
    "    if self.is_train:\n",
    "        self.dep_var_stats = dict()\n",
    "\n",
    "    funs = {\n",
    "        'median': lambda x: torch.median(tensor(x)).item(),\n",
    "        'mean': lambda x: torch.mean(tensor(x)).item(),\n",
    "        '5%': lambda x: np.percentile(x, 5),\n",
    "        '95%': lambda x: np.percentile(x, 95),\n",
    "    }\n",
    "    _conts = []\n",
    "    # computing stats for self.dep_var on the coarsest possible level\n",
    "    for name, fun in funs.items():\n",
    "        name = f'{self.dep_var}_{name}'\n",
    "        _conts.append(name)\n",
    "        self.conts.append(name)\n",
    "\n",
    "        if self.is_train:\n",
    "            value = fun(self.df_core[self.dep_var].values)\n",
    "            self.df_core[name] = value\n",
    "            self.dep_var_stats[name] = value\n",
    "        else:\n",
    "            self.df_core[name] = self.dep_var_stats[name]\n",
    "\n",
    "    # adding stats of self.dep_var on a more granular level\n",
    "    if grp_cols is not None:\n",
    "        t_col = 'timestampHour'\n",
    "        do_add_t = t_col in grp_cols and t_col not in self.df_core.columns.values\n",
    "        if do_add_t:\n",
    "            self.df_core[t_col] = self.df_core['timestamp'].dt.hour\n",
    "\n",
    "        assert all([c in self.df_core.columns.values for c in grp_cols])\n",
    "\n",
    "        for fun_name, fun in funs.items():\n",
    "            name = f'{self.dep_var}_{\"-\".join(grp_cols)}_{fun_name}'\n",
    "            _conts.append(name)\n",
    "            self.conts.append(name)\n",
    "\n",
    "            if self.is_train:\n",
    "\n",
    "                self.dep_var_stats[name] = (self.df_core.groupby(grp_cols)[self.dep_var]\n",
    "                                            .agg(fun)\n",
    "                                            .rename(name))\n",
    "            self.df_core = self.df_core.join(self.dep_var_stats[name], on=grp_cols)\n",
    "            self.df_core[name].fillna(self.dep_var_stats[f'{self.dep_var}_{fun_name}'], inplace=True)\n",
    "\n",
    "        if do_add_t:\n",
    "            self.df_core.drop(columns=[t_col], inplace=True)\n",
    "    logger.info(f'Added continuous target columns: {_conts}')\n",
    "    return self.df_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor()\n",
    "tfms_config = {'add_dep_var_stats':{}}\n",
    "mask = (ashrae_data['meter_train']['building_id']==60)&(ashrae_data['meter_train']['meter']==1)\n",
    "\n",
    "tmp = ashrae_data['meter_train'].loc[mask]\n",
    "_df, _vars = processor(tmp, tfms_configs=tfms_config)\n",
    "\n",
    "display(_df.head().T, _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mask = (ashrae_data['meter_test']['building_id']==60)&(ashrae_data['meter_test']['meter']==1)\n",
    "tmp_test = ashrae_data['meter_test'].loc[mask]\n",
    "_df_test, _ = processor(tmp_test, tfms_configs=tfms_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "expected_cols = ['building_id', 'meter', 'timestamp', 'meter_reading_log1p', \n",
    "                 'meter_reading_log1p_median', 'meter_reading_log1p_mean',\n",
    "                 'meter_reading_log1p_5%', 'meter_reading_log1p_95%', \n",
    "                 'meter_reading_log1p_building_id-timestampHour-meter_median',\n",
    "                 'meter_reading_log1p_building_id-timestampHour-meter_mean',\n",
    "                 'meter_reading_log1p_building_id-timestampHour-meter_5%',\n",
    "                 'meter_reading_log1p_building_id-timestampHour-meter_95%']\n",
    "assert list(_df.columns.values) == expected_cols, f'columns {_df.columns.values} did not meet the expected columns: {expected_cols}'\n",
    "assert (_vars['conts'] == ['meter_reading_log1p_median',\n",
    "                           'meter_reading_log1p_mean',\n",
    "                           'meter_reading_log1p_5%',\n",
    "                           'meter_reading_log1p_95%', \n",
    "                           'meter_reading_log1p_building_id-timestampHour-meter_median',\n",
    "                           'meter_reading_log1p_building_id-timestampHour-meter_mean',\n",
    "                           'meter_reading_log1p_building_id-timestampHour-meter_5%',\n",
    "                           'meter_reading_log1p_building_id-timestampHour-meter_95%'])\n",
    "assert (_vars['cats'] == ['building_id', 'meter'])\n",
    "assert len(_df) == len(tmp)\n",
    "assert all(inspection.show_nans(_df_test)['# NaNs'] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "DEFAULT_ONEHOT_COLS = ['meter']\n",
    "\n",
    "@patch\n",
    "def add_onehot_encoded(self:Processor, onehot_cols:typing.List[str]=None):\n",
    "    if self.test_run: return\n",
    "    onehot_cols = DEFAULT_ONEHOT_COLS if onehot_cols is None else onehot_cols\n",
    "\n",
    "    t_col = 'timestampHour'\n",
    "    do_add_t = t_col in onehot_cols and t_col not in self.df_core.columns.values\n",
    "    if do_add_t:\n",
    "        self.df_core[t_col] = self.df_core['timestamp'].dt.hour\n",
    "\n",
    "    self.df_core['id'] = [str(v) for v in zip(*[self.df_core[v] for v in onehot_cols])]\n",
    "\n",
    "    if self.is_train:\n",
    "        self.onehot_cols = onehot_cols\n",
    "        self.onehot_tfm = OneHotEncoder()\n",
    "        self.onehot_tfm.fit(self.df_core.loc[:, ['id']])\n",
    "\n",
    "\n",
    "    names = [f'{\"-\".join(self.onehot_cols)}_{v}' for v in self.onehot_tfm.categories_[0]]\n",
    "    \n",
    "    self.cats.extend(names)\n",
    "\n",
    "    df_onehot = pd.DataFrame(self.onehot_tfm.transform(self.df_core.loc[:, ['id']]).toarray(),\n",
    "                             columns=names, index=self.df_core.index, dtype=bool)\n",
    "    logger.info(f'Added one hot encoded features: {names}')\n",
    "    to_drop = ['id']\n",
    "    if do_add_t:\n",
    "        to_drop.append(t_col)\n",
    "    self.df_core.drop(columns=to_drop, inplace=True)\n",
    "    self.df_core = pd.concat((self.df_core, df_onehot), axis=1)\n",
    "    return self.df_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor()\n",
    "tfms_config = {'add_onehot_encoded':{}}\n",
    "tmp = ashrae_data['meter_train'].loc[(ashrae_data['meter_train']['building_id']<=60)]\n",
    "_df, _vars = processor(tmp, tfms_configs=tfms_config)\n",
    "display(_df.head(), _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "expected_cols = ['building_id', 'meter', 'timestamp', 'meter_reading_log1p', \n",
    "                 'meter_(0,)', 'meter_(1,)']\n",
    "assert list(_df.columns.values) == expected_cols, f'columns {_df.columns.values} did not meet the expected columns: {expected_cols}'\n",
    "assert (_vars['conts'] == [])\n",
    "assert (_vars['cats'] == ['building_id', 'meter', 'meter_(0,)', 'meter_(1,)'])\n",
    "assert len(_df) == len(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running through part of the train / validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor(t_train=t_train)\n",
    "tfms_config = {\n",
    "    'fix_bid_363':{},\n",
    "    'fix_bid_1099':{'threshold': 10.},\n",
    "    'remove_bad_meter0_readings_of_first_141days': {},\n",
    "    'remove_not_summer_0s_meter_2_and_3': {},\n",
    "    'remove_0s_meter0': {},\n",
    "    'add_dep_var_stats':{},\n",
    "    'add_time_features':{},\n",
    "    'add_weather_features':{'fix_time_offset':True,\n",
    "                            'add_na_indicators':True,\n",
    "                            'impute_nas':True},\n",
    "    'add_building_features':{},\n",
    "    'remove_outliers':{'f':10,'dep_var':'meter_reading'},\n",
    "    'remove_imputed_weeks':{'dep_var':'meter_reading'},\n",
    "#     'add_onehot_encoded':{},\n",
    "}\n",
    "tmp = ashrae_data['meter_train'].loc[(ashrae_data['meter_train']['building_id']<=60)]\n",
    "_df_train, _vars = processor(tmp, tfms_configs=tfms_config,\n",
    "                             df_weather=ashrae_data['weather_train'],\n",
    "                             df_building=ashrae_data['building'])\n",
    "display(_df_train.head(), _vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running through part of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tmp = ashrae_data['meter_test'].loc[(ashrae_data['meter_test']['building_id']<=60)]\n",
    "_df_test, _ = processor(tmp, tfms_configs=tfms_config,\n",
    "                        df_weather=ashrae_data['weather_test'],\n",
    "                        df_building=ashrae_data['building'])\n",
    "\n",
    "display(_df_test.head().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def align_test(df_train:pd.DataFrame, var_names:dict,\n",
    "               df_test:pd.DataFrame):\n",
    "    return df_test.loc[:,[v for v in df_train.columns if v != var_names['dep_var']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_df_test = align_test(_df_train, _vars, _df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "a, b = list(_df_test.columns.values), [v for v in _df_train.columns.values if v != _vars['dep_var']]\n",
    "assert a == b, f'Columns are mismatching! \\n\\tIn train but not test: {set(b)-set(a)}\\n\\tIn test but not train: {set(a)-set(b)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done testing. Let's apply the transforms to the entire data set. Takes about 20min with `remove_imputed_weeks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor() # t_train=t_train\n",
    "tfms_config = {\n",
    "    'fix_bid_363':{},\n",
    "    'fix_bid_1099':{'threshold': 10.},\n",
    "    'remove_bad_meter0_readings_of_first_141days': {},\n",
    "    'remove_not_summer_0s_meter_2_and_3': {},\n",
    "    'remove_0s_meter0': {},\n",
    "    'remove_outliers':{'f':10,'dep_var':'meter_reading'},\n",
    "    'remove_imputed_weeks':{'dep_var':'meter_reading'},\n",
    "    'add_dep_var_stats':{},\n",
    "    'add_time_features':{},\n",
    "    'add_weather_features':{'fix_time_offset':True,\n",
    "                            'add_na_indicators':True,\n",
    "                            'impute_nas':True},\n",
    "    'add_building_features':{},\n",
    "#     'add_onehot_encoded':{},\n",
    "}\n",
    "\n",
    "tmp = ashrae_data['meter_train'] if n_samples_quick is None else ashrae_data['meter_train'].sample(n=n_samples_quick)\n",
    "\n",
    "df, var_names = processor(tmp, tfms_configs=tfms_config,\n",
    "                          df_weather=ashrae_data['weather_train'],\n",
    "                          df_building=ashrae_data['building'])\n",
    "display(df.head(), var_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "var_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing the test set (takes ~4 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tmp = ashrae_data['meter_test'] if n_samples_quick is None else ashrae_data['meter_test'].sample(n=n_samples_quick)\n",
    "df_test_p, _ = processor(tmp, tfms_configs=tfms_config,\n",
    "                         df_weather=ashrae_data['weather_test'],\n",
    "                         df_building=ashrae_data['building'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuring that all columns found with NaN values for the test set also have NaN values in the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nans = inspection.show_nans(df_test_p)\n",
    "train_nans = inspection.show_nans(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_nans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sure that columns with NaNs in the test set do not have NaN values in the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nan_cols = [col for col in test_nans.loc[test_nans['# NaNs']>0].index]\n",
    "assert (train_nans.loc[train_nans.index.isin(test_nan_cols),'# NaNs'] == 0).sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test = align_test(df, var_names, df_test_p)\n",
    "if n_samples_quick is None:\n",
    "    assert len(df_test) == len(ashrae_data['meter_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert len(df_test.columns) + 1 == len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def test_var_names(var_names:dict):\n",
    "    assert isinstance(var_names, dict)\n",
    "    assert 'conts' in var_names and 'cats' in var_names and 'dep_var' in var_names\n",
    "    assert isinstance(var_names['conts'], list)\n",
    "    assert isinstance(var_names['cats'], list)\n",
    "    assert isinstance(var_names['dep_var'], str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_var_names(var_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def store_var_names(data_path:Path, var_names:dict):\n",
    "    fname = data_path/'var_names.pckl'\n",
    "    print(f'Storing var names at: {fname}')\n",
    "    with open(fname, 'wb') as f:\n",
    "        pickle.dump(var_names, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "store_var_names(data_path, var_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_var_names(fname:Path):\n",
    "    print(f'Reading var names at: {fname}')\n",
    "    with open(fname, 'rb') as f:\n",
    "        var_names = pickle.load(f)\n",
    "    return var_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# var_names = load_var_names(data_path/'var_names.pckl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_var_names(var_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def store_df(path:Path, df:pd.DataFrame): df.to_parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "store_df(data_path/'X.parquet', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "store_df(data_path/'X_test.parquet', df_test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_df(path:Path): return pd.read_parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# df = load_df(data_path/'X.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing if certain features improve the score beyond the baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training to get a basic idea if the added features do have any benefit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = load_df(data_path/'X.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_test_p = load_df(data_path/'X_test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "var_names = load_var_names(data_path/'var_names.pckl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_tabular_object(df:pd.DataFrame, var_names:dict,\n",
    "                       splits=None, procs:list=[Categorify, FillMissing, Normalize]):\n",
    "    return TabularPandas(df.copy(), procs,\n",
    "                         var_names['cats'], var_names['conts'],\n",
    "                         y_names=var_names['dep_var'],\n",
    "                         splits=splits)\n",
    "\n",
    "SPLIT_PARAMS = dict(\n",
    "    train_frac = .8,\n",
    "    split_kind = 'time_split_day',\n",
    ")\n",
    "\n",
    "\n",
    "def train_predict(df:pd.DataFrame, var_names:dict,\n",
    "                  model, params:dict=None, n_rep:int=3,\n",
    "                  n_samples_train:int=10000,\n",
    "                  n_samples_valid:int=10000,\n",
    "                  procs:list=[Categorify, FillMissing, Normalize],\n",
    "                  split_params:dict=None):\n",
    "\n",
    "    split_params = SPLIT_PARAMS if split_params is None else split_params\n",
    "    y_col = var_names['dep_var']\n",
    "    score_vals = []\n",
    "    params = {} if params is None else params\n",
    "\n",
    "    to = get_tabular_object(df, var_names, procs=procs)\n",
    "\n",
    "    for i in tqdm.tqdm(range(n_rep), total=n_rep, desc='Repetition'):\n",
    "\n",
    "        m = model(**params)\n",
    "        splits = split_dataset(df, **split_params)\n",
    "\n",
    "        mask = to.xs.index.isin(splits[0])\n",
    "\n",
    "        _X = to.xs.loc[~mask, :].iloc[:n_samples_train]\n",
    "        _y = to.ys.loc[~mask, y_col].iloc[:n_samples_train]\n",
    "        m.fit(_X.values, _y.values)\n",
    "\n",
    "        _X = to.xs.loc[mask, :].iloc[:n_samples_valid]\n",
    "        _y = to.ys.loc[mask, y_col].iloc[:n_samples_valid]\n",
    "        pred = m.predict(_X.values)\n",
    "        s = torch.sqrt(F.mse_loss(tensor(pred), tensor(_y.values))).item()\n",
    "        score_vals.append({'iter': i, 'rmse loss': s})\n",
    "\n",
    "    return pd.DataFrame(score_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_params = dict(\n",
    "    #split_kind = 'random',\n",
    "    #split_kind = 'time',\n",
    "    #split_kind = 'fix_time',\n",
    "    split_kind = 'time_split_day',\n",
    "    t_train = None,\n",
    "    train_frac = .8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {'n_estimators': 20, 'max_features': 'sqrt',\n",
    "#           'n_jobs': -1}\n",
    "# model = ensemble.RandomForestRegressor\n",
    "params = {}\n",
    "model = linear_model.LinearRegression\n",
    "n_rep = 21\n",
    "n_samples_train = 10000\n",
    "n_samples_valid = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "var_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is not always necessary. Sensible in the case of a linear model to remove categorical values which are not onehot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "to_remove = {'cats':['building_id', 'meter'], 'conts': []}\n",
    "\n",
    "for k in ['cats', 'conts']:\n",
    "    var_names[k] = [_v for _v in var_names[k] if _v not in to_remove[k]]\n",
    "var_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "procs = [] #[Categorify, FillMissing, Normalize]\n",
    "df_rep = train_predict(df.copy(), var_names, model, params=params, \n",
    "                       n_rep=n_rep, n_samples_train=n_samples_train,\n",
    "                       n_samples_valid=n_samples_valid, procs=procs,\n",
    "                       split_params=split_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rep['rmse loss'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(df_rep, y='rmse loss', range_y=(0, 2.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline model = RandomForest with 20 estimators and sqrt features, training over 100k samples and predicting over 1k\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>input</th>\n",
    "        <th>model</th>\n",
    "        <th>rmse loss</th>\n",
    "        <th>time [s/it]</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>meter and building id only</td>\n",
    "        <td>random forest</td>\n",
    "        <td>1.2 - 1.21</td>\n",
    "        <td>10.2</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>using dep_var stats</td>\n",
    "        <td>random forest</td>\n",
    "        <td>1.16 - 1.18</td>\n",
    "        <td>17.3</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>using time stats</td>\n",
    "        <td>random forest</td>\n",
    "        <td>1.2 - 1.21</td>\n",
    "        <td>13.2 - 13.7</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>using building info</td>\n",
    "        <td>random forest</td>\n",
    "        <td>1.19</td>\n",
    "        <td>17 - 18</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>using weather (+ building) info</td>\n",
    "        <td>random forest</td>\n",
    "        <td>1.13 - 1.139</td>\n",
    "        <td>14.6 - 15</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>using all above</td>\n",
    "        <td>random forest</td>\n",
    "        <td>1.19 - 1.21</td>\n",
    "        <td>20 - 26</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>removing leading 0s in `dep_var`</td>\n",
    "        <td>random forest</td>\n",
    "        <td>.36 - .37</td>\n",
    "        <td>4</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>removing trailing 0s in `dep_var`</td>\n",
    "        <td>random forest</td>\n",
    "        <td>1.2</td>\n",
    "        <td>4</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>removing empty weeks before the first full week</td>\n",
    "        <td>random forest</td>\n",
    "        <td>1.16</td>\n",
    "        <td>4</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>meter only</td>\n",
    "        <td>linear model</td>\n",
    "        <td>2.2</td>\n",
    "        <td>5</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>meter + hour</td>\n",
    "        <td>linear model</td>\n",
    "        <td>2.1</td>\n",
    "        <td>5</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>meter reading stats only (meter, building_id, hour)</td>\n",
    "        <td>linear model</td>\n",
    "        <td>1.23 - 1.24 / 1.68 - 1.7</td>\n",
    "        <td>5</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>meter + meter reading stats (meter, building_id, hour)</td>\n",
    "        <td>linear model</td>\n",
    "        <td>1.51 - 1.52</td>\n",
    "        <td>5</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>meter reading stats (meter, building_id, hour)</td>\n",
    "        <td>random forest</td>\n",
    "        <td>0.58 - 0.6</td>\n",
    "        <td>5</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>meter + meter reading stats (meter, building_id, hour)</td>\n",
    "        <td>random forest</td>\n",
    "        <td>1.21 - 1.22</td>\n",
    "        <td>5</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing `dep_var` distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "splits = split_dataset(df, **split_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "to = get_tabular_object(df, var_names, splits=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_train = 10000\n",
    "n_samples_valid = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# params = {'n_estimators': 20, 'max_features': 'sqrt'}\n",
    "# model = ensemble.RandomForestRegressor\n",
    "m = model(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_X = to.train.xs.sample(n_samples_train).values\n",
    "_y = to.train.ys.sample(n_samples_train).values.ravel()\n",
    "m.fit(_X, _y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_X = to.valid.xs.sample(n_samples_valid).values\n",
    "_y = to.valid.ys.sample(n_samples_valid).values.ravel()\n",
    "pred = m.predict(_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert np.isfinite(_y).all() and np.isfinite(pred).all()\n",
    "assert _y.shape == pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def hist_plot_preds(y0:np.ndarray, y1:np.ndarray,\n",
    "                    label0:str='y0', label1:str='y1'):\n",
    "    res = pd.concat(\n",
    "        (\n",
    "            pd.DataFrame({\n",
    "                'y': y0,\n",
    "                'set': [label0] * len(y0)\n",
    "            }),\n",
    "            pd.DataFrame({\n",
    "                'y':y1,\n",
    "                'set': [label1] * len(y1)\n",
    "            })\n",
    "        ),\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    return px.histogram(res, x='y', color='set', marginal='box',\n",
    "                        barmode='overlay', histnorm='probability density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_plot_preds(_y, pred, label0='truth (valid)', label1='prediction (valid)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting confidently wrong predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BoldlyWrongTimeseries:\n",
    "    def __init__(self, xs, y_true, y_pred, info:pd.DataFrame=None):\n",
    "        if info is None:\n",
    "            self.df = xs.loc[:,['meter', 'building_id', 'timestamp']].copy()\n",
    "        else:\n",
    "            assert all([v in info.columns.values for v in ['meter', 'building_id', 'timestamp']])\n",
    "            self.df = xs.join(info)\n",
    "\n",
    "        for col in ['meter', 'building_id']:\n",
    "            self.df[col] = self.df[col].astype('category')\n",
    "            self.df[col].cat.set_categories(sorted(self.df[col].unique()),\n",
    "                                            ordered=True, inplace=True)\n",
    "\n",
    "        self.df['y_true'] = y_true\n",
    "        self.df['y_pred'] = y_pred\n",
    "        self.compute_misses()\n",
    "\n",
    "    def compute_misses(self):\n",
    "        fun = lambda x: np.sqrt(np.mean(x**2))\n",
    "        self.miss = (self.df.assign(difference=lambda x: x['y_pred']-x['y_true'])\n",
    "                     .groupby(['building_id', 'meter'])\n",
    "                     .agg(loss=pd.NamedAgg(column='difference', aggfunc=fun))\n",
    "                     .dropna()\n",
    "                     .sort_values('loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_X = to.valid.xs \n",
    "_y = to.valid.ys.values.ravel() \n",
    "pred = m.predict(to.valid.xs.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert _y.shape == pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "bwt = BoldlyWrongTimeseries(to.valid.xs.join(df.loc[:,['building_id', 'meter','timestamp']]), \n",
    "                            _y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert len(bwt.miss) == 2380"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding plotting capability based on the loss or meter/building id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def plot_boldly_wrong(self:BoldlyWrongTimeseries,\n",
    "                      nth_last:int=None,\n",
    "                      meter:int=None, bid:int=None):\n",
    "\n",
    "    assert (meter is not None and bid is not None) or (nth_last is not None)\n",
    "\n",
    "    if nth_last is not None:\n",
    "        ix = self.miss.iloc[[nth_last],:]\n",
    "        meter = ix.index[0][1]\n",
    "        bid = ix.index[0][0]\n",
    "        loss = ix[\"loss\"].values[0]\n",
    "    else:\n",
    "        ix = self.miss.xs((bid,meter))\n",
    "        loss = ix.values[0]\n",
    "\n",
    "\n",
    "    df_plot = self.df.loc[(self.df['meter']==int(meter)) & (self.df['building_id']==int(bid))]\n",
    "    df_plot = pd.concat((\n",
    "        df_plot[['timestamp', 'y_true']].rename(columns={'y_true':'y'}).assign(label='true'),\n",
    "        df_plot[['timestamp', 'y_pred']].rename(columns={'y_pred':'y'}).assign(label='pred')),\n",
    "        ignore_index=True\n",
    "    )\n",
    "    return df_plot.plot(kind='scatter', x='timestamp',\n",
    "                        y='y', color='label', opacity=.4,\n",
    "                        title=f'pos {nth_last}: meter = {meter}, building_id = {bid}<br>loss = {loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bwt.plot_boldly_wrong(nth_last=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bwt.plot_boldly_wrong(meter=2, bid=1099)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding widgets for interactive exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def init_widgets(self:BoldlyWrongTimeseries):\n",
    "    self.int_txt_loss = widgets.IntText(min=-len(self.miss), max=len(self.miss),\n",
    "                                        description='Position', value=-1)\n",
    "    self.int_txt_meter = widgets.IntText(min=self.df['meter'].min(), max=self.df['meter'].max(),\n",
    "                                         description='Meter')\n",
    "    self.int_txt_bid = widgets.IntText(min=self.df['building_id'].min(), max=self.df['building_id'].max(),\n",
    "                                       description='building id')\n",
    "    self.run_btn = widgets.Button(description='plot')\n",
    "    self.switch_btn = widgets.Checkbox(description='Loss-based', value=True)\n",
    "    self.run_btn.on_click(self.click_boldly_wrong)\n",
    "    self.out_wdg = widgets.Output()\n",
    "\n",
    "@patch\n",
    "def run_boldly(self:BoldlyWrongTimeseries):\n",
    "    if not hasattr(self, 'switch_btn'):\n",
    "        self.init_widgets()\n",
    "    return widgets.VBox([self.switch_btn, self.int_txt_loss,\n",
    "                         self.int_txt_meter, self.int_txt_bid,\n",
    "                         self.run_btn, self.out_wdg])\n",
    "\n",
    "@patch\n",
    "def click_boldly_wrong(self:BoldlyWrongTimeseries, change):\n",
    "    self.out_wdg.clear_output()\n",
    "    nth_last = None if self.switch_btn.value == False else self.int_txt_loss.value\n",
    "    meter = None if self.switch_btn.value == True else self.int_txt_meter.value\n",
    "    bid = None if self.switch_btn.value == True else self.int_txt_bid.value\n",
    "    with self.out_wdg:\n",
    "        print(f'nth_last {nth_last} meter {meter} bid {bid}')\n",
    "        try:\n",
    "            self.plot_boldly_wrong(nth_last=nth_last, meter=meter, bid=bid).show()\n",
    "        except:\n",
    "            raise ValueError(f'nth_last {nth_last} meter {meter} bid {bid} not a valid combination! Likely due to missing meter/bid combination')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bwt.run_boldly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38_fastai]",
   "language": "python",
   "name": "conda-env-py38_fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "255px",
    "width": "272px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
