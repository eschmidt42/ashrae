{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data\n",
    "\n",
    "> Preprocessing data for the modelling step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import typing\n",
    "import pickle\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from ashrae import loading, inspection\n",
    "\n",
    "from fastai.tabular.all import *\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from sklearn import linear_model, tree, model_selection, ensemble\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import itertools\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as us_calendar\n",
    "\n",
    "import math\n",
    "from loguru import logger\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "pd.options.plotting.backend = \"plotly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_var = 'meter_reading'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loading.N_TRAIN = 100_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = loading.DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ashrae_data = loading.load_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def split_dataset(X:pd.DataFrame, split_kind:str='random',\n",
    "                  train_frac:float=.8, t_train:pd.DataFrame=None):\n",
    "\n",
    "    def random_split():\n",
    "        n_train = int(len(X)*train_frac)\n",
    "        train_bool = X.index.isin(np.random.choice(X.index.values, size=n_train, replace=False))\n",
    "        return train_bool\n",
    "\n",
    "    def time_split():\n",
    "        assert 'timestamp' in X.columns\n",
    "        time_col = 'timestamp'\n",
    "        ts = X[time_col].sort_values(ascending=True)\n",
    "        ix = int(len(X)*train_frac)\n",
    "        threshold_t = ts.iloc[ix:].values[0]\n",
    "        return X[time_col] < threshold_t\n",
    "\n",
    "    def time_split_day():\n",
    "        time_col = 'timestampDayofyear'\n",
    "\n",
    "        if time_col not in X.columns:\n",
    "            t = X['timestamp'].dt.dayofyear\n",
    "        else:\n",
    "            t = X[time_col]\n",
    "\n",
    "        days = (t.value_counts()\n",
    "                .rename('count')\n",
    "                .sample(frac=1)\n",
    "                .to_frame()\n",
    "                .cumsum()\n",
    "                .pipe(lambda x: x.loc[x['count'] <= (train_frac * len(t))]))\n",
    "\n",
    "        num_train_days = len(days)\n",
    "        mask = t.isin(days.index.values)\n",
    "\n",
    "        assert mask.sum() > 0\n",
    "        return mask\n",
    "\n",
    "    def fix_time_split():\n",
    "        assert t_train is not None\n",
    "        time_col = 'timestamp'\n",
    "        assert time_col in X.columns\n",
    "\n",
    "        mask = X[time_col].isin(t_train[time_col])\n",
    "        assert mask.sum() > 0\n",
    "        return mask\n",
    "\n",
    "    split_funs = {\n",
    "        'random': random_split,\n",
    "        'time': time_split,\n",
    "        'fix_time': fix_time_split,\n",
    "        'time_split_day': time_split_day,\n",
    "    }\n",
    "\n",
    "    assert split_kind in split_funs\n",
    "    train_bool = split_funs[split_kind]()\n",
    "\n",
    "    train_idx = np.where(train_bool)[0]\n",
    "    valid_idx = np.where(~train_bool)[0]\n",
    "\n",
    "    return (list(train_idx), list(valid_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#split_kind = 'random'\n",
    "#split_kind = 'time'\n",
    "# split_kind = 'fix_time'\n",
    "split_kind = 'time_split_day'\n",
    "\n",
    "t_train = None\n",
    "train_frac = .8\n",
    "splits = split_dataset(ashrae_data['meter_train'], split_kind=split_kind, train_frac=train_frac,\n",
    "                       t_train=t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'sets {len(splits)}, train {len(splits[0])} = {len(splits[0])/len(ashrae_data[\"meter_train\"]):.4f}, valid {len(splits[1])} = {len(splits[1])/len(ashrae_data[\"meter_train\"]):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_train = ashrae_data['meter_train'].iloc[splits[0]][['timestamp']]\n",
    "t_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#t_train.to_parquet(data_path/'t_train.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tmp = pd.concat((ashrae_data['meter_train'].iloc[splits[0]]\n",
    "                 .assign(label='train')\n",
    "                 .assign(meter_reading=lambda x: np.log(x['meter_reading']+1)),\n",
    "                (ashrae_data['meter_train'].iloc[splits[1]]\n",
    "                 .assign(label='valid')\n",
    "                 .assign(meter_reading=lambda x: np.log(x['meter_reading']+1)))), \n",
    "                axis=0, ignore_index=True)\n",
    "tmp.groupby('label').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(tmp.sample(10000), x='timestamp', y='meter_reading', color='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "potentially sensible cleaning to do (https://www.kaggle.com/purist1024/ashrae-simple-data-cleanup-lb-1-08-no-leaks):\n",
    "* remove all 0s for meter 0\n",
    "* remove all 0s for meter 2 and 3 if not summer\n",
    "* potentially remove 0s for meter 1 during winter\n",
    "* remove \"known-bad\" electrical readings from the first 141 days of the data for site 0 (i.e. UCF)\n",
    "* remove most absurdly high readings from building 1099. These are orders of magnitude higher than all data, and have been emperically seen in LB probes to be harmful outliers.\n",
    "* time time zone for weather data\n",
    "* impute nas for weather data\n",
    "* convert cyclic features, like hour, to  2d features (sin,cos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "DEP_VAR = 'meter_reading'\n",
    "TIME_COL = 'timestamp'\n",
    "\n",
    "class Processor:\n",
    "\n",
    "    dep_var_stats:dict = None\n",
    "    \n",
    "    def __init__(self, dep_var:str=None, time_col:str=None,\n",
    "                 t_train:pd.Series=None):\n",
    "        self.dep_var = DEP_VAR if dep_var is None else dep_var\n",
    "        self.time_col = TIME_COL if time_col is None else time_col\n",
    "        self.t_train = t_train\n",
    "    \n",
    "    def __call__(self, df_core:pd.DataFrame, df_building:pd.DataFrame=None,\n",
    "                 df_weather:pd.DataFrame=None, t_train:pd.DataFrame=None,\n",
    "                 tfms_configs:dict=None) -> (pd.DataFrame, dict):\n",
    "\n",
    "        # check if `df` is a test set (dep_var is missing)\n",
    "        self.is_train = self.dep_var in df_core.columns\n",
    "        self.df_core = df_core.copy()\n",
    "        self.conts, self.cats, self.cats_order = [], [], {}\n",
    "        self.cats += ['building_id', 'meter']\n",
    "        self.n = len(df_core)\n",
    "\n",
    "        # core pieces of dependent and independent variables\n",
    "        self.update_dep_var()\n",
    "\n",
    "        if tfms_configs is None:\n",
    "            logger.info('Empty transform configs `tfms_configs`. Returning ...')\n",
    "        else:\n",
    "            # if `t_train` (timestamps which belong to the training set) are provided perform a check which rows are effected\n",
    "            if self.t_train is not None:\n",
    "                self.t_train_set = set(self.t_train.values.ravel())\n",
    "\n",
    "            self.df_building = df_building.copy() if df_building is not None else None\n",
    "            self.df_weather = df_weather.copy() if df_weather is not None else None\n",
    "\n",
    "            # running transformations\n",
    "            self.sanity_check_input_for_tfms(tfms_configs)\n",
    "            for fun_name, config in tfms_configs.items():\n",
    "                self.df_core = getattr(self, fun_name)(**config)\n",
    "\n",
    "        df_core, var_names = self.cleanup()\n",
    "        logger.info(f'Reduced samples by {self.n - len(df_core)} rows = {(self.n - len(df_core))/self.n*100:.2f} %')\n",
    "        return df_core, var_names\n",
    "\n",
    "    @property\n",
    "    def t_in_train_set(self):\n",
    "        return self.df_core['timestamp'].isin(self.t_train_set)\n",
    "\n",
    "    def update_dep_var(self) -> pd.DataFrame:\n",
    "\n",
    "        if self.dep_var.endswith('log1p'):\n",
    "            return self.df_core\n",
    "\n",
    "        dep_var_new = f'{self.dep_var}_log1p'\n",
    "        if self.is_train:\n",
    "            self.df_core[dep_var_new] = np.log(self.df_core[self.dep_var].values + 1) # 3s\n",
    "#             self.df_core[dep_var_new] = self.df_core[self.dep_var].apply(lambda x: math.log(x+1)) # 12s with math.log, 27s with np.log\n",
    "#             self.df_core[dep_var_new] = self.df_core[self.dep_var].swifter.apply(lambda x: math.log(x+1)) # 15s with math.log + swifter\n",
    "        self.dep_var = dep_var_new\n",
    "        return self.df_core\n",
    "    \n",
    "    def sanity_check_input_for_tfms(self, tfms_configs:dict):\n",
    "        # sanity check presence of df_building if df_weather is given\n",
    "        if self.df_weather is not None:\n",
    "            assert self.df_building is not None, 'To join the weather info in `df_weather` you need to pass `df_building`.'\n",
    "\n",
    "        # making sure all required inputs are specified in `tfms_configs`\n",
    "        self.test_run = True\n",
    "        if tfms_configs is not None:\n",
    "            building_fun_names = ['add_building_features']\n",
    "            weather_fun_names = ['add_weather_features']\n",
    "            for fun_name, config in tfms_configs.items():\n",
    "                getattr(self, fun_name)(**config)\n",
    "                if fun_name in building_fun_names:\n",
    "                    assert self.df_building is not None, 'You need to pass `df_building` in Processor.__call__.'\n",
    "                if fun_name in weather_fun_names:\n",
    "                    assert self.df_weather is not None, 'You need to pass `df_weather` in Processor.__call__.'\n",
    "        self.test_run = False\n",
    "\n",
    "    def get_var_names(self) -> dict:\n",
    "        return {'conts': self.conts, 'cats': self.cats, 'dep_var': self.dep_var}\n",
    "\n",
    "    def cleanup(self) -> (pd.DataFrame, dict):\n",
    "        # converting cats to category type\n",
    "        for col in self.cats:\n",
    "            if self.df_core[col].dtype == bool: continue\n",
    "            self.df_core[col] = self.df_core[col].astype('category')\n",
    "            if col in self.cats_order:\n",
    "                self.df_core[col].cat.set_categories(self.cats_order[col],\n",
    "                                                     ordered=True, inplace=True)\n",
    "\n",
    "        # removing features\n",
    "        to_remove_cols = ['meter_reading', 'timestampYear'] # , self.time_col\n",
    "        self.df_core.drop(columns=[c for c in self.df_core.columns if c in to_remove_cols],\n",
    "                          inplace=True)\n",
    "\n",
    "        # shrinking the data frame\n",
    "        self.df_core = df_shrink(self.df_core, int2uint=True)\n",
    "\n",
    "        var_names = self.get_var_names()\n",
    "\n",
    "        if not self.is_train:\n",
    "            self.df_core.set_index('row_id', inplace=True)\n",
    "\n",
    "        missing_cols = [col for col in self.df_core.columns.values if col not in self.cats + self.conts + [self.dep_var]\n",
    "                        and col not in ['timestampElapsed', self.time_col, 'meter_reading']]\n",
    "\n",
    "        assert len(missing_cols) == 0, f'Missed to assign columns: {missing_cols} to `conts` or `cats`'\n",
    "\n",
    "        return self.df_core, var_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only applying the `dep_var` transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor()\n",
    "_df, _vars = processor(ashrae_data['meter_train'])\n",
    "display(_df.head(), _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(_df.groupby('meter').sample(1000), \n",
    "             x='meter_reading_log1p', facet_row='meter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "cols = ['building_id', 'meter', 'timestamp', 'meter_reading_log1p']\n",
    "assert len(_df.columns) == len(cols) and cols == list(_df.columns.values), f'Unexpected columns: {_df.columns} != {cols}'\n",
    "assert len(_vars) == 3 and len(_vars['conts']) == 0 and _vars['cats'] == ['building_id', 'meter'] and _vars['dep_var'] == 'meter_reading_log1p'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building 363"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The meter readings for `building_id` 363 before July 30th are likely due to a construction phase since the bulding's year value is 2017. So this method removes the readings from during the construction time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def fix_bid_363(self:Processor):\n",
    "    if self.test_run: return\n",
    "    if not self.is_train: return self.df_core\n",
    "    assert 'timestamp' in self.df_core.columns\n",
    "    rm = (self.df_core['building_id'] == 363)\n",
    "    rm = rm & (self.df_core['meter'] == 0)\n",
    "    rm = rm & (self.df_core['timestamp'] < pd.to_datetime('2016-07-30'))\n",
    "    ok = ~rm\n",
    "    if self.is_train and self.t_train is not None:\n",
    "        ok = ok | ~self.t_in_train_set\n",
    "    logger.info(f'Fixing building_id 363: removing {(~ok).sum()} data points = {(~ok).sum()/len(ok)*100:.2f} %')\n",
    "    self.df_core = self.df_core.loc[ok,:]\n",
    "    return self.df_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor()\n",
    "tfms_config = {'fix_bid_363':{}}\n",
    "_df, _vars = processor(ashrae_data['meter_train'], tfms_configs=tfms_config)\n",
    "display(_df.head(), _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "if loading.N_TRAIN is None: assert len(_df) == len(ashrae_data['meter_train']) - 5063"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor(t_train=t_train)\n",
    "tfms_config = {'fix_bid_363':{}}\n",
    "_df, _vars = processor(ashrae_data['meter_train'], tfms_configs=tfms_config)\n",
    "display(_df.head(), _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert len(ashrae_data['meter_train']) - 5063 <= len(_df) < len(ashrae_data['meter_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "it = inspection.InspectTimeseries(_df, building=ashrae_data['building'],\n",
    "                                  dep_var='meter_reading_log1p')\n",
    "it.inspect_boldly(bid=363, meter=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seem to be quite a few imputed / filled values in the meter readings, being visible as constant meter readings for more than a week at a time. This method removes those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "it = inspection.InspectTimeseries(ashrae_data['meter_train'], \n",
    "                                  building=ashrae_data['building'])\n",
    "it.inspect_boldly(bid=363, meter=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing all 0s for meter 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def remove_0s_meter0(self:Processor):\n",
    "    if self.test_run: return\n",
    "    if not self.is_train: return self.df_core\n",
    "    assert 'timestamp' in self.df_core.columns\n",
    "    rm = (self.df_core['meter'] == 0)\n",
    "    rm = rm & (self.df_core[self.dep_var] == 0)\n",
    "    ok = ~rm\n",
    "    if self.is_train and self.t_train is not None:\n",
    "        ok = ok | ~self.t_in_train_set\n",
    "    logger.info(f'Removing 0s for meter 0: removing {(~ok).sum()} data points = {(~ok).sum()/len(ok)*100:.2f} %')\n",
    "    self.df_core = self.df_core.loc[ok,:]\n",
    "    return self.df_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor()\n",
    "tfms_config = {'remove_0s_meter0':{}}\n",
    "_df, _vars = processor(ashrae_data['meter_train'], tfms_configs=tfms_config)\n",
    "display(_df.head(), _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "if loading.N_TRAIN is None: assert len(_df) == len(ashrae_data['meter_train']) - 530169\n",
    "assert 0 in _df.loc[_df['meter']!=0, 'meter_reading_log1p'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "it = inspection.InspectTimeseries(_df, building=ashrae_data['building'],\n",
    "                                  dep_var='meter_reading_log1p')\n",
    "it.inspect_boldly(meter=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing all 0s for meters 2 and 3 outside of summer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def remove_not_summer_0s_meter_2_and_3(self:Processor):\n",
    "    if self.test_run: return\n",
    "    if not self.is_train: return self.df_core\n",
    "    assert 'timestamp' in self.df_core.columns\n",
    "    add_month = 'timestampMonth' not in self.df_core.columns\n",
    "    if add_month:\n",
    "        self.df_core['timestampMonth'] = self.df_core['timestamp'].dt.month\n",
    "    rm = (self.df_core['meter'].isin([2,3]))\n",
    "    rm = rm & (self.df_core[self.dep_var] == 0)\n",
    "    rm = rm & (self.df_core['timestampMonth'].isin([6,7,8]))\n",
    "    ok = ~rm\n",
    "    if self.is_train and self.t_train is not None:\n",
    "        ok = ok | ~self.t_in_train_set\n",
    "    logger.info(f'Removing 0s for meter 2 and 3 during summer: removing {(~ok).sum()} data points = {(~ok).sum()/len(ok)*100:.2f} %')\n",
    "    self.df_core = self.df_core.loc[ok,:]\n",
    "    if add_month:\n",
    "        self.df_core.drop(columns=['timestampMonth'],inplace=True)\n",
    "    return self.df_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor()\n",
    "tfms_config = {'remove_not_summer_0s_meter_2_and_3':{}}\n",
    "_df, _vars = processor(ashrae_data['meter_train'], tfms_configs=tfms_config)\n",
    "display(_df.head(), _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "if loading.N_TRAIN is None: assert len(_df) == len(ashrae_data['meter_train']) - 253743"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "it = inspection.InspectTimeseries(_df, building=ashrae_data['building'],\n",
    "                                  dep_var='meter_reading_log1p')\n",
    "it.inspect_boldly(meter=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing \"bad\" electrical (meter 0) readings from the first 141 days of site 0 (i.e. UCF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def remove_bad_meter0_readings_of_first_141days(self:Processor):\n",
    "    if self.test_run: return\n",
    "    if not self.is_train: return self.df_core\n",
    "    assert 'timestamp' in self.df_core.columns\n",
    "    add_month = 'timestampDayofyear' not in self.df_core.columns\n",
    "    if add_month:\n",
    "        self.df_core['timestampDayofyear'] = self.df_core['timestamp'].dt.dayofyear\n",
    "    add_site_id = 'site_id' not in self.df_core.columns\n",
    "    if add_site_id:\n",
    "        assert self.df_building is not None, 'df_building cannot be None for this method.'\n",
    "        self.df_core = pd.merge(self.df_core, self.df_building.loc[:,['building_id','site_id']], on='building_id', how='left')\n",
    "        assert self.df_core['site_id'].isna().sum() == 0\n",
    "    rm = self.df_core['meter'] == 0\n",
    "    rm = rm & (self.df_core['site_id'] == 0)\n",
    "    rm = rm & (self.df_core['timestampDayofyear'] < 141)\n",
    "    ok = ~rm\n",
    "    if self.is_train and self.t_train is not None:\n",
    "        ok = ok | ~self.t_in_train_set\n",
    "    logger.info(f'Bad readings for meter 0 for the first 141 days for site 0: removing {(~ok).sum()} data points = {(~ok).sum()/len(ok)*100:.2f} %')\n",
    "    self.df_core = self.df_core.loc[ok,:]\n",
    "    if add_month:\n",
    "        self.df_core.drop(columns=['timestampDayofyear'],inplace=True)\n",
    "    if add_site_id:\n",
    "        self.df_core.drop(columns=['site_id'],inplace=True)\n",
    "    return self.df_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor()\n",
    "tfms_config = {'remove_bad_meter0_readings_of_first_141days':{}}\n",
    "_df, _vars = processor(ashrae_data['meter_train'], df_building=ashrae_data['building'],\n",
    "                       tfms_configs=tfms_config)\n",
    "display(_df.head(), _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "if loading.N_TRAIN is None: assert len(_df) == len(ashrae_data['meter_train']) - 346112\n",
    "assert (_df.loc[(_df['building_id']==0)&(_df['meter']==0), 'timestamp'] < pd.to_datetime('2016-05-20')).sum() == 0, 'Not correctly removed all first 141 days for meter 0 and site_id 0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "it = inspection.InspectTimeseries(_df, building=ashrae_data['building'],\n",
    "                                  dep_var='meter_reading_log1p')\n",
    "it.inspect_boldly(meter=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing absurdly high meter 2 readings for building 1099"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def fix_bid_1099(self:Processor, threshold:float=10.):\n",
    "    if self.test_run: return\n",
    "    if not self.is_train: return self.df_core\n",
    "    assert 'timestamp' in self.df_core.columns\n",
    "    rm = (self.df_core['building_id'] == 1099)\n",
    "    rm = rm & (self.df_core['meter'] == 2)\n",
    "    rm = rm & (self.df_core[self.dep_var] > threshold)\n",
    "    ok = ~rm\n",
    "    if self.is_train and self.t_train is not None:\n",
    "        ok = ok | ~self.t_in_train_set\n",
    "    logger.info(f'Fixing building_id 1099: removing {(~ok).sum()} data points = {(~ok).sum()/len(ok)*100:.2f} %')\n",
    "    self.df_core = self.df_core.loc[ok,:]\n",
    "    return self.df_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor()\n",
    "tfms_config = {'fix_bid_1099':{}}\n",
    "_df, _vars = processor(ashrae_data['meter_train'], tfms_configs=tfms_config)\n",
    "display(_df.head(), _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "if loading.N_TRAIN is None: assert len(_df) == len(ashrae_data['meter_train']) - 3377\n",
    "assert (_df.loc[(_df['building_id']==1099) & (_df['meter']==2), 'meter_reading_log1p'] > 10).sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor(t_train=t_train)\n",
    "tfms_config = {'fix_bid_1099':{}}\n",
    "_df, _vars = processor(ashrae_data['meter_train'], tfms_configs=tfms_config)\n",
    "display(_df.head(), _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "if loading.N_TRAIN is None: assert len(ashrae_data['meter_train']) - 3377 <= len(_df) < len(ashrae_data['meter_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "it = inspection.InspectTimeseries(_df, building=ashrae_data['building'],\n",
    "                                  dep_var='meter_reading_log1p')\n",
    "it.inspect_boldly(bid=1099, meter=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing imputed weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def remove_imputed_weeks(self:Processor, dep_var='meter_reading'):\n",
    "    if self.test_run: return\n",
    "    if not self.is_train: return self.df_core\n",
    "    grp = ['building_id', 'meter', pd.Grouper(key='timestamp', freq='W-MON')]\n",
    "    wks = (self.df_core.groupby(grp)[dep_var]).describe(percentiles=[.05,.95])\n",
    "\n",
    "    min_date = self.df_core['timestamp'].dt.date.min() - pd.Timedelta(7,unit='w')\n",
    "    max_date = self.df_core['timestamp'].dt.date.max() + pd.Timedelta(7,unit='d')\n",
    "    w_range = pd.date_range(min_date, max_date, freq='W-MON')\n",
    "\n",
    "    self.df_core['week'] = [v.right for v in pd.cut(self.df_core['timestamp'], w_range)]\n",
    "\n",
    "    self.df_core = self.df_core.join(wks.loc[:,['5%','95%']],\n",
    "                                     on=['building_id','meter','week'])\n",
    "    rm = np.isclose(self.df_core['5%'], self.df_core['95%'])\n",
    "    ok = ~rm\n",
    "    if self.is_train and self.t_train is not None:\n",
    "        ok = ok | ~self.t_in_train_set\n",
    "    logger.info(f'Imputed weeks: removing {(~ok).sum()} data points = {(~ok).sum()/len(ok)*100:.2f} %')\n",
    "    self.df_core = self.df_core.loc[ok,:].drop(columns=['5%','95%','week'])\n",
    "    return self.df_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor()\n",
    "tfms_config = {'remove_imputed_weeks':{'dep_var':'meter_reading'}}\n",
    "tmp = ashrae_data['meter_train'].loc[(ashrae_data['meter_train']['building_id']==0)&(ashrae_data['meter_train']['meter']==0)]\n",
    "_df, _vars = processor(tmp, tfms_configs=tfms_config)\n",
    "display(_df.head(), _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "if loading.N_TRAIN is None: assert len(_df) == len(tmp) - 3265"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor(t_train=t_train)\n",
    "tfms_config = {'remove_imputed_weeks':{'dep_var':'meter_reading'}}\n",
    "tmp = ashrae_data['meter_train'].loc[(ashrae_data['meter_train']['building_id']==0)&(ashrae_data['meter_train']['meter']==0)]\n",
    "_df, _vars = processor(tmp, tfms_configs=tfms_config)\n",
    "display(_df.head(), _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert len(tmp) - 3265 <= len(_df) < len(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "it = inspection.InspectTimeseries(_df, building=ashrae_data['building'],\n",
    "                                  dep_var='meter_reading_log1p')\n",
    "it.inspect_boldly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are outliers! 😨 Let's remove them as well. Example `building_id` 60 and `meter` 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "it = inspection.InspectTimeseries(ashrae_data['meter_train'], \n",
    "                                  building=ashrae_data['building'])\n",
    "it.inspect_boldly(bid=60, meter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def remove_outliers(self:Processor, f:float=10, dep_var:str='meter_reading'):\n",
    "    if self.test_run: return None\n",
    "    if not self.is_train: return self.df_core\n",
    "    s = self.df_core.groupby(['building_id','meter'])[dep_var].describe()\n",
    "    s['threshold'] = s['50%'] + (s['75%'] - s['50%']) * f\n",
    "    self.df_core = self.df_core.join(s.loc[:,['threshold']],\n",
    "                                     on=['building_id', 'meter'])\n",
    "    ok = self.df_core[dep_var] < self.df_core['threshold']\n",
    "    if self.is_train and self.t_train is not None:\n",
    "        ok = ok | ~self.t_in_train_set\n",
    "\n",
    "    logger.info(f'Outliers: removing {(~ok).sum()} data points = {(~ok).sum()/len(ok)*100:.2f} %')\n",
    "    self.df_core = self.df_core.loc[ok,:].drop(columns=['threshold'])\n",
    "    return self.df_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor()\n",
    "tfms_config = {'remove_outliers':{'f':10,'dep_var':'meter_reading'}}\n",
    "tmp = ashrae_data['meter_train'].loc[(ashrae_data['meter_train']['building_id']==60)&(ashrae_data['meter_train']['meter']==1)]\n",
    "_df, _vars = processor(tmp, tfms_configs=tfms_config)\n",
    "display(_df.head(), _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "if loading.N_TRAIN is None: assert len(_df) == len(tmp) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor(t_train=t_train)\n",
    "tfms_config = {'remove_outliers':{'f':10,'dep_var':'meter_reading'}}\n",
    "tmp = ashrae_data['meter_train'].loc[(ashrae_data['meter_train']['building_id']==60)&(ashrae_data['meter_train']['meter']==1)]\n",
    "_df, _vars = processor(tmp, tfms_configs=tfms_config)\n",
    "display(_df.head(), _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert  len(tmp) - 1 <= len(_df) <= len(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "it = inspection.InspectTimeseries(_df, building=ashrae_data['building'],\n",
    "                                  dep_var='meter_reading_log1p')\n",
    "it.inspect_boldly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding random noise feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def add_random_noise_features(self:Processor, col:str='random_noise',\n",
    "                              noise_func:typing.Callable=lambda x: stats.norm.rvs(size=x, loc=0, scale=1),\n",
    "                              noise_type:str='cont'):\n",
    "    assert noise_type in ['cont', 'cat']\n",
    "    if self.test_run: return\n",
    "    n = len(self.df_core)\n",
    "    self.df_core[col] = noise_func(n)\n",
    "\n",
    "    if noise_type == 'cont': \n",
    "        self.conts.append(col)\n",
    "        logger.info(f'Added noise feature: \\n\\tcontinuous: {[col]}')\n",
    "    else: \n",
    "        self.cats.append(col)\n",
    "        logger.info(f'Added noise feature: \\n\\tcategorical: {[col]}')\n",
    "        \n",
    "    return self.df_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor()\n",
    "tfms_config = {'add_random_noise_features':{}}\n",
    "tmp = ashrae_data['meter_train'].loc[(ashrae_data['meter_train']['building_id']==60)&(ashrae_data['meter_train']['meter']==1)]\n",
    "_df, _vars = processor(tmp, tfms_configs=tfms_config)\n",
    "display(_df.head(), _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "expected_cols = ['building_id', 'meter', 'timestamp', 'meter_reading_log1p', \n",
    "                 'random_noise']\n",
    "assert list(_df.columns.values) == expected_cols, f'columns {_df.columns.values} did not meet the expected columns: {expected_cols}'\n",
    "assert (_vars['conts'] == ['random_noise'])\n",
    "assert (_vars['cats'] == ['building_id', 'meter',])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding building features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def add_building_features(self:Processor):\n",
    "    if self.test_run: return\n",
    "    n = len(self.df_core)\n",
    "    self.df_core = pd.merge(self.df_core, self.df_building, on='building_id', how='left')\n",
    "    assert n == len(self.df_core)\n",
    "    _cats = ['site_id', 'primary_use']\n",
    "    _conts = ['square_feet', 'year_built', 'floor_count']\n",
    "    logger.info(f'Added building features: \\n\\tcategorical: {_cats}\\n\\tcontinuous: {_conts}')\n",
    "    self.cats.extend(_cats)\n",
    "    self.conts.extend(_conts)\n",
    "    return self.df_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor()\n",
    "tfms_config = {'add_building_features':{}}\n",
    "tmp = ashrae_data['meter_train'].loc[(ashrae_data['meter_train']['building_id']==60)&(ashrae_data['meter_train']['meter']==1)]\n",
    "_df, _vars = processor(tmp, tfms_configs=tfms_config,\n",
    "                       df_building=ashrae_data['building'])\n",
    "display(_df.head(), _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "expected_cols = ['building_id', 'meter', 'timestamp', 'meter_reading_log1p', \n",
    "                 'site_id', 'primary_use', 'square_feet', 'year_built', 'floor_count']\n",
    "assert list(_df.columns.values) == expected_cols, f'columns {_df.columns.values} did not meet the expected columns: {expected_cols}'\n",
    "assert (_vars['conts'] == ['square_feet', 'year_built', 'floor_count'])\n",
    "assert (_vars['cats'] == ['building_id', 'meter', 'site_id', 'primary_use'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding weather features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "site_GMT_offsets = [-5, 0, -7, -5, -8, 0, -5, -5, -5, -6, -7, -5, 0, -6, -5, -5]\n",
    "GMT_offset_map = {site: offset for site, offset in enumerate(site_GMT_offsets)}\n",
    "\n",
    "@patch\n",
    "def add_weather_features(self:Processor,\n",
    "                         fix_time_offset:bool=False,\n",
    "                         add_na_indicators:bool=False,\n",
    "                         impute_nas:bool=False):\n",
    "    if self.test_run: return\n",
    "    n = len(self.df_core)\n",
    "    add_site_id = 'site_id' not in self.df_core.columns\n",
    "    if add_site_id:\n",
    "        self.df_core = self.df_core.join(self.df_building.set_index('building_id').loc[:,['site_id']],\n",
    "                                         on='building_id')\n",
    "\n",
    "    if fix_time_offset:\n",
    "        dt = (self.df_weather['site_id']\n",
    "              .map(GMT_offset_map)\n",
    "              .apply(lambda x: pd.Timedelta(x, unit='hours')))\n",
    "        self.df_weather['timestamp'] = self.df_weather['timestamp'] + dt\n",
    "\n",
    "    cols = ['air_temperature', 'cloud_coverage', 'dew_temperature',\n",
    "            'precip_depth_1_hr', 'sea_level_pressure', 'wind_direction',\n",
    "            'wind_speed']\n",
    "\n",
    "    # adding na columns\n",
    "    na_cols = []\n",
    "    if add_na_indicators:\n",
    "        for col in cols:\n",
    "            nas = self.df_weather[col].isna()\n",
    "            na_col = f'{col}_na(processor)'\n",
    "            if nas.sum()>0:\n",
    "                self.df_weather[na_col] = nas\n",
    "                self.df_weather[na_col] = self.df_weather[na_col].astype(bool)\n",
    "                na_cols.append(na_col)\n",
    "\n",
    "    # imputing na columns\n",
    "    if impute_nas:\n",
    "        new_weather = []\n",
    "        aggs = {col: self.df_weather[col].median()\n",
    "                for col in cols}\n",
    "        for site, grp in self.df_weather.groupby('site_id'):\n",
    "            grp = grp.sort_values('timestamp')\n",
    "            for col in cols:\n",
    "                nas = grp[col].isna()\n",
    "                if nas.sum() == len(grp):\n",
    "                    grp[col] = aggs[col]\n",
    "                elif nas.sum() > 0:\n",
    "                    grp[col] = grp[col].interpolate(limit_direction='both',\n",
    "                                                    method='linear')\n",
    "                nas = grp[col].isna()\n",
    "                grp.loc[nas, col] = aggs[col]\n",
    "\n",
    "            new_weather.append(grp)\n",
    "        n_weather = len(self.df_weather)\n",
    "        self.df_weather = pd.concat(new_weather)\n",
    "        assert len(self.df_weather) == n_weather, f'Interpolation step changed rows from {n_weather} to {len(self.df_weather)}'\n",
    "\n",
    "    self.df_core = pd.merge(self.df_core, self.df_weather,\n",
    "                            on=['site_id', 'timestamp'],\n",
    "                            how='left')\n",
    "    assert n == len(self.df_core), f'Merging lead to an increase from {n} rows to {len(self.df_core)}'\n",
    "\n",
    "    if add_site_id:\n",
    "        self.df_core.drop(columns=['site_id'], inplace=True)\n",
    "    \n",
    "    _conts = ['wind_direction', 'air_temperature', 'dew_temperature', 'precip_depth_1_hr',\n",
    "              'sea_level_pressure', 'wind_speed', 'cloud_coverage']\n",
    "    self.cats.extend(na_cols)\n",
    "#     self.cats_order['cloud_coverage'] = sorted([v for v in self.df_core['cloud_coverage'].unique() if np.isfinite(v)])\n",
    "    self.conts.extend(_conts)\n",
    "    logger.info(f'Added weather features: \\n\\tcategorical: {na_cols}\\n\\tcontinuous: {_conts}')\n",
    "    return self.df_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor()\n",
    "tfms_config = {'add_weather_features':{'fix_time_offset':False,\n",
    "                                       'add_na_indicators':False,\n",
    "                                       'impute_nas':False}}\n",
    "tmp = ashrae_data['meter_train'].loc[(ashrae_data['meter_train']['building_id']==60)&(ashrae_data['meter_train']['meter']==1)]\n",
    "_df, _vars = processor(tmp, tfms_configs=tfms_config,\n",
    "                       df_building=ashrae_data['building'],\n",
    "                       df_weather=ashrae_data['weather_train'])\n",
    "display(_df.head().T, _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "expected_cols = ['building_id', 'meter', 'timestamp', 'meter_reading_log1p', \n",
    "                 'air_temperature', 'cloud_coverage', 'dew_temperature', \n",
    "                 'precip_depth_1_hr', 'sea_level_pressure',\n",
    "                 'wind_direction', 'wind_speed']\n",
    "assert list(_df.columns.values) == expected_cols, f'columns {_df.columns.values} did not meet the expected columns: {expected_cols}'\n",
    "assert (_vars['conts'] == ['wind_direction', 'air_temperature',\n",
    "                           'dew_temperature', 'precip_depth_1_hr',\n",
    "                           'sea_level_pressure',  'wind_speed',\n",
    "                           'cloud_coverage'])\n",
    "assert (_vars['cats'] == ['building_id', 'meter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor()\n",
    "tfms_config = {'add_weather_features':{'fix_time_offset':True,\n",
    "                                       'add_na_indicators':False,\n",
    "                                       'impute_nas':False}}\n",
    "tmp = ashrae_data['meter_train'].loc[(ashrae_data['meter_train']['building_id']==60)&(ashrae_data['meter_train']['meter']==1)]\n",
    "_df, _vars = processor(tmp, tfms_configs=tfms_config,\n",
    "                       df_building=ashrae_data['building'],\n",
    "                       df_weather=ashrae_data['weather_train'])\n",
    "display(_df.head().T, _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "expected_cols = ['building_id', 'meter', 'timestamp', 'meter_reading_log1p', \n",
    "                 'air_temperature', 'cloud_coverage', 'dew_temperature', \n",
    "                 'precip_depth_1_hr', 'sea_level_pressure',\n",
    "                 'wind_direction', 'wind_speed']\n",
    "assert list(_df.columns.values) == expected_cols, f'columns {_df.columns.values} did not meet the expected columns: {expected_cols}'\n",
    "assert (_vars['conts'] == ['wind_direction', 'air_temperature',\n",
    "                           'dew_temperature', 'precip_depth_1_hr',\n",
    "                           'sea_level_pressure',  'wind_speed',\n",
    "                           'cloud_coverage'])\n",
    "assert (_vars['cats'] == ['building_id', 'meter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor()\n",
    "tfms_config = {'add_weather_features':{'fix_time_offset':True,\n",
    "                                       'add_na_indicators':True,\n",
    "                                       'impute_nas':False}}\n",
    "tmp = ashrae_data['meter_train'].loc[(ashrae_data['meter_train']['building_id']==60)&(ashrae_data['meter_train']['meter']==1)]\n",
    "_df, _vars = processor(tmp, tfms_configs=tfms_config,\n",
    "                       df_building=ashrae_data['building'],\n",
    "                       df_weather=ashrae_data['weather_train'])\n",
    "display(_df.head().T, _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "expected_cols = ['building_id', 'meter', 'timestamp', 'meter_reading_log1p', \n",
    "                 'air_temperature', 'cloud_coverage', 'dew_temperature', \n",
    "                 'precip_depth_1_hr', 'sea_level_pressure',\n",
    "                 'wind_direction', 'wind_speed',\n",
    "                 'air_temperature_na(processor)', 'cloud_coverage_na(processor)',\n",
    "                 'dew_temperature_na(processor)', 'precip_depth_1_hr_na(processor)',\n",
    "                 'sea_level_pressure_na(processor)', 'wind_direction_na(processor)',\n",
    "                 'wind_speed_na(processor)']\n",
    "assert list(_df.columns.values) == expected_cols, f'columns {_df.columns.values} did not meet the expected columns: {expected_cols}'\n",
    "assert (_vars['conts'] == ['wind_direction', 'air_temperature',\n",
    "                           'dew_temperature', 'precip_depth_1_hr',\n",
    "                           'sea_level_pressure',  'wind_speed', 'cloud_coverage'])\n",
    "assert (_vars['cats'] == ['building_id', 'meter',\n",
    "                          'air_temperature_na(processor)', 'cloud_coverage_na(processor)',\n",
    "                          'dew_temperature_na(processor)', 'precip_depth_1_hr_na(processor)',\n",
    "                          'sea_level_pressure_na(processor)', 'wind_direction_na(processor)',\n",
    "                          'wind_speed_na(processor)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor()\n",
    "tfms_config = {'add_weather_features':{'fix_time_offset':True,\n",
    "                                       'add_na_indicators':False,\n",
    "                                       'impute_nas':True}}\n",
    "tmp = ashrae_data['meter_train'].loc[(ashrae_data['meter_train']['building_id']==60)&(ashrae_data['meter_train']['meter']==1)]\n",
    "_df, _vars = processor(tmp, tfms_configs=tfms_config,\n",
    "                       df_building=ashrae_data['building'],\n",
    "                       df_weather=ashrae_data['weather_train'])\n",
    "display(_df.head().T, _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "expected_cols = ['building_id', 'meter', 'timestamp', 'meter_reading_log1p', \n",
    "                 'air_temperature', 'cloud_coverage', 'dew_temperature', \n",
    "                 'precip_depth_1_hr', 'sea_level_pressure',\n",
    "                 'wind_direction', 'wind_speed']\n",
    "assert list(_df.columns.values) == expected_cols, f'columns {_df.columns.values} did not meet the expected columns: {expected_cols}'\n",
    "assert (_vars['conts'] == ['wind_direction', 'air_temperature',\n",
    "                           'dew_temperature', 'precip_depth_1_hr',\n",
    "                           'sea_level_pressure',  'wind_speed',\n",
    "                           'cloud_coverage'])\n",
    "assert (_vars['cats'] == ['building_id', 'meter'])\n",
    "assert _df[expected_cols[:4]].isna().sum().sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def add_time_features(self:Processor):\n",
    "    if self.test_run: return\n",
    "    _cats = ['timestampMonth', 'timestampDay', 'timestampWeek', 'timestampDayofweek',\n",
    "                      'timestampDayofyear', 'timestampIs_month_end', 'timestampIs_month_start',\n",
    "                      'timestampIs_quarter_start', 'timestampIs_quarter_end',\n",
    "                      'timestampIs_year_start', 'timestampIs_year_end', 'timestampHour',\n",
    "                      'timestampIs_us_holiday']\n",
    "    self.cats.extend(_cats)\n",
    "\n",
    "    self.df_core = add_datepart(self.df_core, self.time_col, drop=False)\n",
    "\n",
    "    self.df_core['timestampHour'] = self.df_core[self.time_col].dt.hour\n",
    "\n",
    "    dates_range = pd.date_range(start='2015-12-31', end='2019-01-01')\n",
    "    us_holidays = us_calendar().holidays(start=dates_range.min(), end=dates_range.max())\n",
    "\n",
    "    self.df_core['timestampIs_us_holiday'] = (self.df_core['timestamp'].dt.date.astype('datetime64')\n",
    "                                              .isin(us_holidays)\n",
    "                                              .astype(bool))\n",
    "    logger.info(f'Added categorical time features: {_cats}')\n",
    "    self.cats_order.update({\n",
    "        c: sorted(self.df_core[c].unique()) for c in ['timestampMonth', 'timestampDay',\n",
    "                                                      'timestampWeek', 'timestampDayofweek',\n",
    "                                                      'timestampDayofyear', 'timestampHour']\n",
    "    })\n",
    "    return self.df_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor()\n",
    "tfms_config = {'add_time_features':{}}\n",
    "tmp = ashrae_data['meter_train'].loc[(ashrae_data['meter_train']['building_id']==60)&(ashrae_data['meter_train']['meter']==1)]\n",
    "_df, _vars = processor(tmp, tfms_configs=tfms_config)\n",
    "display(_df.head().T, _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "expected_cols = ['building_id', 'meter', 'timestamp', 'timestampWeek', 'meter_reading_log1p', \n",
    "                 'timestampMonth', 'timestampDay', 'timestampDayofweek',\n",
    "                 'timestampDayofyear', 'timestampIs_month_end', 'timestampIs_month_start',\n",
    "                 'timestampIs_quarter_end', 'timestampIs_quarter_start',\n",
    "                 'timestampIs_year_end', 'timestampIs_year_start', 'timestampElapsed',\n",
    "                 'timestampHour', 'timestampIs_us_holiday']\n",
    "assert list(_df.columns.values) == expected_cols, f'columns {_df.columns.values} did not meet the expected columns: {expected_cols}'\n",
    "assert (_vars['conts'] == [])\n",
    "assert (_vars['cats'] == ['building_id', 'meter', 'timestampMonth',\n",
    "                          'timestampDay', 'timestampWeek', 'timestampDayofweek',\n",
    "                          'timestampDayofyear', 'timestampIs_month_end', \n",
    "                          'timestampIs_month_start', 'timestampIs_quarter_start', \n",
    "                          'timestampIs_quarter_end', 'timestampIs_year_start',\n",
    "                          'timestampIs_year_end', 'timestampHour',\n",
    "                          'timestampIs_us_holiday'])\n",
    "assert len(_df) == len(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding polar transformed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def add_polar_transformed_features_single_col(self:Processor, col:str, lb:float, ub:float):\n",
    "    if self.test_run: return\n",
    "    \n",
    "    x_col = f'{col}_x'\n",
    "    y_col = f'{col}_y'\n",
    "    _conts = [x_col, y_col]\n",
    "    \n",
    "    self.df_core[x_col] = self.df_core[col].apply(lambda x: np.cos((x-lb)*2*np.pi/(ub-lb)))\n",
    "    self.df_core[y_col] = self.df_core[col].apply(lambda x: np.sin((x-lb)*2*np.pi/(ub-lb)))\n",
    "    \n",
    "    self.conts.extend(_conts)\n",
    "    logger.info(f'Added continuous features: {col} -> {_conts}')\n",
    "    \n",
    "    return self.df_core\n",
    "\n",
    "@patch\n",
    "def add_polar_transformed_features_multiple_cols(self:Processor, cols:typing.List[str], bounds:typing.List[tuple]):\n",
    "    if self.test_run: return\n",
    "    assert len(cols) == len(bounds)\n",
    "    \n",
    "    for col, (lb, ub) in zip(cols, bounds):\n",
    "        self.df_core = self.add_polar_transformed_features_single_col(col, lb, ub)\n",
    "    logger.info(f'Added continuous features for {cols}')\n",
    "    \n",
    "    return self.df_core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor()\n",
    "tfms_config = {\n",
    "    'add_time_features':{},\n",
    "    'add_polar_transformed_features_single_col':{'col':'timestampHour', 'lb':0, 'ub':24}}\n",
    "tmp = ashrae_data['meter_train'].loc[(ashrae_data['meter_train']['building_id']==60)&(ashrae_data['meter_train']['meter']==1)]\n",
    "_df, _vars = processor(tmp, tfms_configs=tfms_config)\n",
    "display(_df.head().T, _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert (_vars['conts'] == ['timestampHour_x', 'timestampHour_y'])\n",
    "assert len(_df) == len(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor()\n",
    "tfms_config = {\n",
    "    'add_time_features':{},\n",
    "    'add_polar_transformed_features_multiple_cols':{'cols':['timestampHour', 'timestampDayofyear'], 'bounds':[(0,24), (1,365)]}},\n",
    "tmp = ashrae_data['meter_train'].loc[(ashrae_data['meter_train']['building_id']==60)&(ashrae_data['meter_train']['meter']==1)]\n",
    "_df, _vars = processor(tmp, tfms_configs=tfms_config)\n",
    "display(_df.head().T, _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert (_vars['conts'] == ['timestampHour_x', 'timestampHour_y',\n",
    "                           'timestampDayofyear_x', 'timestampDayofyear_y'])\n",
    "assert len(_df) == len(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding statistics of target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "DEFAULT_GRP_COLS = ['building_id', 'timestampHour', 'meter']\n",
    "\n",
    "@patch\n",
    "def add_dep_var_stats(self:Processor, grp_cols:typing.List[str]=None):\n",
    "    if self.test_run: return\n",
    "\n",
    "    grp_cols = DEFAULT_GRP_COLS if grp_cols is None else grp_cols\n",
    "\n",
    "    assert self.is_train or self.dep_var_stats is not None\n",
    "    if self.is_train:\n",
    "        self.dep_var_stats = dict()\n",
    "\n",
    "    funs = {\n",
    "        'median': lambda x: torch.median(tensor(x)).item(),\n",
    "        'mean': lambda x: torch.mean(tensor(x)).item(),\n",
    "        '5%': lambda x: np.percentile(x, 5),\n",
    "        '95%': lambda x: np.percentile(x, 95),\n",
    "    }\n",
    "    _conts = []\n",
    "    # computing stats for self.dep_var on the coarsest possible level\n",
    "    for name, fun in funs.items():\n",
    "        name = f'{self.dep_var}_{name}'\n",
    "        _conts.append(name)\n",
    "        self.conts.append(name)\n",
    "\n",
    "        if self.is_train:\n",
    "            value = fun(self.df_core[self.dep_var].values)\n",
    "            self.df_core[name] = value\n",
    "            self.dep_var_stats[name] = value\n",
    "        else:\n",
    "            self.df_core[name] = self.dep_var_stats[name]\n",
    "\n",
    "    # adding stats of self.dep_var on a more granular level\n",
    "    if grp_cols is not None:\n",
    "        t_col = 'timestampHour'\n",
    "        do_add_t = t_col in grp_cols and t_col not in self.df_core.columns.values\n",
    "        if do_add_t:\n",
    "            self.df_core[t_col] = self.df_core['timestamp'].dt.hour\n",
    "\n",
    "        assert all([c in self.df_core.columns.values for c in grp_cols])\n",
    "\n",
    "        for fun_name, fun in funs.items():\n",
    "            name = f'{self.dep_var}_{\"-\".join(grp_cols)}_{fun_name}'\n",
    "            _conts.append(name)\n",
    "            self.conts.append(name)\n",
    "\n",
    "            if self.is_train:\n",
    "\n",
    "                self.dep_var_stats[name] = (self.df_core.groupby(grp_cols)[self.dep_var]\n",
    "                                            .agg(fun)\n",
    "                                            .rename(name))\n",
    "            self.df_core = self.df_core.join(self.dep_var_stats[name], on=grp_cols)\n",
    "            self.df_core[name].fillna(self.dep_var_stats[f'{self.dep_var}_{fun_name}'], inplace=True)\n",
    "\n",
    "        if do_add_t:\n",
    "            self.df_core.drop(columns=[t_col], inplace=True)\n",
    "    logger.info(f'Added continuous target columns: {_conts}')\n",
    "    return self.df_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor()\n",
    "tfms_config = {'add_dep_var_stats':{}}\n",
    "mask = (ashrae_data['meter_train']['building_id']==60)&(ashrae_data['meter_train']['meter']==1)\n",
    "\n",
    "tmp = ashrae_data['meter_train'].loc[mask]\n",
    "_df, _vars = processor(tmp, tfms_configs=tfms_config)\n",
    "\n",
    "display(_df.head().T, _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mask = (ashrae_data['meter_test']['building_id']==60)&(ashrae_data['meter_test']['meter']==1)\n",
    "tmp_test = ashrae_data['meter_test'].loc[mask]\n",
    "_df_test, _ = processor(tmp_test, tfms_configs=tfms_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "expected_cols = ['building_id', 'meter', 'timestamp', 'meter_reading_log1p', \n",
    "                 'meter_reading_log1p_median', 'meter_reading_log1p_mean',\n",
    "                 'meter_reading_log1p_5%', 'meter_reading_log1p_95%', \n",
    "                 'meter_reading_log1p_building_id-timestampHour-meter_median',\n",
    "                 'meter_reading_log1p_building_id-timestampHour-meter_mean',\n",
    "                 'meter_reading_log1p_building_id-timestampHour-meter_5%',\n",
    "                 'meter_reading_log1p_building_id-timestampHour-meter_95%']\n",
    "assert list(_df.columns.values) == expected_cols, f'columns {_df.columns.values} did not meet the expected columns: {expected_cols}'\n",
    "assert (_vars['conts'] == ['meter_reading_log1p_median',\n",
    "                           'meter_reading_log1p_mean',\n",
    "                           'meter_reading_log1p_5%',\n",
    "                           'meter_reading_log1p_95%', \n",
    "                           'meter_reading_log1p_building_id-timestampHour-meter_median',\n",
    "                           'meter_reading_log1p_building_id-timestampHour-meter_mean',\n",
    "                           'meter_reading_log1p_building_id-timestampHour-meter_5%',\n",
    "                           'meter_reading_log1p_building_id-timestampHour-meter_95%'])\n",
    "assert (_vars['cats'] == ['building_id', 'meter'])\n",
    "assert len(_df) == len(tmp)\n",
    "assert all(loading.show_nans(_df_test)['# NaNs'] == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding one hot encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "DEFAULT_ONEHOT_COLS = ['meter']\n",
    "\n",
    "@patch\n",
    "def add_onehot_encoded(self:Processor, onehot_cols:typing.List[str]=None):\n",
    "    if self.test_run: return\n",
    "    onehot_cols = DEFAULT_ONEHOT_COLS if onehot_cols is None else onehot_cols\n",
    "\n",
    "    t_col = 'timestampHour'\n",
    "    do_add_t = t_col in onehot_cols and t_col not in self.df_core.columns.values\n",
    "    if do_add_t:\n",
    "        self.df_core[t_col] = self.df_core['timestamp'].dt.hour\n",
    "\n",
    "    self.df_core['id'] = [str(v) for v in zip(*[self.df_core[v] for v in onehot_cols])]\n",
    "\n",
    "    if self.is_train:\n",
    "        self.onehot_cols = onehot_cols\n",
    "        self.onehot_tfm = OneHotEncoder()\n",
    "        self.onehot_tfm.fit(self.df_core.loc[:, ['id']])\n",
    "\n",
    "\n",
    "    names = [f'{\"-\".join(self.onehot_cols)}_{v}' for v in self.onehot_tfm.categories_[0]]\n",
    "    \n",
    "    self.cats.extend(names)\n",
    "\n",
    "    df_onehot = pd.DataFrame(self.onehot_tfm.transform(self.df_core.loc[:, ['id']]).toarray(),\n",
    "                             columns=names, index=self.df_core.index, dtype=bool)\n",
    "    logger.info(f'Added one hot encoded features: {names}')\n",
    "    to_drop = ['id']\n",
    "    if do_add_t:\n",
    "        to_drop.append(t_col)\n",
    "    self.df_core.drop(columns=to_drop, inplace=True)\n",
    "    self.df_core = pd.concat((self.df_core, df_onehot), axis=1)\n",
    "    return self.df_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor()\n",
    "tfms_config = {'add_onehot_encoded':{}}\n",
    "tmp = ashrae_data['meter_train'].loc[(ashrae_data['meter_train']['building_id']<=60)]\n",
    "_df, _vars = processor(tmp, tfms_configs=tfms_config)\n",
    "display(_df.head(), _vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "expected_cols = ['building_id', 'meter', 'timestamp', 'meter_reading_log1p', \n",
    "                 'meter_(0,)', 'meter_(1,)']\n",
    "assert list(_df.columns.values) == expected_cols, f'columns {_df.columns.values} did not meet the expected columns: {expected_cols}'\n",
    "assert (_vars['conts'] == [])\n",
    "assert (_vars['cats'] == ['building_id', 'meter', 'meter_(0,)', 'meter_(1,)'])\n",
    "assert len(_df) == len(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting between kBTU and kWh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "site 0 and meter 0 have the unit kBTU instead of kWh: [post](https://www.kaggle.com/c/ashrae-energy-prediction/discussion/119261)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "buildings affected by the unit conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "site0_bids = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104]\n",
    "kBTU_to_kWh = 0.2931\n",
    "kWh_to_kBTU = 1/kBTU_to_kWh\n",
    "\n",
    "def convert_meter_reading(df:pd.DataFrame, bids, c:float, meter:int=0, \n",
    "                          dep_var:str='meter_reading'):\n",
    "    mask = (df['building_id'].isin(bids)) & (df['meter'] == meter)\n",
    "    df.loc[mask, dep_var] = c * df.loc[mask, dep_var]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tmp = ashrae_data['meter_train'].loc[(ashrae_data['meter_train']['building_id'].isin(site0_bids))].copy()\n",
    "convert_meter_reading(tmp, site0_bids, \n",
    "                      kBTU_to_kWh).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "@patch\n",
    "def convert_kBTU_to_kWh(self:Processor):\n",
    "    if self.test_run: return\n",
    "    if self.is_train:\n",
    "        self.df_core = convert_meter_reading(self.df_core, \n",
    "                                             site0_bids,\n",
    "                                             kBTU_to_kWh,\n",
    "                                             dep_var=self.dep_var)\n",
    "    logger.info('converted meter readings for meter 0 and site_id 0 from kBTU to kWh')\n",
    "    return self.df_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor()\n",
    "tfms_config = {'convert_kBTU_to_kWh':{}}\n",
    "_df, _vars = processor(tmp, tfms_configs=tfms_config)\n",
    "display(_df.head(), _vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying multiple processing steps through part of the train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = Processor(t_train=t_train)\n",
    "tfms_config = {\n",
    "    'add_time_features':{},\n",
    "    'add_weather_features':{'fix_time_offset':True,\n",
    "                            'add_na_indicators':True,\n",
    "                            'impute_nas':True},\n",
    "    'add_building_features':{},\n",
    "}\n",
    "tmp = ashrae_data['meter_train']\n",
    "df_train, var_names = processor(tmp, tfms_configs=tfms_config,\n",
    "                                df_weather=ashrae_data['weather_train'],\n",
    "                                df_building=ashrae_data['building'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running through part of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tmp = ashrae_data['meter_test']\n",
    "df_test, _ = processor(tmp, tfms_configs=tfms_config,\n",
    "                       df_weather=ashrae_data['weather_test'],\n",
    "                       df_building=ashrae_data['building'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sure the columns are aligned in train/var and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def align_test(df_train:pd.DataFrame, var_names:dict,\n",
    "               df_test:pd.DataFrame):\n",
    "    return df_test.loc[:,[v for v in df_train.columns if v != var_names['dep_var']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_test = align_test(df_train, var_names, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "a, b = list(df_test.columns.values), [v for v in df_train.columns.values if v != _vars['dep_var']]\n",
    "assert a == b, f'Columns are mismatching! \\n\\tIn train but not test: {set(b)-set(a)}\\n\\tIn test but not train: {set(a)-set(b)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying processing to all samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done testing. Let's apply the transforms to the entire data set. Takes about 20min with `remove_imputed_weeks`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity checking processing results and storing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert len(df_test.columns) + 1 == len(df_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def test_var_names(var_names:dict):\n",
    "    assert isinstance(var_names, dict)\n",
    "    assert 'conts' in var_names and 'cats' in var_names and 'dep_var' in var_names\n",
    "    assert isinstance(var_names['conts'], list)\n",
    "    assert isinstance(var_names['cats'], list)\n",
    "    assert isinstance(var_names['dep_var'], str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_var_names(var_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def store_var_names(data_path:Path, var_names:dict):\n",
    "    fname = data_path/'var_names.pckl'\n",
    "    print(f'Storing var names at: {fname}')\n",
    "    with open(fname, 'wb') as f:\n",
    "        pickle.dump(var_names, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "store_var_names(data_path, var_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_var_names(fname:Path):\n",
    "    print(f'Reading var names at: {fname}')\n",
    "    with open(fname, 'rb') as f:\n",
    "        var_names = pickle.load(f)\n",
    "    return var_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# var_names = load_var_names(data_path/'var_names.pckl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_var_names(var_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def store_df(path:Path, df:pd.DataFrame): df.to_parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "store_df(data_path/'X.parquet', df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "store_df(data_path/'X_test.parquet', df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_df(path:Path): return pd.read_parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_train = load_df(data_path/'X.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All processing in one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "@typed\n",
    "def preprocess_all(ashrae_data:dict, tfms_config:dict):\n",
    "    \n",
    "    processor = Processor() # t_train=t_train\n",
    "\n",
    "    df, var_names = processor(ashrae_data['meter_train'], tfms_configs=tfms_config,\n",
    "                              df_weather=ashrae_data['weather_train'],\n",
    "                              df_building=ashrae_data['building'])\n",
    "\n",
    "    df_test_p, _ = processor(ashrae_data['meter_test'], tfms_configs=tfms_config,\n",
    "                             df_weather=ashrae_data['weather_test'],\n",
    "                             df_building=ashrae_data['building'])\n",
    "\n",
    "    test_nans = loading.show_nans(df_test_p)\n",
    "    train_nans = loading.show_nans(df)\n",
    "\n",
    "    test_nan_cols = [col for col in test_nans.loc[test_nans['# NaNs']>0].index]\n",
    "    assert (train_nans.loc[train_nans.index.isin(test_nan_cols),'# NaNs'] == 0).sum() == 0\n",
    "\n",
    "    df_test = align_test(df, var_names, df_test_p)\n",
    "    assert len(df_test) == len(ashrae_data['meter_test'])\n",
    "\n",
    "    assert len(df_test.columns) + 1 == len(df.columns)\n",
    "\n",
    "    test_var_names(var_names)\n",
    "    \n",
    "    return df, df_test, var_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tfms_config = {\n",
    "    'add_random_noise_features':{},\n",
    "    'add_time_features':{},\n",
    "    'add_weather_features':{'fix_time_offset':True,\n",
    "                            'add_na_indicators':True,\n",
    "                            'impute_nas':True},\n",
    "    'add_building_features':{},\n",
    "}\n",
    "\n",
    "df_train, df_test, var_names = preprocess_all(ashrae_data, tfms_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38_fastai]",
   "language": "python",
   "name": "conda-env-py38_fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "294px",
    "width": "313px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
