{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling & submitting\n",
    "\n",
    "> Playing with different models and submitting predictions over the test set to kaggle. Predicting each meter individually. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current implementation of this notebook leads to (private leaderboard score): \n",
    "- baseline (linear regression on dep_var_stats and meter 1hot) 1.7\n",
    "- RandomForest 1.65, \n",
    "- tabular_learner 1.55 and \n",
    "- lgbm 1.67. \n",
    "\n",
    "Those scores relate to a validation set error (`nb score`) of .8 - 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from ashrae import preprocessing\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import typing\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn import linear_model, tree, model_selection, ensemble\n",
    "\n",
    "from fastai.tabular.all import *\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.plotting.backend = \"plotly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_test = True\n",
    "do_submit = True\n",
    "data_path = Path(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def evaluate_torch(y_true:torch.Tensor, y_pred:torch.Tensor): return torch.sqrt(F.mse_loss(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "cnr = lambda x: x.clone().numpy().ravel() # clone numpy ravel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "var_names = preprocessing.load_var_names(data_path/'var_names.pckl')\n",
    "var_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df = preprocessing.load_df(data_path/'X.parquet')#.sample(100000)\n",
    "\n",
    "if do_test:\n",
    "    df_test = preprocessing.load_df(data_path/'X_test.parquet')#.sample(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling `df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n = len(df)\n",
    "\n",
    "if False: # per building_id and meter sampling\n",
    "    n_sample_per_bid = 500\n",
    "    replace = True\n",
    "\n",
    "    df = (df.groupby(['building_id', 'meter'])\n",
    "         .sample(n=n_sample_per_bid, replace=replace))\n",
    "\n",
    "if False: # general sampling\n",
    "    frac_samples = .9\n",
    "    replace = False\n",
    "\n",
    "    df = (df.sample(frac=frac_samples, replace=replace))\n",
    "\n",
    "print(f'using {len(df)} samples = {len(df)/n*100:.2f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "var_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "var_names_no_anns = {\n",
    "    'dep_var': var_names['dep_var'],\n",
    "    'conts': [v for v in var_names['conts'] if 'meter_reading' in v],\n",
    "    'cats': [v for v in var_names['cats'] if v.startswith('meter_')]\n",
    "}\n",
    "var_names_no_anns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "var_names_anns = {\n",
    "    'dep_var': var_names['dep_var'],\n",
    "    'conts': var_names['conts'],\n",
    "    'cats': [v for v in var_names['cats'] if not v.startswith('meter_')]\n",
    "}\n",
    "var_names_anns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t_train = pd.read_parquet(data_path/'t_train.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "split_kind = 'random'\n",
    "#split_kind = 'time'\n",
    "# split_kind = 'fix_time'\n",
    "# split_kind = 'time_split_day'\n",
    "\n",
    "# t_train = None\n",
    "train_frac = .8\n",
    "meter_train_samples = {}\n",
    "\n",
    "all_splits = {}\n",
    "for meter, _df in df.groupby('meter'):\n",
    "    splits = preprocessing.split_dataset(_df, split_kind=split_kind, \n",
    "                                         train_frac=train_frac,\n",
    "                                         t_train=t_train)\n",
    "    all_splits[meter] = splits\n",
    "    meter_train_samples[meter] = len(splits[0])\n",
    "    print(f'meter: {meter} â‡’ sets {len(splits)}, train {len(splits[0])} = {len(splits[0])/len(df):.4f}, valid {len(splits[1])} = {len(splits[1])/len(_df):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# procs = [] \n",
    "procs = [FillMissing, Normalize, Categorify]\n",
    "\n",
    "tos = {}\n",
    "for meter, _df in df.groupby('meter'):\n",
    "    print(meter)\n",
    "    splits = all_splits[meter]\n",
    "    display(_df.head())\n",
    "    \n",
    "    tos[meter] = preprocessing.get_tabular_object(_df,\n",
    "                                          var_names_anns,\n",
    "    #                                       var_names_no_anns,\n",
    "    #                                       var_names, \n",
    "                                          splits=splits,\n",
    "                                          procs=procs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meter_train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# train_bs = 256\n",
    "# val_bs = 256\n",
    "train_bs = val_bs = {\n",
    "    0: 100000,\n",
    "    1: 40000,\n",
    "    2: 20000,\n",
    "    3: 10000,\n",
    "}\n",
    "\n",
    "all_dls = {meter: to.dataloaders(bs=train_bs[meter], val_bs=val_bs[meter])\n",
    "           for meter, to in tos.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: Takes about 12min with the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_bs = 4096*4\n",
    "\n",
    "if do_test:\n",
    "    all_test_dls = {meter: dls.test_dl(df_test.loc[df_test['meter']==meter,:], bs=test_bs) \n",
    "                    for meter, dls in all_dls.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_ys(ys:typing.List[tuple]):\n",
    "    'preds is a list of tuples, each of size 2. the first entry is the index and the second the predictions'\n",
    "    y = pd.Series(np.concatenate([_y for (_,_y) in ys]),\n",
    "                  index=np.concatenate([_ix for (_ix,_) in ys]))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "params = {'n_estimators': 20, 'max_features': 'sqrt'}\n",
    "model = ensemble.RandomForestRegressor\n",
    "# params = {}\n",
    "# model = linear_model.LinearRegression\n",
    "\n",
    "ms = {meter: model(**params) for meter in tos}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for meter in ms:\n",
    "    ms[meter].fit(tos[meter].train.xs.values, \n",
    "                 tos[meter].train.ys.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "y_valid_preds = [(tos[meter].valid.xs.index, ms[meter].predict(tos[meter].valid.xs.values))\n",
    "                 for meter in tos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_valid_pred = sort_ys(y_valid_preds)\n",
    "y_valid_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if do_test:\n",
    "    y_test_preds = [(all_test_dls[meter].xs.index, \n",
    "                     ms[meter].predict(all_test_dls[meter].xs))\n",
    "                     for meter in all_test_dls]\n",
    "    y_test_pred = sort_ys(y_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_valid_true = [(tos[meter].valid.ys.index, tos[meter].valid.ys.values.ravel())\n",
    "                for meter in tos]\n",
    "y_valid_true = sort_ys(y_valid_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_valid_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nb_score = evaluate_torch(torch.from_numpy(y_valid_true.values), \n",
    "                          torch.from_numpy(y_valid_pred.values)).item()\n",
    "print(f'sklearn loss {nb_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `fastai`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fastai finding: make sure your test set values are not out of domain $\\Rightarrow$ `timestampYear` in this notebook is put into the training set but there only takes on the value 2016.0, but in the test set it's 2017.0 and 2018.0, causing the predictions to zero out everywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ranges = {meter: [0, np.max([to.train.ys.values.max(), to.valid.ys.values.max()])]\n",
    "            for meter, to in tos.items()}\n",
    "y_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Swish(nn.ReLU):\n",
    "    def forward(self, input:Tensor) -> Tensor:\n",
    "        if self.inplace:\n",
    "            res = input.clone()\n",
    "            torch.sigmoid_(res)\n",
    "            input *= res\n",
    "            return input\n",
    "        else:\n",
    "            return torch.sigmoid(input) * input\n",
    "    \n",
    "class Sine(nn.ReLU):\n",
    "    def forward(self, input:Tensor) -> Tensor:\n",
    "        if self.inplace:\n",
    "            return torch.sin_(input)\n",
    "        else:\n",
    "            return torch.sin(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [500, 250, 125]\n",
    "\n",
    "embed_p = .1\n",
    "# embed_p = 0.\n",
    "\n",
    "# ps = [.1, .1, .1, .1, .1]\n",
    "ps = [.1 for _ in layers]\n",
    "ps[0] = .2\n",
    "\n",
    "# config = None\n",
    "config = tabular_config(embed_p=embed_p, ps=ps,\n",
    "#                         act_cls=Swish(inplace=True)\n",
    "                        )\n",
    "# config = tabular_config(act_cls=nn.ReLU(inplace=True))\n",
    "# config = tabular_config(act_cls=Swish(inplace=True))\n",
    "# config = tabular_config(act_cls=Sine(inplace=True))\n",
    "\n",
    "learners = {meter: tabular_learner(dls, y_range=y_ranges[meter], \n",
    "                                   layers=layers, n_out=1, \n",
    "                                   config=config, loss_func=evaluate_torch)\n",
    "            for meter, dls in all_dls.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learners[meter] = tabular_learner(all_dls[meter], y_range=y_ranges[meter], \n",
    "#                                   layers=layers, n_out=1, \n",
    "#                                   config=config, loss_func=evaluate_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meter = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learners[meter].lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learners[meter].fit_one_cycle(12, lr_max=7e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learners[meter].recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0: .42\n",
    "1: .87\n",
    "2: 1.1\n",
    "3: 1.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "y_valid_preds, y_valid_true = [], []\n",
    "for meter in learners:\n",
    "    pred, true = learners[meter].get_preds()\n",
    "    y_valid_preds.append((tos[meter].valid.xs.index, cnr(pred)))\n",
    "    y_valid_true.append((tos[meter].valid.xs.index, cnr(true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_valid_pred = sort_ys(y_valid_preds)\n",
    "y_valid_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_valid_true = sort_ys(y_valid_true)\n",
    "y_valid_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if do_test:\n",
    "    y_test_preds = [(all_test_dls[meter].xs.index, \n",
    "                     cnr(learners[meter].get_preds(dl=all_test_dls[meter])[0]))\n",
    "                     for meter in all_test_dls]\n",
    "    y_test_pred = sort_ys(y_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_score = evaluate_torch(torch.from_numpy(y_valid_true.values), \n",
    "                          torch.from_numpy(y_valid_pred.values)).item()\n",
    "print(f'fastai loss {nb_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_pred, y_valid_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `lightgbm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lgb_trains = {meter: lgb.Dataset(to.train.xs.values, to.train.ys.values.ravel())\n",
    "              for meter, to in tos.items()}\n",
    "lgb_evals = {meter: lgb.Dataset(to.valid.xs.values, to.valid.ys.values.ravel(), \n",
    "                       reference=lgb_trains[meter])\n",
    "             for meter, to in tos.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'l2',\n",
    "    'num_leaves': 42,\n",
    "    'learning_rate': 0.5,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "gbms = {meter: lgb.train(params, lgb_train,\n",
    "                num_boost_round=100,\n",
    "                valid_sets=lgb_evals[meter],\n",
    "                early_stopping_rounds=5)\n",
    "        for meter, lgb_train in lgb_trains.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "y_valid_preds = [(tos[meter].valid.xs.index, gbms[meter].predict(tos[meter].valid.xs.values,\n",
    "                                                                 num_iteration=gbms[meter].best_iteration))\n",
    "                 for meter in tos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_valid_pred = sort_ys(y_valid_preds)\n",
    "y_valid_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if do_test:\n",
    "    y_test_preds = [(all_test_dls[meter].xs.index, \n",
    "                     gbms[meter].predict(all_test_dls[meter].xs.values,\n",
    "                                         num_iteration=gbms[meter].best_iteration))\n",
    "                     for meter in all_test_dls]\n",
    "    y_test_pred = sort_ys(y_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_valid_true = [(tos[meter].valid.ys.index, tos[meter].valid.ys.values.ravel())\n",
    "                for meter in tos]\n",
    "y_valid_true = sort_ys(y_valid_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_score = evaluate_torch(torch.from_numpy(y_valid_true.values), \n",
    "                          torch.from_numpy(y_valid_pred.values)).item()\n",
    "print(f'lightgbm loss {nb_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `dep_var` distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train vs validation distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "pick_random = lambda x: np.random.choice(x, size=5000, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing.hist_plot_preds(pick_random(y_valid_true), \n",
    "                              pick_random(y_valid_pred), \n",
    "                              label0='truth', label1='prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_test:\n",
    "    preprocessing.hist_plot_preds(pick_random(y_valid_true), \n",
    "                                  pick_random(y_test_pred), \n",
    "                                  label0='truth (validation)', \n",
    "                                  label1='prediction (test set)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boldly wrong predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "base_cols = ['building_id', 'meter','timestamp']\n",
    "miss_cols = [v for v in base_cols if v not in tos[0].valid.xs.columns]\n",
    "miss_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if len(miss_cols) > 0:\n",
    "    tmp = pd.concat([to.valid.xs.drop(columns=set(base_cols).difference(miss_cols)).join(df.loc[:,base_cols])\n",
    "                     for to in tos.values()])\n",
    "else:\n",
    "    tmp = pd.concat([to.valid.xs for to in tos.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bwt = preprocessing.BoldlyWrongTimeseries(tmp, y_valid_true, y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwt.run_boldly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding:\n",
    "- lgbm makes predictions of negative values!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission to kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = y_test_pred.sort_index()\n",
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if do_test:\n",
    "    y_test_pred_original = torch.exp(tensor(y_test_pred)) - 1\n",
    "\n",
    "    y_out = pd.DataFrame(cnr(y_test_pred_original),\n",
    "                         columns=['meter_reading'],\n",
    "                         index=df_test.index)\n",
    "    display(y_out.head())\n",
    "\n",
    "    assert len(y_out) == 41697600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if do_submit:\n",
    "    y_out.to_csv(data_path/'my_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`kaggle competitions submit -c ashrae-energy-prediction -f submission.csv -m \"Message\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act = 'ReLu'\n",
    "\n",
    "# model_msg = f'baseline (linear regression)'\n",
    "model_msg = f'tabular_learner (one per meter): act {act}, layers {layers}, ps {ps}, embed_p {embed_p}'\n",
    "split_msg = f'split kind \"{split_kind}\" train_frac {train_frac}'\n",
    "samples_msg = f'num samples {len(df)} = {len(df)/n*100:.2f} %'\n",
    "features_msg = f'train_bs = {train_bs} dep_var_stats and 1hot meter and remove leading empty weeks and us_holidays and fix bid 363'\n",
    "score_msg = f'nb score {nb_score:.4f}'\n",
    "# message = ['baseline (linear regression on dep_var_stats and 1hot meter) ', '500 obs/bid', f'nb score {nb_score:.4f}']\n",
    "# message = ['random forest', '500 obs/bid', 'all features', f'nb score {nb_score:.4f}']\n",
    "# message = ['lightgbm', '500 obs/bid', '100 rounds', '42 leaves', 'lr .5', f'nb score {nb_score:.4f}']\n",
    "# message = ['tabular_learner', '500 obs/bid', 'all features', f'layers {layers}, embed_p .1, ps [.1,.1,.1]', f'nb score {nb_score:.4f}']\n",
    "message = ' + '.join([model_msg, samples_msg, split_msg, features_msg, score_msg])\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if do_test and do_submit:\n",
    "    print('Submitting...')\n",
    "    !kaggle competitions submit -c ashrae-energy-prediction -f '{data_path}/my_submission.csv' -m '{message}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**submission scores**\n",
    "\n",
    "random forest:\n",
    "- 5 obs per building ID, .75 max_features, 100 estimators: \n",
    "    - nb score = 2.37\n",
    "    - kaggle score = 1.68 / 1.86\n",
    "    \n",
    "tabular learner:\n",
    "- 5 obs per building ID, layers=[500,250], lr = 2e-3: \n",
    "    - nb score = 1.55\n",
    "    - kaggle score = 1.8 / 2.13\n",
    "- 5 obs per building ID, layers=[500,250], second run with lr = 1e-3: \n",
    "    - nb score = 1.57\n",
    "    - kaggle score = 1.846 / 2.13\n",
    "- 50 obs per building ID, layers=[500,250], 2 rounds: \n",
    "    - nb score = 1.39\n",
    "    - kaggle score = 1.722 / 2.51\n",
    "- 50 obs per building ID, layers=[500,250], 2 rounds: \n",
    "    - nb score = 1.34\n",
    "    - kaggle score = 1.641 / 2.266\n",
    "- 50 obs per building ID, layers=[500,250], 2 rounds, bs=256: \n",
    "    - nb score = 1.32\n",
    "    - kaggle score = 1.643 / 1.926\n",
    "- 500 obs per building ID, layers=[500,250], 3 rounds: \n",
    "    - nb score = 1.19\n",
    "    - kaggle score = 1.62 / 2.55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding:\n",
    "- nb scores are lower than the kaggle scores\n",
    "- random forest seems to have public and private score closer to each other than tabular learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**randomly splitting**\n",
    "    \n",
    "Finding (modified target values, all info = info except time):\n",
    "- Linear:\n",
    "    - meter only @100k: 2.1\n",
    "    - all info minus time @100k: 2.3\n",
    "    - all info incl time @100k: 2.32\n",
    "    - all info incl time + ids @100k: 2.32\n",
    "- RandomForest:\n",
    "    - meter only @100k: 2.2\n",
    "    - all info minus time @100k: 2.7\n",
    "    - all info incl time @100k: 2.74\n",
    "    - all info incl time + ids @100k: 2.82\n",
    "- tabular_learner:\n",
    "    - meter only @100k: 2.1\n",
    "    - all info minus time @100k: 1.56\n",
    "    - all info incl time @100k: 1.52\n",
    "    - all info incl time + ids @100k: 0.96\n",
    "    \n",
    "**splitting along time**\n",
    "Finding:\n",
    "- Linear:\n",
    "    - meter only @100k: 2.1\n",
    "    - all info minus time @100K: 2.2\n",
    "    - all info incl time @100k: 2.3\n",
    "    - all info incl time + ids @100k: 2.29\n",
    "- RandomForest:\n",
    "    - meter only @100k: 2.1\n",
    "    - all info minus time @100K: 2.7\n",
    "    - all info incl time @100k: 2.52\n",
    "    - all info incl time + ids @100k: 2.62\n",
    "- tabular_learner:\n",
    "    - meter only @100k: 2.06\n",
    "    - all info minus time @100K: 1.62\n",
    "    - all info incl time @100k: 1.62\n",
    "    - all info incl time + ids @100k: 1.31"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38_fastai]",
   "language": "python",
   "name": "conda-env-py38_fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
