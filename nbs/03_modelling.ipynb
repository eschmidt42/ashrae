{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling meter readings\n",
    "\n",
    "> First building a base model predicting medians/means, then linear, tree-based and ANN-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding: make sure your test set values are not out of domain $\\Rightarrow$ `timestampYear` in this notebook is put into the training set but there only takes on the value 2016.0, but in the test set it's 2017.0 and 2018.0, causing the predictions to zero out everywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import typing\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn import linear_model, tree, model_selection, ensemble\n",
    "\n",
    "from fastai.tabular.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.plotting.backend = \"plotly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def evaluate_torch(y_true:torch.Tensor, y_pred:torch.Tensor): return torch.sqrt(torch.mean((y_pred - y_true)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with open(base_path/'var_types.pckl', 'rb') as f:\n",
    "    var_types = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous, categorical = var_types['cont'], var_types['cat']\n",
    "continuous, categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous = continuous[:9] + [continuous[-1]]\n",
    "continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading `X` and `X_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_all = pd.read_parquet(base_path/'X.parquet') #.sample(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all.groupby('building_id').size().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample_per_bid = 50\n",
    "replace = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (X_all.groupby('building_id')\n",
    "     .sample(n=n_sample_per_bid, replace=replace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'using {len(X)} samples = {len(X)/len(X_all)*100:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(len(X), X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_test = pd.read_parquet(base_path/'X_test.parquet') #.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(len(X_test), X_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def split_dataset(X:pd.DataFrame, split_kind:str='random',\n",
    "                  train_frac:float=8):\n",
    "    \n",
    "    def random_split():\n",
    "        n_train = int(len(X)*train_frac)\n",
    "        train_bool = X.index.isin(np.random.choice(X.index.values, size=n_train, replace=False))\n",
    "        return train_bool\n",
    "    \n",
    "    def time_split():\n",
    "        time_col = 'timestampElapsed'\n",
    "        ts = X[time_col].sort_values(ascending=True)\n",
    "        ix = int(len(X)*train_frac)\n",
    "        threshold_t = ts.iloc[ix:].values[0]\n",
    "        return X[time_col] < threshold_t\n",
    "     \n",
    "    split_funs = {\n",
    "        'random': random_split,\n",
    "        'time': time_split,\n",
    "    }\n",
    "    \n",
    "    assert split_kind in split_funs\n",
    "    train_bool = split_funs[split_kind]()\n",
    "    \n",
    "    train_idx = np.where(train_bool)[0]\n",
    "    valid_idx = np.where(~train_bool)[0]\n",
    "\n",
    "    return (list(train_idx), list(valid_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "split_kind = 'random'\n",
    "#split_kind = 'time'\n",
    "splits = split_dataset(X, split_kind=split_kind, train_frac=.8)\n",
    "#splits=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(X.iloc[splits[0]].loc[:, 'timestampMonth'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super simplistic input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# procs = [Categorify, FillMissing, Normalize]\n",
    "# to = TabularPandas(X, procs, ['meter'],\n",
    "#                    [], y_names='meter_reading', splits=splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "procs = [Categorify, FillMissing, Normalize]\n",
    "to = TabularPandas(X, procs, categorical,\n",
    "                   continuous, \n",
    "                   y_names='meter_reading', splits=splits)\n",
    "\n",
    "train_bs = 256\n",
    "val_bs = 256\n",
    "dls = to.dataloaders(bs=train_bs, val_bs=val_bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_dl = dls.test_dl(X_test, bs=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to.train.xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to.train.ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_plot_preds(y_valid_pred, y_test_pred):\n",
    "    res = pd.concat(\n",
    "        (\n",
    "            pd.DataFrame({\n",
    "                'y': y_valid_pred, \n",
    "                'set': ['valid']*len(y_valid_pred)\n",
    "            }),\n",
    "            pd.DataFrame({\n",
    "                'y':y_test_pred, \n",
    "                'set': ['test']*len(y_test_pred)\n",
    "            })\n",
    "        ),\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    return px.histogram(res, x='y', color='set', marginal='box',\n",
    "                        barmode='overlay', histnorm='probability density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### super simple base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEstimator:\n",
    "    cols = []\n",
    "    def fit(self, X, y):\n",
    "        self.cols = [col for col in ['building_id', 'meter'] if col in X.columns]\n",
    "        tmp = pd.concat((X.loc[:,self.cols], to.train.ys), axis=1)\n",
    "        self.consts = tmp.groupby(self.cols)['meter_reading'].describe().to_dict()\n",
    "        display(self.consts)\n",
    "    def predict(self, X, quantity:str='mean'):\n",
    "#         if len(self.cols) == 1:\n",
    "        return [self.consts[quantity][tuple(row[self.cols].values)] for i,row in X.iterrows()]\n",
    "#         elif len(self.cols) == 2:\n",
    "#             return [self.consts[quantity][row[self.cols[0]]][row[self.cols[1]]] for i,row in X.iterrows()]\n",
    "#         raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se = SimpleEstimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se.fit(to.train.xs, to.train.ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "y_valid_pred = se.predict(to.valid.xs, quantity='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = to.valid.ys.values.ravel()\n",
    "y_pred = y_valid_pred\n",
    "assert y_true.shape[0] == len(y_pred)\n",
    "evaluate_torch(torch.from_numpy(y_true), \n",
    "               torch.Tensor(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# y_test_pred = se.predict(test_dl.xs)\n",
    "# y_test_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# _y_train = np.random.choice(y_valid_pred, size=5000)\n",
    "# _y_test = np.random.choice(y_test_pred, size=5000)\n",
    "\n",
    "# hist_plot_preds(_y_train, _y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# y_test_pred = torch.Tensor(y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = ensemble.RandomForestRegressor(n_estimators=100, max_features=.75, criterion='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "m.fit(to.train.xs, to.train.ys.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_pred = m.predict(to.valid.xs.values)\n",
    "y_valid_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = to.valid.ys.values.ravel()\n",
    "y_pred = y_valid_pred\n",
    "assert y_true.shape == y_pred.shape\n",
    "nb_score = evaluate_torch(torch.from_numpy(y_true), \n",
    "                          torch.from_numpy(y_pred))\n",
    "nb_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_test_pred = m.predict(test_dl.xs)\n",
    "y_test_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_y_train = np.random.choice(y_valid_pred, size=5000)\n",
    "_y_test = np.random.choice(y_test_pred, size=5000)\n",
    "\n",
    "hist_plot_preds(_y_train, _y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_test_pred = torch.Tensor(y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding:\n",
    "- values in the range of 90. this is way to large, but the values predicted for the validation set are okay. what is different between the prediction over the validation set and the test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling with fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_range = [np.min([to.train.ys.values.min(), to.valid.ys.values.min()]),\n",
    "           np.max([to.train.ys.values.max(), to.valid.ys.values.max()]),]\n",
    "y_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_range = [to.train.ys.values.min(),\n",
    "#            to.train.ys.values.max()]\n",
    "# y_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = tabular_learner(dls, y_range=y_range, layers=[500,250],\n",
    "                        n_out=1, loss_func=evaluate_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(5, 5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_pred, y_valid_true = learn.get_preds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_pred, y_valid_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_score = evaluate_torch(y_valid_pred, y_valid_true)\n",
    "nb_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred, _ = learn.get_preds(dl=test_dl)\n",
    "y_test_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_y_valid = y_valid_pred.clone().numpy().ravel()\n",
    "_y_valid = np.random.choice(_y_valid, size=5000)\n",
    "\n",
    "_y_test = y_test_pred.clone().numpy().ravel()\n",
    "_y_test = np.random.choice(_y_test, size=5000)\n",
    "\n",
    "hist_plot_preds(_y_valid, _y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming back and storing in submission format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_original = torch.exp(y_test_pred) - 1\n",
    "y_test_pred_original[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_out = pd.DataFrame(y_test_pred_original.clone().numpy(),\n",
    "                     columns=['meter_reading'])\n",
    "y_out.index.rename('row_id', inplace=True)\n",
    "y_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(y_out) == 41697600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_out.to_csv('test_submission_randomforest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_out.to_csv('test_submission_tabularlearner.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`kaggle competitions submit -c ashrae-energy-prediction -f submission.csv -m \"Message\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c ashrae-energy-prediction -f test_submission_randomforest.csv -m f\"50 obs per bid - randomforest with filtered outliers nb score {nb_score}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c ashrae-energy-prediction -f test_submission_tabularlearner.csv -m f\"50 obs per bid - tabularlearner with bs=256 and filtered outliers and bid&sid nb score {nb_score}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**submission scores**\n",
    "\n",
    "random forest:\n",
    "- 5 obs per building ID, .75 max_features, 100 estimators: \n",
    "    - nb score = 2.37\n",
    "    - kaggle score = 1.68 / 1.86\n",
    "    \n",
    "tabular learner:\n",
    "- 5 obs per building ID, layers=[500,250], lr = 2e-3: \n",
    "    - nb score = 1.55\n",
    "    - kaggle score = 1.8 / 2.13\n",
    "- 5 obs per building ID, layers=[500,250], second run with lr = 1e-3: \n",
    "    - nb score = 1.57\n",
    "    - kaggle score = 1.846 / 2.13\n",
    "- 50 obs per building ID, layers=[500,250], 2 rounds: \n",
    "    - nb score = 1.39\n",
    "    - kaggle score = 1.722 / 2.51\n",
    "- 50 obs per building ID, layers=[500,250], 2 rounds: \n",
    "    - nb score = 1.34\n",
    "    - kaggle score = 1.641 / 2.266\n",
    "- 50 obs per building ID, layers=[500,250], 2 rounds, bs=256: \n",
    "    - nb score = 1.32\n",
    "    - kaggle score = 1.643 / 1.926\n",
    "- 500 obs per building ID, layers=[500,250], 3 rounds: \n",
    "    - nb score = 1.19\n",
    "    - kaggle score = 1.62 / 2.55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding:\n",
    "- nb scores are lower than the kaggle scores\n",
    "- random forest seems to have public and private score closer to each other than tabular learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**randomly splitting**\n",
    "    \n",
    "Finding (modified target values, all info = info except time):\n",
    "- Linear:\n",
    "    - meter only @100k: 2.1\n",
    "    - all info minus time @100k: 2.3\n",
    "    - all info incl time @100k: 2.32\n",
    "    - all info incl time + ids @100k: 2.32\n",
    "- RandomForest:\n",
    "    - meter only @100k: 2.2\n",
    "    - all info minus time @100k: 2.7\n",
    "    - all info incl time @100k: 2.74\n",
    "    - all info incl time + ids @100k: 2.82\n",
    "- tabular_learner:\n",
    "    - meter only @100k: 2.1\n",
    "    - all info minus time @100k: 1.56\n",
    "    - all info incl time @100k: 1.52\n",
    "    - all info incl time + ids @100k: 0.96\n",
    "    \n",
    "**splitting along time**\n",
    "Finding:\n",
    "- Linear:\n",
    "    - meter only @100k: 2.1\n",
    "    - all info minus time @100K: 2.2\n",
    "    - all info incl time @100k: 2.3\n",
    "    - all info incl time + ids @100k: 2.29\n",
    "- RandomForest:\n",
    "    - meter only @100k: 2.1\n",
    "    - all info minus time @100K: 2.7\n",
    "    - all info incl time @100k: 2.52\n",
    "    - all info incl time + ids @100k: 2.62\n",
    "- tabular_learner:\n",
    "    - meter only @100k: 2.06\n",
    "    - all info minus time @100K: 1.62\n",
    "    - all info incl time @100k: 1.62\n",
    "    - all info incl time + ids @100k: 1.31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-py38_fastai]",
   "language": "python",
   "name": "conda-env-.conda-py38_fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
