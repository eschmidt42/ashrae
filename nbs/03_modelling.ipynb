{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling meter readings\n",
    "\n",
    "> First building a base model predicting medians/means, then linear, tree-based and ANN-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding: make sure your test set values are not out of domain $\\Rightarrow$ `timestampYear` in this notebook is put into the training set but there only takes on the value 2016.0, but in the test set it's 2017.0 and 2018.0, causing the predictions to zero out everywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import typing\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn import linear_model, tree, model_selection, ensemble\n",
    "\n",
    "from fastai.tabular.all import *\n",
    "\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.plotting.backend = \"plotly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def evaluate_torch(y_true:torch.Tensor, y_pred:torch.Tensor): return torch.sqrt(F.mse_loss(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with open(base_path/'var_types.pckl', 'rb') as f:\n",
    "    var_types = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous, categorical = var_types['cont'], var_types['cat']\n",
    "continuous, categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous = continuous[:9] + [continuous[-1]]\n",
    "continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading `X` and `X_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_all = pd.read_parquet(base_path/'X.parquet') #.sample(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all.groupby('building_id').size().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample_per_bid = 1000\n",
    "replace = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (X_all.groupby(['building_id', 'meter'])\n",
    "     .sample(n=n_sample_per_bid, replace=replace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'using {len(X)} samples = {len(X)/len(X_all)*100:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(len(X), X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.groupby(['building_id', 'meter']).size().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if predict_test:\n",
    "    X_test = pd.read_parquet(base_path/'X_test.parquet') #.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if predict_test:\n",
    "    display(len(X_test), X_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def split_dataset(X:pd.DataFrame, split_kind:str='random',\n",
    "                  train_frac:float=8):\n",
    "    \n",
    "    def random_split():\n",
    "        n_train = int(len(X)*train_frac)\n",
    "        train_bool = X.index.isin(np.random.choice(X.index.values, size=n_train, replace=False))\n",
    "        return train_bool\n",
    "    \n",
    "    def time_split():\n",
    "        time_col = 'timestampElapsed'\n",
    "        ts = X[time_col].sort_values(ascending=True)\n",
    "        ix = int(len(X)*train_frac)\n",
    "        threshold_t = ts.iloc[ix:].values[0]\n",
    "        return X[time_col] < threshold_t\n",
    "     \n",
    "    split_funs = {\n",
    "        'random': random_split,\n",
    "        'time': time_split,\n",
    "    }\n",
    "    \n",
    "    assert split_kind in split_funs\n",
    "    train_bool = split_funs[split_kind]()\n",
    "    \n",
    "    train_idx = np.where(train_bool)[0]\n",
    "    valid_idx = np.where(~train_bool)[0]\n",
    "\n",
    "    return (list(train_idx), list(valid_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "split_kind = 'random'\n",
    "#split_kind = 'time'\n",
    "splits = split_dataset(X, split_kind=split_kind, train_frac=.8)\n",
    "#splits=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(X.iloc[splits[0]].loc[:, 'timestampMonth'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super simplistic input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# procs = [Categorify, FillMissing, Normalize]\n",
    "# to = TabularPandas(X, procs, ['meter'],\n",
    "#                    [], y_names='meter_reading', splits=splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "procs = [Categorify, FillMissing, Normalize]\n",
    "to = TabularPandas(X, procs, categorical,\n",
    "                   continuous, \n",
    "                   y_names='meter_reading', splits=splits)\n",
    "\n",
    "train_bs = 256*4\n",
    "val_bs = 256*4\n",
    "dls = to.dataloaders(bs=train_bs, val_bs=val_bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if predict_test:\n",
    "    test_dl = dls.test_dl(X_test, bs=1024*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to.train.xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to.train.ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_plot_preds(y_valid_pred, y_test_pred):\n",
    "    res = pd.concat(\n",
    "        (\n",
    "            pd.DataFrame({\n",
    "                'y': y_valid_pred, \n",
    "                'set': ['valid']*len(y_valid_pred)\n",
    "            }),\n",
    "            pd.DataFrame({\n",
    "                'y':y_test_pred, \n",
    "                'set': ['test']*len(y_test_pred)\n",
    "            })\n",
    "        ),\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    return px.histogram(res, x='y', color='set', marginal='box',\n",
    "                        barmode='overlay', histnorm='probability density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### super simple base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEstimator:\n",
    "    cols = []\n",
    "    def fit(self, X, y):\n",
    "        self.cols = [col for col in ['building_id', 'meter'] if col in X.columns]\n",
    "        tmp = pd.concat((X.loc[:,self.cols], to.train.ys), axis=1)\n",
    "        self.consts = tmp.groupby(self.cols)['meter_reading'].describe().to_dict()\n",
    "    def predict(self, X, quantity:str='mean'):\n",
    "        return [self.consts[quantity][tuple(row[self.cols].values)] for i,row in X.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se = SimpleEstimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "se.fit(to.train.xs, to.train.ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_valid_pred = se.predict(to.valid.xs, quantity='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = to.valid.ys.values.ravel()\n",
    "y_pred = y_valid_pred\n",
    "assert y_true.shape[0] == len(y_pred)\n",
    "evaluate_torch(torch.from_numpy(y_true), \n",
    "               torch.Tensor(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if predict_test:\n",
    "    y_test_pred = se.predict(test_dl.xs)\n",
    "    display(y_test_pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if predict_test:\n",
    "    _y_train = np.random.choice(y_valid_pred, size=5000)\n",
    "    _y_test = np.random.choice(y_test_pred, size=5000)\n",
    "\n",
    "    hist_plot_preds(_y_train, _y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if predict_test:\n",
    "    y_test_pred = torch.Tensor(y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = ensemble.RandomForestRegressor(n_estimators=100, max_features=.75, criterion='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "m.fit(to.train.xs, to.train.ys.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_pred = m.predict(to.valid.xs.values)\n",
    "y_valid_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = to.valid.ys.values.ravel()\n",
    "y_pred = y_valid_pred\n",
    "assert y_true.shape == y_pred.shape\n",
    "nb_score = evaluate_torch(torch.from_numpy(y_true), \n",
    "                          torch.from_numpy(y_pred))\n",
    "nb_score.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot boldly wrong time series**\n",
    "\n",
    "Get errors by `building_id` and `meter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BoldlyWrongTimeseries:\n",
    "    def __init__(self, xs, y_true, y_pred):\n",
    "        self.df = xs.loc[:,['meter', 'building_id', 'timestampElapsed']].copy()\n",
    "#         self.get_predictions(model, xs)\n",
    "        self.df['y_true'] = y_true\n",
    "        self.df['y_pred'] = y_pred\n",
    "        self.compute_misses(y_true)\n",
    "\n",
    "#     def get_predictions(self, model, xs):\n",
    "#         self.df['y_pred'] = m.predict(xs)\n",
    "\n",
    "    def compute_misses(self, y_true):\n",
    "        fun = lambda x: np.mean(x**2)\n",
    "        self.miss = (self.df.assign(difference=lambda x: x['y_pred']-x['y_true'])\n",
    "                     .groupby(['meter', 'building_id'])\n",
    "                     .agg(loss=pd.NamedAgg(column='difference', aggfunc=fun))\n",
    "                     .sort_values('loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bwt = BoldlyWrongTimeseries(to.valid.xs, to.valid.ys, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding something to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def plot_boldly_wrong(self, nth_last:int=None,\n",
    "                      meter:int=None, bid:int=None):\n",
    "    assert (meter is not None and bid is not None) or (nth_last is not None)\n",
    "    if nth_last is not None:\n",
    "        ix = self.miss.iloc[[nth_last],:]\n",
    "        meter = ix.index[0][0]\n",
    "        bid = ix.index[0][1]\n",
    "        loss = ix[\"loss\"].values[0]\n",
    "    else:\n",
    "        ix = self.miss.xs((meter, bid))\n",
    "        loss = ix.values[0]\n",
    "        \n",
    "    df_plot = self.df.loc[(self.df['meter']==meter) & (self.df['building_id']==bid)]\n",
    "    df_plot = pd.concat((\n",
    "        df_plot[['timestampElapsed', 'y_true']].rename(columns={'y_true':'y'}).assign(label='true'),\n",
    "        df_plot[['timestampElapsed', 'y_pred']].rename(columns={'y_pred':'y'}).assign(label='pred'))\n",
    "    )\n",
    "    return df_plot.plot(kind='scatter', x='timestampElapsed', \n",
    "                        y='y', color='label', \n",
    "                        title=f'pos {nth_last}: meter = {meter}, building_id = {bid}<br>loss = {loss:.3f}')\n",
    "    \n",
    "    \n",
    "BoldlyWrongTimeseries.plot_boldly_wrong = plot_boldly_wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwt.plot_boldly_wrong(nth_last=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwt.plot_boldly_wrong(meter=1, bid=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def init_widgets(self):\n",
    "    self.int_txt_loss = widgets.IntText(min=-len(self.miss), max=len(self.miss),\n",
    "                                   description='Position')\n",
    "    self.int_txt_meter = widgets.IntText(min=self.df['meter'].min(), max=self.df['meter'].max(),\n",
    "                                   description='Meter')\n",
    "    self.int_txt_bid = widgets.IntText(min=self.df['building_id'].min(), max=self.df['building_id'].max(),\n",
    "                                   description='building id')\n",
    "    self.run_btn = widgets.Button(description='plot')\n",
    "    self.switch_btn = widgets.Checkbox(description='Loss-based', value=True)\n",
    "    self.run_btn.on_click(self.click_boldly_wrong)\n",
    "    self.out_wdg = widgets.Output()\n",
    "    \n",
    "def run_boldly(self):\n",
    "    if not hasattr(self, 'switch_btn'):\n",
    "        self.init_widgets()\n",
    "    return widgets.VBox([self.switch_btn, self.int_txt_loss, \n",
    "                         self.int_txt_meter, self.int_txt_bid, \n",
    "                         self.run_btn, self.out_wdg])\n",
    "\n",
    "def click_boldly_wrong(self, change):\n",
    "    self.out_wdg.clear_output()\n",
    "    nth_last = None if self.switch_btn.value == False else self.int_txt_loss.value\n",
    "    meter = None if self.switch_btn.value == True else self.int_txt_meter.value\n",
    "    bid = None if self.switch_btn.value == True else self.int_txt_bid.value\n",
    "    with self.out_wdg:\n",
    "        print(f'nth_last {nth_last} meter {meter} bid {bid}')\n",
    "        try:\n",
    "            self.plot_boldly_wrong(nth_last=nth_last, meter=meter, bid=bid).show()\n",
    "        except:\n",
    "            raise ValueError(f'nth_last {nth_last} meter {meter} bid {bid} not a valid combination! Likely due to missing meter/bid combination')\n",
    "\n",
    "BoldlyWrongTimeseries.init_widgets = init_widgets\n",
    "BoldlyWrongTimeseries.click_boldly_wrong = click_boldly_wrong\n",
    "BoldlyWrongTimeseries.run_boldly = run_boldly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwt.run_boldly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check predictions `dep_var` distribution vaid vs test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if predict_test:\n",
    "    y_test_pred = m.predict(test_dl.xs)\n",
    "    display(y_test_pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if predict_test:\n",
    "    _y_train = np.random.choice(y_valid_pred, size=5000)\n",
    "    _y_test = np.random.choice(y_test_pred, size=5000)\n",
    "\n",
    "    hist_plot_preds(_y_train, _y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if predict_test:\n",
    "    y_test_pred = torch.Tensor(y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding:\n",
    "- values in the range of 90. this is way to large, but the values predicted for the validation set are okay. what is different between the prediction over the validation set and the test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling with fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_range = [np.min([to.train.ys.values.min(), to.valid.ys.values.min()]),\n",
    "           np.max([to.train.ys.values.max(), to.valid.ys.values.max()]),]\n",
    "y_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_range = [0,\n",
    "           np.max([to.train.ys.values.max(), to.valid.ys.values.max()]),]\n",
    "y_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_range = [to.train.ys.values.min(),\n",
    "#            to.train.ys.values.max()]\n",
    "# y_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = tabular_learner(dls, y_range=y_range, layers=[500,250],\n",
    "                        n_out=1, loss_func=evaluate_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(5, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_pred, y_valid_true = learn.get_preds()\n",
    "y_valid_pred[:5], y_valid_true[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_score = evaluate_torch(y_valid_pred, y_valid_true)\n",
    "nb_score.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone numpy ravel\n",
    "cnr = lambda x: x.clone().numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bwt_tl = BoldlyWrongTimeseries(to.valid.xs, cnr(y_valid_true), cnr(y_valid_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwt_tl.run_boldly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwt.run_boldly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if predict_test:\n",
    "    y_test_pred, _ = learn.get_preds(dl=test_dl)\n",
    "    display(y_test_pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if predict_test:\n",
    "    _y_valid = y_valid_pred.clone().numpy().ravel()\n",
    "    _y_valid = np.random.choice(_y_valid, size=5000)\n",
    "\n",
    "    _y_test = y_test_pred.clone().numpy().ravel()\n",
    "    _y_test = np.random.choice(_y_test, size=5000)\n",
    "\n",
    "    hist_plot_preds(_y_valid, _y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming back and storing in submission format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if predict_test:\n",
    "    y_test_pred_original = torch.exp(y_test_pred) - 1\n",
    "    displa(y_test_pred_original[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if predict_test:\n",
    "    y_out = pd.DataFrame(y_test_pred_original.clone().numpy(),\n",
    "                         columns=['meter_reading'])\n",
    "    y_out.index.rename('row_id', inplace=True)\n",
    "    display(y_out.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if predict_test:\n",
    "    assert len(y_out) == 41697600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if predict_test:\n",
    "    y_out.to_csv('test_submission_randomforest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if predict_test:\n",
    "    y_out.to_csv('test_submission_tabularlearner.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`kaggle competitions submit -c ashrae-energy-prediction -f submission.csv -m \"Message\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle competitions submit -c ashrae-energy-prediction -f test_submission_randomforest.csv -m f\"50 obs per bid - randomforest with filtered outliers nb score {nb_score}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle competitions submit -c ashrae-energy-prediction -f test_submission_tabularlearner.csv -m f\"1000 obs per bid - tabularlearner with bs=256 and filtered outliers & 0s and bid&sid nb score {nb_score}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**submission scores**\n",
    "\n",
    "random forest:\n",
    "- 5 obs per building ID, .75 max_features, 100 estimators: \n",
    "    - nb score = 2.37\n",
    "    - kaggle score = 1.68 / 1.86\n",
    "    \n",
    "tabular learner:\n",
    "- 5 obs per building ID, layers=[500,250], lr = 2e-3: \n",
    "    - nb score = 1.55\n",
    "    - kaggle score = 1.8 / 2.13\n",
    "- 5 obs per building ID, layers=[500,250], second run with lr = 1e-3: \n",
    "    - nb score = 1.57\n",
    "    - kaggle score = 1.846 / 2.13\n",
    "- 50 obs per building ID, layers=[500,250], 2 rounds: \n",
    "    - nb score = 1.39\n",
    "    - kaggle score = 1.722 / 2.51\n",
    "- 50 obs per building ID, layers=[500,250], 2 rounds: \n",
    "    - nb score = 1.34\n",
    "    - kaggle score = 1.641 / 2.266\n",
    "- 50 obs per building ID, layers=[500,250], 2 rounds, bs=256: \n",
    "    - nb score = 1.32\n",
    "    - kaggle score = 1.643 / 1.926\n",
    "- 500 obs per building ID, layers=[500,250], 3 rounds: \n",
    "    - nb score = 1.19\n",
    "    - kaggle score = 1.62 / 2.55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding:\n",
    "- nb scores are lower than the kaggle scores\n",
    "- random forest seems to have public and private score closer to each other than tabular learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**randomly splitting**\n",
    "    \n",
    "Finding (modified target values, all info = info except time):\n",
    "- Linear:\n",
    "    - meter only @100k: 2.1\n",
    "    - all info minus time @100k: 2.3\n",
    "    - all info incl time @100k: 2.32\n",
    "    - all info incl time + ids @100k: 2.32\n",
    "- RandomForest:\n",
    "    - meter only @100k: 2.2\n",
    "    - all info minus time @100k: 2.7\n",
    "    - all info incl time @100k: 2.74\n",
    "    - all info incl time + ids @100k: 2.82\n",
    "- tabular_learner:\n",
    "    - meter only @100k: 2.1\n",
    "    - all info minus time @100k: 1.56\n",
    "    - all info incl time @100k: 1.52\n",
    "    - all info incl time + ids @100k: 0.96\n",
    "    \n",
    "**splitting along time**\n",
    "Finding:\n",
    "- Linear:\n",
    "    - meter only @100k: 2.1\n",
    "    - all info minus time @100K: 2.2\n",
    "    - all info incl time @100k: 2.3\n",
    "    - all info incl time + ids @100k: 2.29\n",
    "- RandomForest:\n",
    "    - meter only @100k: 2.1\n",
    "    - all info minus time @100K: 2.7\n",
    "    - all info incl time @100k: 2.52\n",
    "    - all info incl time + ids @100k: 2.62\n",
    "- tabular_learner:\n",
    "    - meter only @100k: 2.06\n",
    "    - all info minus time @100K: 1.62\n",
    "    - all info incl time @100k: 1.62\n",
    "    - all info incl time + ids @100k: 1.31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-py38_fastai]",
   "language": "python",
   "name": "conda-env-.conda-py38_fastai-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
