{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling & submitting\n",
    "\n",
    "> Playing with different models and submitting predictions over the test set to kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOs\n",
    "* remove the timestampElapsed field and see if the kaggle private score improves => it does a bit, reached 1.5 private loss\n",
    "* feature importance https://scikit-learn.org/stable/modules/permutation_importance.html\n",
    "* predict values one or two years into the future (the patterns should remain very similar) using the timestamp_Elapsed field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding: make sure your test set values are not out of domain $\\Rightarrow$ `timestampYear` in this notebook is put into the training set but there only takes on the value 2016.0, but in the test set it's 2017.0 and 2018.0, causing the predictions to zero out everywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from ashrae import preprocessing\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import typing\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn import linear_model, tree, model_selection, ensemble\n",
    "\n",
    "from fastai.tabular.all import *\n",
    "\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.plotting.backend = \"plotly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_test = True\n",
    "do_test = False\n",
    "do_submit = False\n",
    "data_path = Path(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def evaluate_torch(y_true:torch.Tensor, y_pred:torch.Tensor): return torch.sqrt(F.mse_loss(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "var_names = preprocessing.load_var_names(data_path/'var_names.pckl')\n",
    "var_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = preprocessing.load_df(data_path/'X.parquet')\n",
    "\n",
    "if do_test:\n",
    "    df_test = preprocessing.load_df(data_path/'X_test.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling `df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n = len(df)\n",
    "\n",
    "if True: # per building_id and meter sampling\n",
    "    n_sample_per_bid = 50\n",
    "    replace = True\n",
    "\n",
    "    df = (df.groupby(['building_id', 'meter'])\n",
    "         .sample(n=n_sample_per_bid, replace=replace))\n",
    "\n",
    "    print(f'using {len(df)} samples = {len(df)/n*100:.2f} %')\n",
    "\n",
    "if False: # general sampling\n",
    "    frac_samples = .5\n",
    "    replace = False\n",
    "\n",
    "    df = (df.sample(frac=frac_samples, replace=replace))\n",
    "\n",
    "    print(f'using {len(df)} samples = {len(df)/n*100:.2f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def split_dataset(X:pd.DataFrame, split_kind:str='random',\n",
    "                  train_frac:float=8):\n",
    "    \n",
    "    def random_split():\n",
    "        n_train = int(len(X)*train_frac)\n",
    "        train_bool = X.index.isin(np.random.choice(X.index.values, size=n_train, replace=False))\n",
    "        return train_bool\n",
    "    \n",
    "    def time_split():\n",
    "        time_col = 'timestampElapsed'\n",
    "        ts = X[time_col].sort_values(ascending=True)\n",
    "        ix = int(len(X)*train_frac)\n",
    "        threshold_t = ts.iloc[ix:].values[0]\n",
    "        return X[time_col] < threshold_t\n",
    "     \n",
    "    split_funs = {\n",
    "        'random': random_split,\n",
    "        'time': time_split,\n",
    "    }\n",
    "    \n",
    "    assert split_kind in split_funs\n",
    "    train_bool = split_funs[split_kind]()\n",
    "    \n",
    "    train_idx = np.where(train_bool)[0]\n",
    "    valid_idx = np.where(~train_bool)[0]\n",
    "\n",
    "    return (list(train_idx), list(valid_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "split_kind = 'random'\n",
    "#split_kind = 'time'\n",
    "splits = split_dataset(df, split_kind=split_kind, train_frac=.8)\n",
    "#splits=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "to = preprocessing.get_tabular_object(df, var_names, splits=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_bs = 256*8\n",
    "val_bs = 256*8\n",
    "\n",
    "dls = to.dataloaders(bs=train_bs, val_bs=val_bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_bs = 1024*4\n",
    "\n",
    "if do_test:\n",
    "    test_dl = dls.test_dl(df_test, bs=test_bs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "params = {'n_estimators': 20, 'max_features': 'sqrt'}\n",
    "model = ensemble.RandomForestRegressor\n",
    "# params = {}\n",
    "# model = linear_model.LinearRegression\n",
    "\n",
    "m = model(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "m.fit(to.train.xs.values, to.train.ys.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_pred = m.predict(to.valid.xs.values)\n",
    "\n",
    "if do_test:\n",
    "    y_test_pred = m.predict(test_dl.xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_true = to.valid.ys.values.ravel()\n",
    "nb_score = evaluate_torch(torch.from_numpy(y_valid_true), \n",
    "                          torch.from_numpy(y_valid_pred)).item()\n",
    "print(f'sklearn loss {nb_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `fastai`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_range = [0,\n",
    "           np.max([to.train.ys.values.max(), to.valid.ys.values.max()]),]\n",
    "y_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Swish(nn.ReLU):\n",
    "    def forward(self, input:Tensor) -> Tensor:\n",
    "        if self.inplace:\n",
    "            res = input.clone()\n",
    "            torch.sigmoid_(res)\n",
    "            input *= res\n",
    "            return input\n",
    "        else:\n",
    "            return torch.sigmoid(input) * input\n",
    "    \n",
    "class Sine(nn.ReLU):\n",
    "    def forward(self, input:Tensor) -> Tensor:\n",
    "        if self.inplace:\n",
    "            return torch.sin_(input)\n",
    "        else:\n",
    "            return torch.sin(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [50, 25]\n",
    "\n",
    "config = None\n",
    "# config = tabular_config(act_cls=nn.ReLU(inplace=True))\n",
    "# config = tabular_config(act_cls=Swish(inplace=True))\n",
    "# config = tabular_config(act_cls=Sine(inplace=True))\n",
    "\n",
    "learn = tabular_learner(dls, y_range=y_range, layers=layers,\n",
    "                        n_out=1, config=config, loss_func=evaluate_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(5, 5e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_valid_pred, y_valid_true = learn.get_preds()\n",
    "\n",
    "if do_test:\n",
    "    y_test_pred, _ = learn.get_preds(dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_score = evaluate_torch(y_valid_true, \n",
    "                          y_valid_pred).item()\n",
    "print(f'fastai loss {nb_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone numpy ravel\n",
    "cnr = lambda x: x.clone().numpy().ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `dep_var` distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train vs validation distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pick_random = lambda x: np.random.choice(x, size=5000, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing.hist_plot_preds(pick_random(y_valid_true), \n",
    "                              pick_random(y_valid_pred), \n",
    "                              label0='truth', label1='prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_test:\n",
    "    preprocessing.hist_plot_preds(pick_random(y_valid_true), \n",
    "                                  pick_random(y_test_pred), \n",
    "                                  label0='truth (validation)', \n",
    "                                  label1='prediction (test set)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boldly wrong predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bwt = preprocessing.BoldlyWrongTimeseries(to.valid.xs, y_valid_true, y_valid_pred,\n",
    "                                          t=df.iloc[splits[1]].loc[:,['timestampElapsed']].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwt.run_boldly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding fastai:\n",
    "- sudden jumps betweem 0 and 8 or so cause issues for the model. for the most serious cases like bid 79 and meter 2 the predicted values lie between the true values\n",
    "- there are oddly frequent values at 3.766 for meter 2 big 76, 7.6 for meter 3 bid 1219, 3.7 4.39 5.76 for meter 3 bid 1257\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission to kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_test and do_submit:\n",
    "    y_test_pred_original = torch.exp(tensor(y_test_pred)) - 1\n",
    "\n",
    "    y_out = pd.DataFrame(cnr(y_test_pred_original),\n",
    "                         columns=['meter_reading'],\n",
    "                         index=df_test.index)\n",
    "#     y_out.index.rename('row_id', inplace=True) # TODO: make sure the row_id value is actually correct\n",
    "    display(y_out.head())\n",
    "\n",
    "    assert len(y_out) == 41697600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if predict_test and do_submit:\n",
    "    y_out.to_csv(data_path/'my_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`kaggle competitions submit -c ashrae-energy-prediction -f submission.csv -m \"Message\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = ['random forest', '50 obs/bid', f'nb score {nb_score:.4f}']\n",
    "# message = ['linear model', '50 obs/bid', f'nb score {nb_score:.4f}']\n",
    "# message = ['tabular_learner', '50 obs/bid', f'nb score {nb_score:.4f}']\n",
    "message = ' + '.join(message)\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_test & do_submit:\n",
    "    print('Submitting...')\n",
    "    !kaggle competitions submit -c ashrae-energy-prediction -f '{data_path}/my_submission.csv' -m '{message}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**submission scores**\n",
    "\n",
    "random forest:\n",
    "- 5 obs per building ID, .75 max_features, 100 estimators: \n",
    "    - nb score = 2.37\n",
    "    - kaggle score = 1.68 / 1.86\n",
    "    \n",
    "tabular learner:\n",
    "- 5 obs per building ID, layers=[500,250], lr = 2e-3: \n",
    "    - nb score = 1.55\n",
    "    - kaggle score = 1.8 / 2.13\n",
    "- 5 obs per building ID, layers=[500,250], second run with lr = 1e-3: \n",
    "    - nb score = 1.57\n",
    "    - kaggle score = 1.846 / 2.13\n",
    "- 50 obs per building ID, layers=[500,250], 2 rounds: \n",
    "    - nb score = 1.39\n",
    "    - kaggle score = 1.722 / 2.51\n",
    "- 50 obs per building ID, layers=[500,250], 2 rounds: \n",
    "    - nb score = 1.34\n",
    "    - kaggle score = 1.641 / 2.266\n",
    "- 50 obs per building ID, layers=[500,250], 2 rounds, bs=256: \n",
    "    - nb score = 1.32\n",
    "    - kaggle score = 1.643 / 1.926\n",
    "- 500 obs per building ID, layers=[500,250], 3 rounds: \n",
    "    - nb score = 1.19\n",
    "    - kaggle score = 1.62 / 2.55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding:\n",
    "- nb scores are lower than the kaggle scores\n",
    "- random forest seems to have public and private score closer to each other than tabular learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**randomly splitting**\n",
    "    \n",
    "Finding (modified target values, all info = info except time):\n",
    "- Linear:\n",
    "    - meter only @100k: 2.1\n",
    "    - all info minus time @100k: 2.3\n",
    "    - all info incl time @100k: 2.32\n",
    "    - all info incl time + ids @100k: 2.32\n",
    "- RandomForest:\n",
    "    - meter only @100k: 2.2\n",
    "    - all info minus time @100k: 2.7\n",
    "    - all info incl time @100k: 2.74\n",
    "    - all info incl time + ids @100k: 2.82\n",
    "- tabular_learner:\n",
    "    - meter only @100k: 2.1\n",
    "    - all info minus time @100k: 1.56\n",
    "    - all info incl time @100k: 1.52\n",
    "    - all info incl time + ids @100k: 0.96\n",
    "    \n",
    "**splitting along time**\n",
    "Finding:\n",
    "- Linear:\n",
    "    - meter only @100k: 2.1\n",
    "    - all info minus time @100K: 2.2\n",
    "    - all info incl time @100k: 2.3\n",
    "    - all info incl time + ids @100k: 2.29\n",
    "- RandomForest:\n",
    "    - meter only @100k: 2.1\n",
    "    - all info minus time @100K: 2.7\n",
    "    - all info incl time @100k: 2.52\n",
    "    - all info incl time + ids @100k: 2.62\n",
    "- tabular_learner:\n",
    "    - meter only @100k: 2.06\n",
    "    - all info minus time @100K: 1.62\n",
    "    - all info incl time @100k: 1.62\n",
    "    - all info incl time + ids @100k: 1.31"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38_fastai]",
   "language": "python",
   "name": "conda-env-py38_fastai-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
