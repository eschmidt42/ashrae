# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02_preprocessing.ipynb (unless otherwise specified).

__all__ = ['Processor', 'DEP_VAR', 'TIME_COL', 'semi_ugly_join', 'test_var_names', 'store_var_names', 'load_var_names',
           'store_df', 'load_df', 'get_tabular_object', 'train_predict', 'hist_plot_preds', 'BoldlyWrongTimeseries']

# Cell
import pandas as pd
from pathlib import Path
import os
import plotly.graph_objects as go
import plotly.express as px
import numpy as np
import typing
import pickle
import ipywidgets as widgets

from sklearn import linear_model, tree, model_selection, ensemble
from ashrae import inspection
from fastai.tabular.all import *

import tqdm

from sklearn import linear_model, tree, model_selection, ensemble

# Cell
DEP_VAR = 'meter_reading'
TIME_COL = 'timestamp'

semi_ugly_join = lambda a, b: "_".join(list("=".join(v) for v in zip([str(v) for v in a],
                                                                     [str(v) for v in b])))

class Processor:

    dep_var_stats:dict = None

    def __call__(self, df_core:pd.DataFrame, df_building:pd.DataFrame=None,
                 df_weather:pd.DataFrame=None, dep_var:str=None, time_col:str=None,
                 add_time_features:bool=False, add_dep_var_stats:bool=False,
                 remove_leading_zeros:bool=False, remove_trailing_zeros:bool=True,
                 remove_empty_weeks_before_first_full_week:bool=True,
                 t_train:pd.DataFrame=None) -> pd.DataFrame:

        # TODO:
        # - add daily features: temperature delta per site_id, total rain fall, ...
        # - add global stats: mean, median and so on of dep_var by building_id or type
        # - add consumption of the days around the day of interest


        # sanity check presence of df_building if df_weather is given
        if df_weather is not None:
            assert df_building is not None, 'To join the weather info in `df_weather` you need to pass `df_building`.'

        self.dep_var = DEP_VAR if dep_var is None else dep_var
        self.time_col = TIME_COL if time_col is None else time_col

        self.conts, self.cats, self.cats_order = [], [], {}

        # sanity check if `df` is a test set (dep_var is missing)
        self.is_train = self.dep_var in df_core.columns

        # core pieces of dependent and independent variables
        self.dep_var_new = f'{self.dep_var}_log1p'
        if self.is_train:
            df_core[self.dep_var_new] = np.log(df_core[self.dep_var].values + 1)
        self.cats += ['building_id', 'meter']

        # add timestamp related fields
        if add_time_features:
            df_core = self.add_time_features(df_core)

        # removing leading zeros
        if remove_leading_zeros and self.is_train:
            df_core = self.remove_leading_zeros(df_core, t_train=t_train)

        # removing trailing zeros
        if remove_trailing_zeros and self.is_train:
            df_core = self.remove_trailing_zeros(df_core, t_train=t_train)

        # removing weeks before the first full week
        if remove_empty_weeks_before_first_full_week and self.is_train:
            df_core = self.remove_empty_weeks_before_first_full_week(df_core, t_train=t_train)

        # adding basic statistics as features
        if add_dep_var_stats:
            df_core = self.add_dep_var_stats(df_core) # , grp_cols=['meter']

        # adding building information
        if df_building is not None:
            df_core = self.add_building_features(df_core, df_building)

        # adding weather information
        if df_weather is not None:
            df_core = self.add_weather_features(df_core, df_weather)


        df_core, var_names = self.cleanup(df_core)
        return df_core, var_names


    def add_dep_var_stats(self, df_core:pd.DataFrame, grp_cols:typing.List[str]=None):
        assert self.is_train or self.dep_var_stats is not None
        if self.is_train:
            self.dep_var_stats = dict()
        funs = {
            'median': lambda x: torch.median(tensor(x)).item(),
            'mean': lambda x: torch.mean(tensor(x)).item(),
            '5%': lambda x: np.percentile(x, 5),
            '95%': lambda x: np.percentile(x, 95),
        }

        # computing stats for self.dep_var on the coarsest possible level
        for name, fun in funs.items():
            name = f'{self.dep_var}_{name}'
            self.conts.append(name)

            if self.is_train:
                value = fun(df_core[self.dep_var].values)
                df_core[name] = value
                self.dep_var_stats[name] = value
            else:
                df_core[name] = self.dep_var_stats[name]

        # adding stats of self.dep_var on a more granular level
        if grp_cols is not None:

            assert all([c in df_core.columns.values for c in grp_cols])
            for keys, grp in df_core.groupby(grp_cols):
                if not isinstance(keys, (tuple,list)): keys = [keys]
                for name, fun in funs.items():
                    name = f'{self.dep_var}_{name}_{semi_ugly_join(grp_cols, keys)}'
                    self.conts.append(name)

                    if self.is_train:
                        value = fun(grp[self.dep_var].values)
                        df_core[name] = value
                        self.dep_var_stats[name] = value
                    else:
                        df_core[name] = self.dep_var_stats[name]

        return df_core

    def add_time_features(self, df_core:pd.DataFrame):
        self.cats.extend(['timestampMonth', 'timestampDay', 'timestampWeek', 'timestampDayofweek',
                          'timestampDayofyear', 'timestampIs_month_end', 'timestampIs_month_start',
                          'timestampIs_quarter_start', 'timestampIs_quarter_end',
                          'timestampIs_year_start', 'timestampIs_year_end', 'timestampHour'])
        df_core = add_datepart(df_core, self.time_col, drop=False)
        df_core['timestampHour'] = df_core[self.time_col].dt.hour
        self.cats_order.update({c: sorted(df_core[c].unique()) for c in ['timestampMonth', 'timestampDay',
                                               'timestampWeek', 'timestampDayofweek',
                                               'timestampDayofyear']})
        return df_core

    def add_building_features(self, df_core:pd.DataFrame, df_building:pd.DataFrame):
        n = len(df_core)
        df_core = pd.merge(df_core, df_building, on='building_id', how='left')
        assert n == len(df_core)

        self.cats.extend(['site_id', 'primary_use'])
        self.conts.extend(['square_feet', 'year_built', 'floor_count'])
        return df_core

    def add_weather_features(self, df_core:pd.DataFrame, df_weather:pd.DataFrame):
        n = len(df_core)
        df_core = pd.merge(df_core, df_weather, on=['site_id', 'timestamp'], how='left')
        assert n == len(df_core)

        self.cats.extend(['cloud_coverage'])
        self.cats_order['cloud_coverage'] = sorted([v for v in df_core['cloud_coverage'].unique() if np.isfinite(v)])
        self.conts.extend(['wind_direction', 'air_temperature', 'dew_temperature', 'precip_depth_1_hr',
                      'sea_level_pressure', 'wind_speed'])
        return df_core

    def remove_leading_zeros(self, df_core:pd.DataFrame, t_train:pd.DataFrame=None):
        'there are time series which start with many 0 values in the dep_var. this method removes those values'

        n = len(df_core)
        assert self.dep_var in df_core.columns
        assert 'timestamp' in df_core.columns
        assert df_core[self.dep_var].min() == 0

        # finding the first timestamps after 0s
        mins = (df_core[df_core[self.dep_var] > 0].groupby(["building_id","meter"])
                .timestamp.min().rename("first_timestamp"))
        df_core = df_core.join(mins,on=["building_id","meter"])

        mask = df_core['first_timestamp'] <= df_core['timestamp']

        if t_train is not None:
            t_mask = df_core['timestamp'].isin(t_train['timestamp'])
            mask = (mask & t_mask) | ~t_mask

        df_core = df_core.loc[mask,:]
        df_core.drop(columns=['first_timestamp'], inplace=True)
        assert len(df_core) < n
        print(f'Removed {(1-len(df_core)/n)*100:.4f} % of rows')

        return df_core


    def remove_trailing_zeros(self, df_core:pd.DataFrame, t_train:pd.DataFrame=None):
        'there are time series which end with many 0 values in the dep_var. this method removes those values'

        n = len(df_core)
        assert self.dep_var in df_core.columns
        assert 'timestamp' in df_core.columns
        assert df_core[self.dep_var].min() == 0

        # finding the first timestamps after 0s
        maxs = (df_core[df_core[self.dep_var] > 0].groupby(["building_id","meter"])
                .timestamp.max().rename("last_timestamp"))
        df_core = df_core.join(maxs,on=["building_id","meter"])

        mask = df_core['last_timestamp'] >= df_core['timestamp']

        if t_train is not None:
            t_mask = df_core['timestamp'].isin(t_train['timestamp'])
            mask = (mask & t_mask) | ~t_mask

        df_core = df_core.loc[mask,:]
        df_core.drop(columns=['last_timestamp'], inplace=True)
        assert len(df_core) < n
        print(f'Removed {(1-len(df_core)/n)*100:.4f} % of rows')

        return df_core

    def remove_empty_weeks_before_first_full_week(self, df_core:pd.DataFrame,
                                                  t_train:pd.DataFrame):
        'there are some timeseries with weeks in the beginning which are basically empty'
        # TODO: something is likely buggy, losing combinations of building_id and meter

        n = len(df_core)
        n_comb = len(df_core.loc[:,['building_id', 'meter']].drop_duplicates())

        def get_combs(df):
            return set([tuple([row['building_id'], row['meter']])
                                for _, row in (df.loc[:,['building_id', 'meter']]
                                               .drop_duplicates()
                                               .iterrows())])

        combs = get_combs(df_core)

        df_core['timestampWeek'] = df_core[self.time_col].dt.isocalendar().week

        counts = (df_core[df_core[self.dep_var] > 0]
                  .groupby(["building_id","meter","timestampWeek"])
                  .timestamp.count()
                  .rename("num_weekly_measurements").reset_index())

        expected_num = 24*7 # hours per week with a measurement
        expected_num *= 0.5

        first_full_week = (counts[counts['num_weekly_measurements'] > expected_num]
                           .groupby(["building_id","meter"])
                           .timestampWeek.min()
                           .rename("first_full_week")
                           .to_frame())

        df_core = df_core.join(first_full_week, on=["building_id","meter"])

        mask = df_core['timestampWeek'] >= df_core['first_full_week']

        na_series = df_core.loc[df_core['first_full_week'].isna(), ['building_id', 'meter']].drop_duplicates()

        print('number of na combinations', len(na_series))
        display('na bids & meters', len(na_series), na_series)

        mask = mask & df_core['first_full_week'].notna() # some time series have in each week less than the required number of observations

        if t_train is not None:
            t_mask = df_core['timestamp'].isin(t_train['timestamp'])
            mask = (mask & t_mask) | ~t_mask

        df_core = df_core.loc[mask,:]

        df_core.drop(columns=['timestampWeek', 'first_full_week'], inplace=True)

        na_combs = get_combs(df_core)
        miss_combs = [v for v in na_combs if v not in combs]
        print('combs diff', miss_combs, len(miss_combs))

        assert len(df_core) < n
        assert len(na_series) == 38
        print(f'Removed {(1-len(df_core)/n)*100:.4f} % of rows')
        print(f'{len(na_series)} of building_id/meter combinations count as empty = {100 * len(na_series) / n_comb:.4f} % of all combinations')
        return df_core


    def cleanup(self, df_core:pd.DataFrame):
        # converting cats to category type
        for col in self.cats:
            if df_core[col].dtype == bool: continue
            df_core[col] = df_core[col].astype('category')
            if col in self.cats_order:
                df_core[col].cat.set_categories(self.cats_order[col],
                                                ordered=True, inplace=True)

        # removing features
        to_remove_cols = [self.dep_var, 'timestampYear'] # , self.time_col
        df_core = df_core.drop(columns=[c for c in df_core.columns if c in to_remove_cols])

        # shrinking the data frame
        df_core = df_shrink(df_core, int2uint=True)

        var_names = {'conts': self.conts, 'cats': self.cats, 'dep_var': self.dep_var_new}
        if not self.is_train:
            df_core.set_index('row_id', inplace=True)
        missing_cols = [col for col in df_core.columns.values if col not in self.cats + self.conts + [self.dep_var_new]
                        and col not in ['timestampElapsed', self.time_col]]
        assert len(missing_cols) == 0, f'Missed to assign columns: {missing_cols} to `conts` or `cats`'
        return df_core, var_names

# Cell
def test_var_names(var_names:dict):
    assert isinstance(var_names, dict)
    assert 'conts' in var_names and 'cats' in var_names and 'dep_var' in var_names
    assert isinstance(var_names['conts'], list)
    assert isinstance(var_names['cats'], list)
    assert isinstance(var_names['dep_var'], str)

# Cell
def store_var_names(data_path:Path, var_names:dict):
    fname = data_path/'var_names.pckl'
    print(f'Storing var names at: {fname}')
    with open(fname, 'wb') as f:
        pickle.dump(var_names, f)

# Cell
def load_var_names(fname:Path):
    print(f'Reading var names at: {fname}')
    with open(fname, 'rb') as f:
        var_names = pickle.load(f)
    return var_names

# Cell
def store_df(path:Path, df:pd.DataFrame): df.to_parquet(path)

# Cell
def load_df(path:Path): return pd.read_parquet(path)

# Cell
def get_tabular_object(df:pd.DataFrame, var_names:dict,
                       splits=None):
    procs = [Categorify, FillMissing, Normalize]
    return TabularPandas(df.copy(), procs,
                         var_names['cats'], var_names['conts'],
                         y_names=var_names['dep_var'],
                         splits=splits)
    return to


def train_predict(df:pd.DataFrame, var_names:dict,
                  model, params:dict=None, n_rep:int=3,
                  n_samples_train:int=10000,
                  n_samples_test:int=10000,
                  test_size:float=.2):

    y_col = var_names['dep_var']
    score_vals = []
    params = {} if params is None else params

    to = get_tabular_object(df, var_names)

    for i in tqdm.tqdm(range(n_rep), total=n_rep, desc='Repetition'):

        m = model(**params)

        mask = to.xs.index.isin(
            np.random.choice(to.xs.index.values, size=int(test_size*len(to.xs)), replace=False)
        )

        _X = to.xs.loc[~mask, :].iloc[:n_samples_train]
        _y = to.ys.loc[~mask, y_col].iloc[:n_samples_train]
        m.fit(_X.values, _y.values)

        _X = to.xs.loc[mask, :].iloc[:n_samples_test]
        _y = to.ys.loc[mask, y_col].iloc[:n_samples_test]
        pred = m.predict(_X.values)
        s = torch.sqrt(F.mse_loss(tensor(pred), tensor(_y.values))).item()
        score_vals.append({'iter': i, 'rmse loss': s})

    return pd.DataFrame(score_vals)

# Cell
def hist_plot_preds(y0:np.ndarray, y1:np.ndarray,
                    label0:str='y0', label1:str='y1'):
    res = pd.concat(
        (
            pd.DataFrame({
                'y': y0,
                'set': [label0] * len(y0)
            }),
            pd.DataFrame({
                'y':y1,
                'set': [label1] * len(y1)
            })
        ),
        ignore_index=True
    )

    return px.histogram(res, x='y', color='set', marginal='box',
                        barmode='overlay', histnorm='probability density')

# Cell
class BoldlyWrongTimeseries:
    def __init__(self, xs, y_true, y_pred, t:pd.DataFrame=None):
        if t is None:
            self.df = xs.loc[:,['meter', 'building_id', 'timestampElapsed']].copy()
        else:
            self.df = xs.loc[:,['meter', 'building_id']].join(t.reset_index().drop_duplicates().set_index('index'))
        self.df['y_true'] = y_true
        self.df['y_pred'] = y_pred
        self.compute_misses(y_true)

    def compute_misses(self, y_true):
        fun = lambda x: np.mean(x**2)
        self.miss = (self.df.assign(difference=lambda x: x['y_pred']-x['y_true'])
                     .groupby(['meter', 'building_id'])
                     .agg(loss=pd.NamedAgg(column='difference', aggfunc=fun))
                     .sort_values('loss'))

# Cell
@patch
def plot_boldly_wrong(self:BoldlyWrongTimeseries,
                      nth_last:int=None,
                      meter:int=None, bid:int=None):

    assert (meter is not None and bid is not None) or (nth_last is not None)

    if nth_last is not None:
        ix = self.miss.iloc[[nth_last],:]
        meter = ix.index[0][0]
        bid = ix.index[0][1]
        loss = ix["loss"].values[0]
    else:
        ix = self.miss.xs((meter, bid))
        loss = ix.values[0]

    df_plot = self.df.loc[(self.df['meter']==meter) & (self.df['building_id']==bid)]
    df_plot = pd.concat((
        df_plot[['timestampElapsed', 'y_true']].rename(columns={'y_true':'y'}).assign(label='true'),
        df_plot[['timestampElapsed', 'y_pred']].rename(columns={'y_pred':'y'}).assign(label='pred'))
    )
    return df_plot.plot(kind='scatter', x='timestampElapsed',
                        y='y', color='label', opacity=.4,
                        title=f'pos {nth_last}: meter = {meter}, building_id = {bid}<br>loss = {loss:.3f}')


BoldlyWrongTimeseries.plot_boldly_wrong = plot_boldly_wrong

# Cell
@patch
def init_widgets(self:BoldlyWrongTimeseries):
    self.int_txt_loss = widgets.IntText(min=-len(self.miss), max=len(self.miss),
                                        description='Position', value=-1)
    self.int_txt_meter = widgets.IntText(min=self.df['meter'].min(), max=self.df['meter'].max(),
                                         description='Meter')
    self.int_txt_bid = widgets.IntText(min=self.df['building_id'].min(), max=self.df['building_id'].max(),
                                       description='building id')
    self.run_btn = widgets.Button(description='plot')
    self.switch_btn = widgets.Checkbox(description='Loss-based', value=True)
    self.run_btn.on_click(self.click_boldly_wrong)
    self.out_wdg = widgets.Output()

@patch
def run_boldly(self:BoldlyWrongTimeseries):
    if not hasattr(self, 'switch_btn'):
        self.init_widgets()
    return widgets.VBox([self.switch_btn, self.int_txt_loss,
                         self.int_txt_meter, self.int_txt_bid,
                         self.run_btn, self.out_wdg])

@patch
def click_boldly_wrong(self:BoldlyWrongTimeseries, change):
    self.out_wdg.clear_output()
    nth_last = None if self.switch_btn.value == False else self.int_txt_loss.value
    meter = None if self.switch_btn.value == True else self.int_txt_meter.value
    bid = None if self.switch_btn.value == True else self.int_txt_bid.value
    with self.out_wdg:
        print(f'nth_last {nth_last} meter {meter} bid {bid}')
        try:
            self.plot_boldly_wrong(nth_last=nth_last, meter=meter, bid=bid).show()
        except:
            raise ValueError(f'nth_last {nth_last} meter {meter} bid {bid} not a valid combination! Likely due to missing meter/bid combination')